{
 "cells": [
  {
      "cell_type": "markdown",
      "id": "6a44f008",
      "metadata": {
        "id": "6a44f008"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dimitarpg13/agentic_architectures_and_design_patterns/blob/main/notebooks/live_web_search/duckduckgo_langgraph_mcp_multiagent_ex1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Web Search System with MCP Protocol and LangGraph\n",
    "\n",
    "This notebook demonstrates an advanced multi-agent system that uses:\n",
    "- **MCP (Model Context Protocol)**: Standardized communication between agents\n",
    "- **LangGraph**: Agent orchestration and workflow management\n",
    "- **DuckDuckGo**: Live web search capabilities\n",
    "- **Multi-Agent Design**: Specialized agents working together\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "User Query â†’ Coordinator Agent â†’ Research Agent (DuckDuckGo)\n",
    "                              â†“\n",
    "                         Analyzer Agent\n",
    "                              â†“\n",
    "                        Synthesizer Agent â†’ Final Response\n",
    "```\n",
    "\n",
    "### Agent Roles:\n",
    "1. **Coordinator**: Routes queries and manages workflow\n",
    "2. **Researcher**: Performs web searches using DuckDuckGo\n",
    "3. **Analyzer**: Evaluates and validates search results\n",
    "4. **Synthesizer**: Combines findings into coherent response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q langchain langchain-openai langgraph duckduckgo-search langchain-community pydantic asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import TypedDict, Annotated, Literal, List, Dict, Any\n",
    "from datetime import datetime\n",
    "import operator\n",
    "\n",
    "from langchain_community.tools import DuckDuckGoSearchRun, DuckDuckGoSearchResults\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "print(\"âœ… All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set Up API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "# Or load from environment\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define MCP Protocol Structures\n",
    "\n",
    "MCP (Model Context Protocol) defines standardized message formats for agent communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCP Message Types\n",
    "class MCPMessage(BaseModel):\n",
    "    \"\"\"Base MCP message structure.\"\"\"\n",
    "    message_id: str\n",
    "    timestamp: str\n",
    "    sender: str\n",
    "    receiver: str\n",
    "    message_type: str\n",
    "    content: Dict[str, Any]\n",
    "    metadata: Dict[str, Any] = Field(default_factory=dict)\n",
    "\n",
    "class MCPSearchRequest(BaseModel):\n",
    "    \"\"\"MCP request for web search.\"\"\"\n",
    "    query: str\n",
    "    num_results: int = 5\n",
    "    search_type: Literal[\"general\", \"news\", \"scholarly\"] = \"general\"\n",
    "\n",
    "class MCPSearchResult(BaseModel):\n",
    "    \"\"\"MCP formatted search result.\"\"\"\n",
    "    title: str\n",
    "    snippet: str\n",
    "    url: str\n",
    "    relevance_score: float = 0.0\n",
    "\n",
    "class MCPAnalysis(BaseModel):\n",
    "    \"\"\"MCP analysis result.\"\"\"\n",
    "    summary: str\n",
    "    key_points: List[str]\n",
    "    credibility_score: float\n",
    "    sources_count: int\n",
    "\n",
    "class MCPSynthesis(BaseModel):\n",
    "    \"\"\"MCP final synthesis.\"\"\"\n",
    "    answer: str\n",
    "    confidence: float\n",
    "    sources: List[str]\n",
    "    reasoning: str\n",
    "\n",
    "print(\"âœ… MCP Protocol structures defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Multi-Agent State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAgentState(TypedDict):\n",
    "    \"\"\"State shared across all agents.\"\"\"\n",
    "    # Original query\n",
    "    query: str\n",
    "    \n",
    "    # MCP messages between agents\n",
    "    mcp_messages: Annotated[List[Dict], operator.add]\n",
    "    \n",
    "    # Agent outputs\n",
    "    search_results: List[Dict]\n",
    "    analysis: Dict[str, Any]\n",
    "    synthesis: Dict[str, Any]\n",
    "    \n",
    "    # Workflow control\n",
    "    current_agent: str\n",
    "    next_agent: str\n",
    "    iteration: int\n",
    "    \n",
    "    # Final output\n",
    "    final_response: str\n",
    "\n",
    "print(\"âœ… Multi-agent state defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create MCP Message Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "def create_mcp_message(\n",
    "    sender: str,\n",
    "    receiver: str,\n",
    "    message_type: str,\n",
    "    content: Dict[str, Any],\n",
    "    metadata: Dict[str, Any] = None\n",
    ") -> Dict:\n",
    "    \"\"\"Create a standardized MCP message.\"\"\"\n",
    "    message = MCPMessage(\n",
    "        message_id=str(uuid.uuid4()),\n",
    "        timestamp=datetime.now().isoformat(),\n",
    "        sender=sender,\n",
    "        receiver=receiver,\n",
    "        message_type=message_type,\n",
    "        content=content,\n",
    "        metadata=metadata or {}\n",
    "    )\n",
    "    return message.model_dump()\n",
    "\n",
    "def log_mcp_message(message: Dict):\n",
    "    \"\"\"Log MCP message for debugging.\"\"\"\n",
    "    print(f\"\\nðŸ“¨ MCP Message:\")\n",
    "    print(f\"  From: {message['sender']} â†’ To: {message['receiver']}\")\n",
    "    print(f\"  Type: {message['message_type']}\")\n",
    "    print(f\"  Time: {message['timestamp']}\")\n",
    "    print(f\"  Content: {json.dumps(message['content'], indent=2)[:200]}...\")\n",
    "\n",
    "print(\"âœ… MCP helper functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Agent 1: Coordinator Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoordinatorAgent:\n",
    "    \"\"\"Coordinates the multi-agent workflow.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.name = \"coordinator\"\n",
    "        \n",
    "    def process(self, state: MultiAgentState) -> MultiAgentState:\n",
    "        \"\"\"Analyze query and route to appropriate agents.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ðŸŽ¯ COORDINATOR AGENT\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        query = state[\"query\"]\n",
    "        \n",
    "        # Analyze the query\n",
    "        prompt = f\"\"\"Analyze this query and determine the search strategy:\n",
    "        \n",
    "Query: {query}\n",
    "\n",
    "Provide:\n",
    "1. Search keywords (comma-separated)\n",
    "2. Number of results needed (1-10)\n",
    "3. Search type (general/news/scholarly)\n",
    "4. Complexity (simple/moderate/complex)\n",
    "\n",
    "Return as JSON with keys: keywords, num_results, search_type, complexity\"\"\"\n",
    "        \n",
    "        response = self.llm.invoke([HumanMessage(content=prompt)])\n",
    "        \n",
    "        # Parse response (simplified)\n",
    "        try:\n",
    "            analysis = json.loads(response.content)\n",
    "        except:\n",
    "            analysis = {\n",
    "                \"keywords\": query,\n",
    "                \"num_results\": 5,\n",
    "                \"search_type\": \"general\",\n",
    "                \"complexity\": \"moderate\"\n",
    "            }\n",
    "        \n",
    "        print(f\"ðŸ“‹ Query Analysis: {analysis}\")\n",
    "        \n",
    "        # Create MCP message for researcher\n",
    "        mcp_msg = create_mcp_message(\n",
    "            sender=\"coordinator\",\n",
    "            receiver=\"researcher\",\n",
    "            message_type=\"search_request\",\n",
    "            content={\n",
    "                \"query\": analysis[\"keywords\"],\n",
    "                \"num_results\": analysis[\"num_results\"],\n",
    "                \"search_type\": analysis[\"search_type\"]\n",
    "            },\n",
    "            metadata={\"complexity\": analysis[\"complexity\"]}\n",
    "        )\n",
    "        \n",
    "        log_mcp_message(mcp_msg)\n",
    "        \n",
    "        return {\n",
    "            \"mcp_messages\": [mcp_msg],\n",
    "            \"current_agent\": \"coordinator\",\n",
    "            \"next_agent\": \"researcher\",\n",
    "            \"iteration\": state.get(\"iteration\", 0) + 1\n",
    "        }\n",
    "\n",
    "print(\"âœ… Coordinator Agent created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Agent 2: Researcher Agent (with DuckDuckGo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearcherAgent:\n",
    "    \"\"\"Performs web searches using DuckDuckGo.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"researcher\"\n",
    "        self.search_tool = DuckDuckGoSearchResults(num_results=5)\n",
    "        \n",
    "    def process(self, state: MultiAgentState) -> MultiAgentState:\n",
    "        \"\"\"Execute web search based on MCP request.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ðŸ” RESEARCHER AGENT\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Get the latest MCP message\n",
    "        mcp_messages = state.get(\"mcp_messages\", [])\n",
    "        if not mcp_messages:\n",
    "            return state\n",
    "            \n",
    "        last_message = mcp_messages[-1]\n",
    "        search_request = last_message[\"content\"]\n",
    "        \n",
    "        print(f\"ðŸ”Ž Searching for: {search_request['query']}\")\n",
    "        \n",
    "        # Perform search\n",
    "        try:\n",
    "            raw_results = self.search_tool.run(search_request[\"query\"])\n",
    "            \n",
    "            # Parse results\n",
    "            if isinstance(raw_results, str):\n",
    "                # Parse string results\n",
    "                import re\n",
    "                results = []\n",
    "                snippets = raw_results.split('snippet: ')\n",
    "                \n",
    "                for i, snippet in enumerate(snippets[1:], 1):  # Skip first empty split\n",
    "                    parts = snippet.split('title: ')\n",
    "                    if len(parts) > 1:\n",
    "                        title = parts[1].split('link: ')[0].strip()\n",
    "                        link = parts[1].split('link: ')[1].strip() if 'link: ' in parts[1] else \"\"\n",
    "                        results.append({\n",
    "                            \"title\": title[:100],\n",
    "                            \"snippet\": parts[0].strip()[:200],\n",
    "                            \"url\": link,\n",
    "                            \"relevance_score\": 1.0 - (i * 0.1)\n",
    "                        })\n",
    "            else:\n",
    "                results = raw_results[:search_request[\"num_results\"]]\n",
    "                \n",
    "            print(f\"âœ… Found {len(results)} results\")\n",
    "            \n",
    "            # Create MCP message with results\n",
    "            mcp_msg = create_mcp_message(\n",
    "                sender=\"researcher\",\n",
    "                receiver=\"analyzer\",\n",
    "                message_type=\"search_results\",\n",
    "                content={\n",
    "                    \"results\": results,\n",
    "                    \"query\": search_request[\"query\"],\n",
    "                    \"results_count\": len(results)\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            log_mcp_message(mcp_msg)\n",
    "            \n",
    "            return {\n",
    "                \"search_results\": results,\n",
    "                \"mcp_messages\": [mcp_msg],\n",
    "                \"current_agent\": \"researcher\",\n",
    "                \"next_agent\": \"analyzer\"\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Search error: {e}\")\n",
    "            return {\n",
    "                \"search_results\": [],\n",
    "                \"current_agent\": \"researcher\",\n",
    "                \"next_agent\": \"analyzer\"\n",
    "            }\n",
    "\n",
    "print(\"âœ… Researcher Agent created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Agent 3: Analyzer Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalyzerAgent:\n",
    "    \"\"\"Analyzes and validates search results.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.name = \"analyzer\"\n",
    "        \n",
    "    def process(self, state: MultiAgentState) -> MultiAgentState:\n",
    "        \"\"\"Analyze search results quality and relevance.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ðŸ“Š ANALYZER AGENT\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        search_results = state.get(\"search_results\", [])\n",
    "        query = state[\"query\"]\n",
    "        \n",
    "        if not search_results:\n",
    "            print(\"âš ï¸  No search results to analyze\")\n",
    "            return state\n",
    "        \n",
    "        # Prepare results summary for analysis\n",
    "        results_summary = \"\\n\".join([\n",
    "            f\"{i+1}. {r.get('title', 'N/A')}: {r.get('snippet', 'N/A')[:100]}\"\n",
    "            for i, r in enumerate(search_results[:5])\n",
    "        ])\n",
    "        \n",
    "        prompt = f\"\"\"Analyze these search results for the query: \"{query}\"\n",
    "\n",
    "Results:\n",
    "{results_summary}\n",
    "\n",
    "Provide:\n",
    "1. Summary of findings (2-3 sentences)\n",
    "2. Key points (3-5 bullet points)\n",
    "3. Credibility score (0-1)\n",
    "4. Are results sufficient? (yes/no)\n",
    "\n",
    "Return as JSON with keys: summary, key_points (array), credibility_score, sufficient\"\"\"\n",
    "        \n",
    "        response = self.llm.invoke([HumanMessage(content=prompt)])\n",
    "        \n",
    "        # Parse analysis\n",
    "        try:\n",
    "            analysis = json.loads(response.content)\n",
    "        except:\n",
    "            analysis = {\n",
    "                \"summary\": \"Analysis of search results completed.\",\n",
    "                \"key_points\": [\"Multiple sources found\", \"Relevant information available\"],\n",
    "                \"credibility_score\": 0.7,\n",
    "                \"sufficient\": True\n",
    "            }\n",
    "        \n",
    "        print(f\"ðŸ“ˆ Credibility Score: {analysis.get('credibility_score', 0)}\")\n",
    "        print(f\"âœ“ Sufficient: {analysis.get('sufficient', False)}\")\n",
    "        \n",
    "        # Create MCP message\n",
    "        mcp_msg = create_mcp_message(\n",
    "            sender=\"analyzer\",\n",
    "            receiver=\"synthesizer\",\n",
    "            message_type=\"analysis_results\",\n",
    "            content=analysis\n",
    "        )\n",
    "        \n",
    "        log_mcp_message(mcp_msg)\n",
    "        \n",
    "        return {\n",
    "            \"analysis\": analysis,\n",
    "            \"mcp_messages\": [mcp_msg],\n",
    "            \"current_agent\": \"analyzer\",\n",
    "            \"next_agent\": \"synthesizer\"\n",
    "        }\n",
    "\n",
    "print(\"âœ… Analyzer Agent created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Agent 4: Synthesizer Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynthesizerAgent:\n",
    "    \"\"\"Synthesizes final response from all agent outputs.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.name = \"synthesizer\"\n",
    "        \n",
    "    def process(self, state: MultiAgentState) -> MultiAgentState:\n",
    "        \"\"\"Create final comprehensive response.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ðŸŽ¨ SYNTHESIZER AGENT\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        query = state[\"query\"]\n",
    "        search_results = state.get(\"search_results\", [])\n",
    "        analysis = state.get(\"analysis\", {})\n",
    "        \n",
    "        # Prepare context\n",
    "        context = f\"\"\"Original Query: {query}\n",
    "\n",
    "Search Results Summary:\n",
    "{analysis.get('summary', 'No analysis available')}\n",
    "\n",
    "Key Points:\n",
    "{chr(10).join('- ' + point for point in analysis.get('key_points', []))}\n",
    "\n",
    "Top Sources:\n",
    "{chr(10).join(f\"{i+1}. {r.get('title', 'N/A')} - {r.get('url', 'N/A')}\" for i, r in enumerate(search_results[:3]))}\n",
    "\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"Based on the following information, create a comprehensive answer:\n",
    "\n",
    "{context}\n",
    "\n",
    "Provide:\n",
    "1. A clear, well-structured answer to the query\n",
    "2. Confidence level (0-1)\n",
    "3. Reasoning for your answer\n",
    "\n",
    "Return as JSON with keys: answer, confidence, reasoning\"\"\"\n",
    "        \n",
    "        response = self.llm.invoke([HumanMessage(content=prompt)])\n",
    "        \n",
    "        # Parse synthesis\n",
    "        try:\n",
    "            synthesis = json.loads(response.content)\n",
    "        except:\n",
    "            synthesis = {\n",
    "                \"answer\": response.content,\n",
    "                \"confidence\": 0.7,\n",
    "                \"reasoning\": \"Based on web search results\"\n",
    "            }\n",
    "        \n",
    "        print(f\"âœ¨ Confidence: {synthesis.get('confidence', 0)}\")\n",
    "        \n",
    "        # Create final MCP message\n",
    "        mcp_msg = create_mcp_message(\n",
    "            sender=\"synthesizer\",\n",
    "            receiver=\"coordinator\",\n",
    "            message_type=\"final_response\",\n",
    "            content={\n",
    "                \"answer\": synthesis[\"answer\"],\n",
    "                \"confidence\": synthesis[\"confidence\"],\n",
    "                \"sources\": [r.get(\"url\", \"\") for r in search_results[:3]]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        log_mcp_message(mcp_msg)\n",
    "        \n",
    "        # Format final response\n",
    "        final_response = f\"\"\"**Answer:**\n",
    "{synthesis['answer']}\n",
    "\n",
    "**Confidence:** {synthesis['confidence']:.0%}\n",
    "\n",
    "**Sources:**\n",
    "{chr(10).join(f\"- {url}\" for url in [r.get('url', '') for r in search_results[:3]] if url)}\n",
    "\n",
    "**Reasoning:** {synthesis.get('reasoning', 'N/A')}\n",
    "\"\"\"\n",
    "        \n",
    "        return {\n",
    "            \"synthesis\": synthesis,\n",
    "            \"mcp_messages\": [mcp_msg],\n",
    "            \"final_response\": final_response,\n",
    "            \"current_agent\": \"synthesizer\",\n",
    "            \"next_agent\": \"end\"\n",
    "        }\n",
    "\n",
    "print(\"âœ… Synthesizer Agent created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Build the Multi-Agent LangGraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "\n",
    "# Initialize agents\n",
    "coordinator = CoordinatorAgent(llm)\n",
    "researcher = ResearcherAgent()\n",
    "analyzer = AnalyzerAgent(llm)\n",
    "synthesizer = SynthesizerAgent(llm)\n",
    "\n",
    "# Create workflow nodes\n",
    "def coordinator_node(state: MultiAgentState) -> MultiAgentState:\n",
    "    return coordinator.process(state)\n",
    "\n",
    "def researcher_node(state: MultiAgentState) -> MultiAgentState:\n",
    "    return researcher.process(state)\n",
    "\n",
    "def analyzer_node(state: MultiAgentState) -> MultiAgentState:\n",
    "    return analyzer.process(state)\n",
    "\n",
    "def synthesizer_node(state: MultiAgentState) -> MultiAgentState:\n",
    "    return synthesizer.process(state)\n",
    "\n",
    "# Build the graph\n",
    "workflow = StateGraph(MultiAgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"coordinator\", coordinator_node)\n",
    "workflow.add_node(\"researcher\", researcher_node)\n",
    "workflow.add_node(\"analyzer\", analyzer_node)\n",
    "workflow.add_node(\"synthesizer\", synthesizer_node)\n",
    "\n",
    "# Define edges (workflow sequence)\n",
    "workflow.set_entry_point(\"coordinator\")\n",
    "workflow.add_edge(\"coordinator\", \"researcher\")\n",
    "workflow.add_edge(\"researcher\", \"analyzer\")\n",
    "workflow.add_edge(\"analyzer\", \"synthesizer\")\n",
    "workflow.add_edge(\"synthesizer\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"\\nâœ… Multi-Agent System Ready!\")\n",
    "print(\"\\nðŸ¤– Agent Pipeline: Coordinator â†’ Researcher â†’ Analyzer â†’ Synthesizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualize the Multi-Agent Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the workflow\n",
    "try:\n",
    "    from IPython.display import Image, display\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not visualize graph: {e}\")\n",
    "    print(\"\\nWorkflow Structure:\")\n",
    "    print(\"Entry â†’ Coordinator â†’ Researcher â†’ Analyzer â†’ Synthesizer â†’ End\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Helper Function to Run Multi-Agent System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multi_agent_search(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Run the multi-agent search system.\n",
    "    \n",
    "    Args:\n",
    "        query: User's search query\n",
    "        \n",
    "    Returns:\n",
    "        Final synthesized response\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸš€ MULTI-AGENT WEB SEARCH SYSTEM\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nâ“ Query: {query}\\n\")\n",
    "    \n",
    "    # Initialize state\n",
    "    initial_state = {\n",
    "        \"query\": query,\n",
    "        \"mcp_messages\": [],\n",
    "        \"search_results\": [],\n",
    "        \"analysis\": {},\n",
    "        \"synthesis\": {},\n",
    "        \"current_agent\": \"start\",\n",
    "        \"next_agent\": \"coordinator\",\n",
    "        \"iteration\": 0,\n",
    "        \"final_response\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Run the workflow\n",
    "    try:\n",
    "        final_state = app.invoke(initial_state)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"âœ… FINAL RESPONSE\")\n",
    "        print(\"=\"*80)\n",
    "        print(final_state[\"final_response\"])\n",
    "        \n",
    "        # Display MCP message summary\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ðŸ“¨ MCP MESSAGE SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        for i, msg in enumerate(final_state.get(\"mcp_messages\", []), 1):\n",
    "            print(f\"{i}. {msg['sender']} â†’ {msg['receiver']}: {msg['message_type']}\")\n",
    "        \n",
    "        return final_state[\"final_response\"]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return f\"Error processing query: {e}\"\n",
    "\n",
    "print(\"âœ… Helper function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Example 1: Current Events Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Current events\n",
    "response1 = run_multi_agent_search(\n",
    "    \"What are the latest breakthroughs in quantum computing?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Example 2: Technical Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Technical comparison\n",
    "response2 = run_multi_agent_search(\n",
    "    \"Compare LangChain vs LlamaIndex for building LLM applications\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Example 3: Complex Research Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Complex research\n",
    "response3 = run_multi_agent_search(\n",
    "    \"What is the environmental impact of AI model training and what solutions are being proposed?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Advanced: Adding Feedback Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced workflow with feedback loops\n",
    "class AdaptiveMultiAgentState(MultiAgentState):\n",
    "    \"\"\"Extended state with feedback loop support.\"\"\"\n",
    "    needs_more_research: bool\n",
    "    research_iterations: int\n",
    "    max_iterations: int\n",
    "\n",
    "def should_continue_research(state: AdaptiveMultiAgentState) -> str:\n",
    "    \"\"\"Determine if more research is needed.\"\"\"\n",
    "    analysis = state.get(\"analysis\", {})\n",
    "    iterations = state.get(\"research_iterations\", 0)\n",
    "    max_iter = state.get(\"max_iterations\", 2)\n",
    "    \n",
    "    # Check if results are sufficient\n",
    "    sufficient = analysis.get(\"sufficient\", True)\n",
    "    credibility = analysis.get(\"credibility_score\", 1.0)\n",
    "    \n",
    "    if not sufficient and credibility < 0.7 and iterations < max_iter:\n",
    "        print(\"ðŸ”„ Feedback Loop: More research needed\")\n",
    "        return \"researcher\"  # Go back to researcher\n",
    "    else:\n",
    "        return \"synthesizer\"  # Proceed to synthesis\n",
    "\n",
    "# Build adaptive workflow (example structure)\n",
    "adaptive_workflow = StateGraph(AdaptiveMultiAgentState)\n",
    "adaptive_workflow.add_node(\"coordinator\", coordinator_node)\n",
    "adaptive_workflow.add_node(\"researcher\", researcher_node)\n",
    "adaptive_workflow.add_node(\"analyzer\", analyzer_node)\n",
    "adaptive_workflow.add_node(\"synthesizer\", synthesizer_node)\n",
    "\n",
    "adaptive_workflow.set_entry_point(\"coordinator\")\n",
    "adaptive_workflow.add_edge(\"coordinator\", \"researcher\")\n",
    "adaptive_workflow.add_edge(\"researcher\", \"analyzer\")\n",
    "\n",
    "# Conditional edge with feedback loop\n",
    "adaptive_workflow.add_conditional_edges(\n",
    "    \"analyzer\",\n",
    "    should_continue_research,\n",
    "    {\n",
    "        \"researcher\": \"researcher\",  # Loop back\n",
    "        \"synthesizer\": \"synthesizer\"  # Continue forward\n",
    "    }\n",
    ")\n",
    "\n",
    "adaptive_workflow.add_edge(\"synthesizer\", END)\n",
    "\n",
    "print(\"âœ… Adaptive workflow with feedback loops created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. MCP Protocol Monitoring Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_mcp_analytics(state: MultiAgentState):\n",
    "    \"\"\"Display analytics on MCP message flow.\"\"\"\n",
    "    messages = state.get(\"mcp_messages\", [])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ“Š MCP PROTOCOL ANALYTICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Message count by type\n",
    "    message_types = {}\n",
    "    for msg in messages:\n",
    "        msg_type = msg[\"message_type\"]\n",
    "        message_types[msg_type] = message_types.get(msg_type, 0) + 1\n",
    "    \n",
    "    print(\"\\nðŸ“¨ Message Types:\")\n",
    "    for msg_type, count in message_types.items():\n",
    "        print(f\"  - {msg_type}: {count}\")\n",
    "    \n",
    "    # Communication flow\n",
    "    print(\"\\nðŸ”„ Communication Flow:\")\n",
    "    for i, msg in enumerate(messages, 1):\n",
    "        print(f\"  {i}. {msg['sender']} â†’ {msg['receiver']} ({msg['message_type']})\")\n",
    "    \n",
    "    # Timing analysis\n",
    "    print(\"\\nâ±ï¸  Timing:\")\n",
    "    if messages:\n",
    "        first_time = datetime.fromisoformat(messages[0][\"timestamp\"])\n",
    "        last_time = datetime.fromisoformat(messages[-1][\"timestamp\"])\n",
    "        duration = (last_time - first_time).total_seconds()\n",
    "        print(f\"  Total Duration: {duration:.2f} seconds\")\n",
    "        print(f\"  Avg Time/Message: {duration/len(messages):.2f} seconds\")\n",
    "\n",
    "print(\"âœ… MCP analytics function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Run Complete System with Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with full analytics\n",
    "query = \"What are the ethical considerations of AI in healthcare?\"\n",
    "\n",
    "initial_state = {\n",
    "    \"query\": query,\n",
    "    \"mcp_messages\": [],\n",
    "    \"search_results\": [],\n",
    "    \"analysis\": {},\n",
    "    \"synthesis\": {},\n",
    "    \"current_agent\": \"start\",\n",
    "    \"next_agent\": \"coordinator\",\n",
    "    \"iteration\": 0,\n",
    "    \"final_response\": \"\"\n",
    "}\n",
    "\n",
    "final_state = app.invoke(initial_state)\n",
    "\n",
    "# Display analytics\n",
    "display_mcp_analytics(final_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. Interactive Multi-Agent Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive mode\n",
    "def interactive_multi_agent_search():\n",
    "    \"\"\"Interactive mode for multi-agent search.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ¤– MULTI-AGENT SEARCH SYSTEM - Interactive Mode\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Type 'quit' to exit, 'analytics' to see MCP stats\\n\")\n",
    "    \n",
    "    conversation_history = []\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"\\nâ“ Your question: \")\n",
    "        \n",
    "        if query.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"\\nðŸ‘‹ Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if query.lower() == 'analytics' and conversation_history:\n",
    "            display_mcp_analytics(conversation_history[-1])\n",
    "            continue\n",
    "        \n",
    "        if not query.strip():\n",
    "            continue\n",
    "        \n",
    "        # Run search\n",
    "        initial_state = {\n",
    "            \"query\": query,\n",
    "            \"mcp_messages\": [],\n",
    "            \"search_results\": [],\n",
    "            \"analysis\": {},\n",
    "            \"synthesis\": {},\n",
    "            \"current_agent\": \"start\",\n",
    "            \"next_agent\": \"coordinator\",\n",
    "            \"iteration\": 0,\n",
    "            \"final_response\": \"\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            final_state = app.invoke(initial_state)\n",
    "            conversation_history.append(final_state)\n",
    "            print(\"\\n\" + final_state[\"final_response\"])\n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ Error: {e}\")\n",
    "\n",
    "# Uncomment to run interactive mode\n",
    "# interactive_multi_agent_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21. Best Practices and Optimization Tips\n",
    "\n",
    "### MCP Protocol Benefits:\n",
    "1. **Standardization**: Consistent message format across agents\n",
    "2. **Traceability**: Full audit trail of agent communications\n",
    "3. **Debugging**: Easy to identify where issues occur\n",
    "4. **Extensibility**: Simple to add new agents or message types\n",
    "\n",
    "### Multi-Agent Design Principles:\n",
    "1. **Separation of Concerns**: Each agent has a specific role\n",
    "2. **Loose Coupling**: Agents communicate via standardized messages\n",
    "3. **Scalability**: Easy to parallelize or distribute agents\n",
    "4. **Fault Tolerance**: One agent failure doesn't crash the system\n",
    "\n",
    "### Performance Optimization:\n",
    "1. **Caching**: Cache search results to avoid redundant queries\n",
    "2. **Parallel Processing**: Run independent agents in parallel\n",
    "3. **Timeout Handling**: Set timeouts for each agent\n",
    "4. **Result Streaming**: Stream results as they become available\n",
    "\n",
    "### Production Considerations:\n",
    "1. **Error Handling**: Implement comprehensive error handling\n",
    "2. **Monitoring**: Track agent performance and MCP message flow\n",
    "3. **Rate Limiting**: Respect API rate limits (especially for search)\n",
    "4. **Testing**: Unit test each agent independently\n",
    "5. **Logging**: Detailed logging for debugging and analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22. Extending the System\n",
    "\n",
    "### Additional Agents to Consider:\n",
    "1. **Fact Checker Agent**: Verifies claims against multiple sources\n",
    "2. **Citation Agent**: Properly formats and tracks sources\n",
    "3. **Translation Agent**: Handles multilingual queries\n",
    "4. **Summarization Agent**: Creates different summary lengths\n",
    "5. **Bias Detection Agent**: Identifies potential bias in sources\n",
    "\n",
    "### Alternative Search Providers:\n",
    "```python\n",
    "# Brave Search (requires API key)\n",
    "from langchain_community.tools import BraveSearch\n",
    "\n",
    "# Tavily (optimized for LLMs)\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# SerpAPI (Google search)\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "```\n",
    "\n",
    "### Enhanced MCP Features:\n",
    "1. **Priority Queue**: Prioritize urgent messages\n",
    "2. **Message Retry**: Automatic retry on failure\n",
    "3. **Message Validation**: Schema validation for all messages\n",
    "4. **Message Routing**: Dynamic routing based on content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23. Resources and Further Learning\n",
    "\n",
    "### Documentation:\n",
    "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
    "- [LangChain Multi-Agent Systems](https://python.langchain.com/docs/use_cases/more/agents/multi_agent/)\n",
    "- [Model Context Protocol Spec](https://modelcontextprotocol.io/)\n",
    "\n",
    "### Related Topics:\n",
    "- Agent architectures (ReAct, Plan-and-Execute)\n",
    "- Tool calling and function schemas\n",
    "- Prompt engineering for agents\n",
    "- Distributed systems patterns\n",
    "\n",
    "### Next Steps:\n",
    "1. Add memory/persistence to agents\n",
    "2. Implement parallel agent execution\n",
    "3. Create a web UI with Streamlit\n",
    "4. Deploy with FastAPI backend\n",
    "5. Add authentication and rate limiting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
