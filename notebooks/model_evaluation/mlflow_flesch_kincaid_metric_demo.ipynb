{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dimitarpg13/agentic_architectures_and_design_patterns/blob/main/notebooks/model_evaluation/mlflow_flesch_kincaid_metric_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apkMQxaamc0J"
      },
      "source": [
        "# MLflow Flesch-Kincaid Readability Metrics Demonstration\n",
        "\n",
        "This notebook provides a comprehensive guide to using Flesch-Kincaid readability metrics with MLflow for evaluating text complexity and readability in NLP applications.\n",
        "\n",
        "## What are Flesch-Kincaid Metrics?\n",
        "\n",
        "The Flesch-Kincaid metrics are two related tests that assess the readability of English text:\n",
        "\n",
        "### 1. Flesch Reading Ease (FRE)\n",
        "- **Range**: 0-100 (higher = easier to read)\n",
        "- **Formula**: 206.835 - 1.015 Ã— (total words/total sentences) - 84.6 Ã— (total syllables/total words)\n",
        "- **Interpretation**:\n",
        "  - 90-100: Very Easy (5th grade)\n",
        "  - 80-89: Easy (6th grade)\n",
        "  - 70-79: Fairly Easy (7th grade)\n",
        "  - 60-69: Standard (8th-9th grade)\n",
        "  - 50-59: Fairly Difficult (10th-12th grade)\n",
        "  - 30-49: Difficult (College)\n",
        "  - 0-29: Very Difficult (College graduate)\n",
        "\n",
        "### 2. Flesch-Kincaid Grade Level (FKGL)\n",
        "- **Range**: Typically 0-18+ (U.S. grade level)\n",
        "- **Formula**: 0.39 Ã— (total words/total sentences) + 11.8 Ã— (total syllables/total words) - 15.59\n",
        "- **Interpretation**: Direct mapping to U.S. education grade levels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_J1mcn5mc0K"
      },
      "source": [
        "## 1. Installation and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2s_KhBFmc0L"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q mlflow textstat pyphen nltk pandas numpy matplotlib seaborn plotly scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBO6EMRRmc0L"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import mlflow.metrics\n",
        "from mlflow.metrics import make_metric, MetricValue\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import warnings\n",
        "import re\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Text analysis libraries\n",
        "import textstat\n",
        "import pyphen\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import cmudict\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('cmudict', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Statistics\n",
        "from scipy import stats\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "\n",
        "print(f\"MLflow version: {mlflow.__version__}\")\n",
        "print(f\"Textstat version: {textstat.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPZmMew5mc0L"
      },
      "source": [
        "## 2. Creating Custom Flesch-Kincaid Metrics for MLflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTfmpmTtmc0M"
      },
      "outputs": [],
      "source": [
        "class FleschKincaidMetrics:\n",
        "    \"\"\"\n",
        "    Comprehensive Flesch-Kincaid readability metrics for MLflow.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Initialize CMU Pronouncing Dictionary for syllable counting\n",
        "        self.cmu_dict = cmudict.dict()\n",
        "        self.pyphen_dic = pyphen.Pyphen(lang='en')\n",
        "\n",
        "    def count_syllables(self, word: str) -> int:\n",
        "        \"\"\"\n",
        "        Count syllables in a word using multiple methods for accuracy.\n",
        "        \"\"\"\n",
        "        word = word.lower().strip()\n",
        "\n",
        "        # Try CMU dictionary first (most accurate)\n",
        "        if word in self.cmu_dict:\n",
        "            return len([ph for ph in self.cmu_dict[word][0] if ph[-1].isdigit()])\n",
        "\n",
        "        # Fallback to pyphen\n",
        "        hyphenated = self.pyphen_dic.inserted(word)\n",
        "        return len(hyphenated.split('-'))\n",
        "\n",
        "    def calculate_text_statistics(self, text: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Calculate comprehensive text statistics needed for readability metrics.\n",
        "        \"\"\"\n",
        "        # Clean text\n",
        "        text = re.sub(r'\\s+', ' ', text.strip())\n",
        "\n",
        "        # Tokenize\n",
        "        sentences = sent_tokenize(text)\n",
        "        words = word_tokenize(text)\n",
        "\n",
        "        # Filter words (remove punctuation)\n",
        "        words = [w for w in words if w.isalnum()]\n",
        "\n",
        "        # Count syllables\n",
        "        total_syllables = sum(self.count_syllables(word) for word in words)\n",
        "\n",
        "        # Calculate statistics\n",
        "        num_sentences = len(sentences)\n",
        "        num_words = len(words)\n",
        "\n",
        "        # Avoid division by zero\n",
        "        if num_sentences == 0 or num_words == 0:\n",
        "            return {\n",
        "                'num_sentences': 0,\n",
        "                'num_words': 0,\n",
        "                'num_syllables': 0,\n",
        "                'avg_words_per_sentence': 0,\n",
        "                'avg_syllables_per_word': 0\n",
        "            }\n",
        "\n",
        "        return {\n",
        "            'num_sentences': num_sentences,\n",
        "            'num_words': num_words,\n",
        "            'num_syllables': total_syllables,\n",
        "            'avg_words_per_sentence': num_words / num_sentences,\n",
        "            'avg_syllables_per_word': total_syllables / num_words\n",
        "        }\n",
        "\n",
        "    def flesch_reading_ease(self, text: str) -> float:\n",
        "        \"\"\"\n",
        "        Calculate Flesch Reading Ease score.\n",
        "        Score range: 0-100 (higher = easier to read)\n",
        "        \"\"\"\n",
        "        stats = self.calculate_text_statistics(text)\n",
        "\n",
        "        if stats['num_words'] == 0 or stats['num_sentences'] == 0:\n",
        "            return 0.0\n",
        "\n",
        "        score = 206.835 - (1.015 * stats['avg_words_per_sentence']) - \\\n",
        "                (84.6 * stats['avg_syllables_per_word'])\n",
        "\n",
        "        # Clamp to 0-100 range\n",
        "        return max(0, min(100, score))\n",
        "\n",
        "    def flesch_kincaid_grade(self, text: str) -> float:\n",
        "        \"\"\"\n",
        "        Calculate Flesch-Kincaid Grade Level.\n",
        "        Returns U.S. school grade level.\n",
        "        \"\"\"\n",
        "        stats = self.calculate_text_statistics(text)\n",
        "\n",
        "        if stats['num_words'] == 0 or stats['num_sentences'] == 0:\n",
        "            return 0.0\n",
        "\n",
        "        grade = (0.39 * stats['avg_words_per_sentence']) + \\\n",
        "                (11.8 * stats['avg_syllables_per_word']) - 15.59\n",
        "\n",
        "        # Typically clamped to minimum of -3.4 (kindergarten) and maximum of 18+\n",
        "        return max(-3.4, min(18, grade))\n",
        "\n",
        "    def create_reading_ease_metric(self):\n",
        "        \"\"\"\n",
        "        Create MLflow metric for Flesch Reading Ease.\n",
        "        \"\"\"\n",
        "        def reading_ease_metric(predictions, targets=None):\n",
        "            if not isinstance(predictions, list):\n",
        "                predictions = [predictions]\n",
        "\n",
        "            scores = [self.flesch_reading_ease(text) for text in predictions]\n",
        "\n",
        "            return MetricValue(\n",
        "                aggregate_results={\n",
        "                    \"flesch_reading_ease_mean\": np.mean(scores),\n",
        "                    \"flesch_reading_ease_std\": np.std(scores),\n",
        "                    \"flesch_reading_ease_min\": np.min(scores),\n",
        "                    \"flesch_reading_ease_max\": np.max(scores)\n",
        "                },\n",
        "                scores=scores\n",
        "            )\n",
        "\n",
        "        return make_metric(\n",
        "            eval_fn=reading_ease_metric,\n",
        "            greater_is_better=True,  # Higher scores = easier to read\n",
        "            name=\"flesch_reading_ease\"\n",
        "        )\n",
        "\n",
        "    def create_grade_level_metric(self):\n",
        "        \"\"\"\n",
        "        Create MLflow metric for Flesch-Kincaid Grade Level.\n",
        "        \"\"\"\n",
        "        def grade_level_metric(predictions, targets=None):\n",
        "            if not isinstance(predictions, list):\n",
        "                predictions = [predictions]\n",
        "\n",
        "            scores = [self.flesch_kincaid_grade(text) for text in predictions]\n",
        "\n",
        "            return MetricValue(\n",
        "                aggregate_results={\n",
        "                    \"flesch_kincaid_grade_mean\": np.mean(scores),\n",
        "                    \"flesch_kincaid_grade_std\": np.std(scores),\n",
        "                    \"flesch_kincaid_grade_min\": np.min(scores),\n",
        "                    \"flesch_kincaid_grade_max\": np.max(scores)\n",
        "                },\n",
        "                scores=scores\n",
        "            )\n",
        "\n",
        "        return make_metric(\n",
        "            eval_fn=grade_level_metric,\n",
        "            greater_is_better=False,  # Lower grade = more accessible\n",
        "            name=\"flesch_kincaid_grade\"\n",
        "        )\n",
        "\n",
        "    def create_comprehensive_readability_metric(self):\n",
        "        \"\"\"\n",
        "        Create a comprehensive readability metric including multiple measures.\n",
        "        \"\"\"\n",
        "        def comprehensive_metric(predictions, targets=None):\n",
        "            if not isinstance(predictions, list):\n",
        "                predictions = [predictions]\n",
        "\n",
        "            all_metrics = []\n",
        "            for text in predictions:\n",
        "                metrics = {\n",
        "                    'flesch_reading_ease': self.flesch_reading_ease(text),\n",
        "                    'flesch_kincaid_grade': self.flesch_kincaid_grade(text),\n",
        "                    'gunning_fog': textstat.gunning_fog(text),\n",
        "                    'smog_index': textstat.smog_index(text),\n",
        "                    'coleman_liau_index': textstat.coleman_liau_index(text),\n",
        "                    'automated_readability_index': textstat.automated_readability_index(text),\n",
        "                    'dale_chall_readability': textstat.dale_chall_readability_score(text),\n",
        "                    'linsear_write_formula': textstat.linsear_write_formula(text)\n",
        "                }\n",
        "                all_metrics.append(metrics)\n",
        "\n",
        "            # Calculate aggregates\n",
        "            aggregate_results = {}\n",
        "            for key in all_metrics[0].keys():\n",
        "                values = [m[key] for m in all_metrics]\n",
        "                aggregate_results[f\"{key}_mean\"] = np.mean(values)\n",
        "                aggregate_results[f\"{key}_std\"] = np.std(values)\n",
        "\n",
        "            # Add consensus grade level\n",
        "            aggregate_results['consensus_grade'] = np.mean([\n",
        "                m['flesch_kincaid_grade'] for m in all_metrics\n",
        "            ])\n",
        "\n",
        "            return MetricValue(\n",
        "                aggregate_results=aggregate_results,\n",
        "                scores=[m['flesch_reading_ease'] for m in all_metrics]\n",
        "            )\n",
        "\n",
        "        return make_metric(\n",
        "            eval_fn=comprehensive_metric,\n",
        "            greater_is_better=True,\n",
        "            name=\"comprehensive_readability\"\n",
        "        )\n",
        "\n",
        "# Initialize metrics\n",
        "fk_metrics = FleschKincaidMetrics()\n",
        "reading_ease_metric = fk_metrics.create_reading_ease_metric()\n",
        "grade_level_metric = fk_metrics.create_grade_level_metric()\n",
        "comprehensive_metric = fk_metrics.create_comprehensive_readability_metric()\n",
        "\n",
        "print(\"âœ… Custom Flesch-Kincaid metrics created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1AL8K1imc0M"
      },
      "source": [
        "## 3. Preparing Sample Text Data with Various Complexity Levels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoxY47Gmmc0M"
      },
      "outputs": [],
      "source": [
        "# Create sample texts with different readability levels\n",
        "sample_texts = [\n",
        "    {\n",
        "        \"category\": \"Children's Book\",\n",
        "        \"target_grade\": \"2-3\",\n",
        "        \"text\": \"\"\"The cat sat on the mat. It was a sunny day. The cat was happy.\n",
        "        Birds sang in the trees. The cat watched them fly. It was fun to watch birds.\n",
        "        The mat was soft and warm. The cat took a nap.\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Young Adult Novel\",\n",
        "        \"target_grade\": \"6-8\",\n",
        "        \"text\": \"\"\"Sarah walked through the forest, listening to the sounds around her.\n",
        "        The leaves rustled gently in the breeze, and birds called from the treetops.\n",
        "        She had been hiking for two hours, following the winding trail that led to the\n",
        "        waterfall. Her backpack contained everything she needed for the day's adventure.\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"News Article\",\n",
        "        \"target_grade\": \"8-10\",\n",
        "        \"text\": \"\"\"The city council voted yesterday to approve a new budget that allocates\n",
        "        significant funding for infrastructure improvements. The decision, which passed\n",
        "        with a narrow majority, will result in road repairs and upgraded public\n",
        "        transportation systems throughout the metropolitan area. Critics argue that the\n",
        "        spending could have been better directed toward education initiatives.\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Academic Paper\",\n",
        "        \"target_grade\": \"College\",\n",
        "        \"text\": \"\"\"The implementation of sophisticated machine learning algorithms has\n",
        "        revolutionized the methodological approaches utilized in contemporary computational\n",
        "        linguistics. Through the systematic application of transformer-based architectures,\n",
        "        researchers have demonstrated unprecedented capabilities in natural language\n",
        "        understanding and generation, thereby facilitating advancements in automated\n",
        "        translation systems and sentiment analysis frameworks.\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Legal Document\",\n",
        "        \"target_grade\": \"Graduate\",\n",
        "        \"text\": \"\"\"Notwithstanding any provisions contained herein to the contrary, the\n",
        "        aforementioned contractual obligations shall remain enforceable pursuant to the\n",
        "        applicable jurisdictional statutes, provided that such enforcement does not\n",
        "        contravene the fundamental principles of equity and fairness as established by\n",
        "        precedential determinations in relevant appellate court decisions.\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Technical Manual\",\n",
        "        \"target_grade\": \"10-12\",\n",
        "        \"text\": \"\"\"To configure the network settings, navigate to the system preferences\n",
        "        panel and select the network configuration option. Enter the IP address, subnet\n",
        "        mask, and gateway information provided by your network administrator. Ensure that\n",
        "        the DNS server addresses are correctly specified to enable proper domain name\n",
        "        resolution. Apply the settings and restart the network service if necessary.\"\"\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Convert to DataFrame and calculate initial metrics\n",
        "df_texts = pd.DataFrame(sample_texts)\n",
        "\n",
        "# Calculate readability scores for each text\n",
        "df_texts['reading_ease'] = df_texts['text'].apply(fk_metrics.flesch_reading_ease)\n",
        "df_texts['grade_level'] = df_texts['text'].apply(fk_metrics.flesch_kincaid_grade)\n",
        "df_texts['word_count'] = df_texts['text'].apply(lambda x: len(word_tokenize(x)))\n",
        "\n",
        "print(\"ðŸ“Š Sample Text Analysis:\")\n",
        "print(\"=\"*80)\n",
        "for _, row in df_texts.iterrows():\n",
        "    print(f\"\\nCategory: {row['category']}\")\n",
        "    print(f\"Target Grade: {row['target_grade']}\")\n",
        "    print(f\"Flesch Reading Ease: {row['reading_ease']:.1f}\")\n",
        "    print(f\"Flesch-Kincaid Grade: {row['grade_level']:.1f}\")\n",
        "    print(f\"Word Count: {row['word_count']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIvFayqDmc0N"
      },
      "source": [
        "## 4. Evaluating Text Generation Models with MLflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SA6oLCMmmc0N"
      },
      "outputs": [],
      "source": [
        "# Set up MLflow experiment\n",
        "mlflow.set_experiment(\"flesch-kincaid-metrics-demo\")\n",
        "\n",
        "def evaluate_text_readability(model_name: str,\n",
        "                             texts: List[str],\n",
        "                             target_audience: str = \"general\",\n",
        "                             desired_grade_range: Tuple[float, float] = None):\n",
        "    \"\"\"\n",
        "    Evaluate text readability using Flesch-Kincaid metrics and log to MLflow.\n",
        "    \"\"\"\n",
        "    with mlflow.start_run(run_name=model_name):\n",
        "        # Log parameters\n",
        "        mlflow.log_param(\"model_name\", model_name)\n",
        "        mlflow.log_param(\"num_texts\", len(texts))\n",
        "        mlflow.log_param(\"target_audience\", target_audience)\n",
        "        if desired_grade_range:\n",
        "            mlflow.log_param(\"desired_grade_min\", desired_grade_range[0])\n",
        "            mlflow.log_param(\"desired_grade_max\", desired_grade_range[1])\n",
        "\n",
        "        # Create evaluation dataframe\n",
        "        eval_df = pd.DataFrame({\"predictions\": texts})\n",
        "\n",
        "        # Evaluate with all readability metrics\n",
        "        metrics = [reading_ease_metric, grade_level_metric, comprehensive_metric]\n",
        "\n",
        "        results = mlflow.evaluate(\n",
        "            data=eval_df,\n",
        "            predictions=\"predictions\",\n",
        "            extra_metrics=metrics,\n",
        "            evaluators=\"default\"\n",
        "        )\n",
        "\n",
        "        # Calculate additional detailed metrics\n",
        "        detailed_scores = calculate_detailed_readability(texts)\n",
        "\n",
        "        # Log detailed metrics\n",
        "        for key, value in detailed_scores.items():\n",
        "            mlflow.log_metric(key, value)\n",
        "\n",
        "        # Check if texts meet target grade range\n",
        "        if desired_grade_range:\n",
        "            grades = [fk_metrics.flesch_kincaid_grade(text) for text in texts]\n",
        "            in_range = sum(desired_grade_range[0] <= g <= desired_grade_range[1]\n",
        "                          for g in grades)\n",
        "            mlflow.log_metric(\"texts_in_target_range\", in_range)\n",
        "            mlflow.log_metric(\"percentage_in_range\", (in_range / len(texts)) * 100)\n",
        "\n",
        "        return results.metrics, detailed_scores\n",
        "\n",
        "def calculate_detailed_readability(texts: List[str]) -> Dict:\n",
        "    \"\"\"\n",
        "    Calculate comprehensive readability statistics.\n",
        "    \"\"\"\n",
        "    scores = {}\n",
        "\n",
        "    # Calculate various readability metrics\n",
        "    fre_scores = [fk_metrics.flesch_reading_ease(text) for text in texts]\n",
        "    fkg_scores = [fk_metrics.flesch_kincaid_grade(text) for text in texts]\n",
        "\n",
        "    # Basic statistics\n",
        "    scores['avg_reading_ease'] = np.mean(fre_scores)\n",
        "    scores['std_reading_ease'] = np.std(fre_scores)\n",
        "    scores['avg_grade_level'] = np.mean(fkg_scores)\n",
        "    scores['std_grade_level'] = np.std(fkg_scores)\n",
        "\n",
        "    # Readability distribution\n",
        "    scores['very_easy_texts'] = sum(1 for s in fre_scores if s >= 90)\n",
        "    scores['easy_texts'] = sum(1 for s in fre_scores if 80 <= s < 90)\n",
        "    scores['standard_texts'] = sum(1 for s in fre_scores if 60 <= s < 80)\n",
        "    scores['difficult_texts'] = sum(1 for s in fre_scores if 30 <= s < 60)\n",
        "    scores['very_difficult_texts'] = sum(1 for s in fre_scores if s < 30)\n",
        "\n",
        "    # Grade level distribution\n",
        "    scores['elementary_level'] = sum(1 for g in fkg_scores if g <= 6)\n",
        "    scores['middle_school_level'] = sum(1 for g in fkg_scores if 6 < g <= 9)\n",
        "    scores['high_school_level'] = sum(1 for g in fkg_scores if 9 < g <= 12)\n",
        "    scores['college_level'] = sum(1 for g in fkg_scores if g > 12)\n",
        "\n",
        "    return scores\n",
        "\n",
        "# Simulate different text generation models\n",
        "model_outputs = {\n",
        "    \"Simple_Text_Generator\": {\n",
        "        \"texts\": [\n",
        "            \"The dog runs fast. He likes to play. The ball is red.\",\n",
        "            \"I go to school. My teacher is nice. We learn new things every day.\",\n",
        "            \"The sun is bright today. Birds fly in the sky. Trees are green.\"\n",
        "        ],\n",
        "        \"target_audience\": \"children\",\n",
        "        \"grade_range\": (1, 4)\n",
        "    },\n",
        "    \"Standard_Text_Generator\": {\n",
        "        \"texts\": [\n",
        "            \"The research demonstrates significant improvements in model performance.\",\n",
        "            \"Our analysis reveals interesting patterns in consumer behavior.\",\n",
        "            \"The implementation of new policies has resulted in measurable outcomes.\"\n",
        "        ],\n",
        "        \"target_audience\": \"general\",\n",
        "        \"grade_range\": (8, 12)\n",
        "    },\n",
        "    \"Academic_Text_Generator\": {\n",
        "        \"texts\": [\n",
        "            \"The epistemological implications of quantum mechanics necessitate a fundamental reconceptualization of deterministic paradigms.\",\n",
        "            \"Methodological innovations in computational linguistics facilitate unprecedented analytical capabilities.\",\n",
        "            \"The heterogeneous nature of socioeconomic variables complicates predictive modeling frameworks.\"\n",
        "        ],\n",
        "        \"target_audience\": \"academic\",\n",
        "        \"grade_range\": (14, 18)\n",
        "    }\n",
        "}\n",
        "\n",
        "# Evaluate each model\n",
        "evaluation_results = {}\n",
        "for model_name, config in model_outputs.items():\n",
        "    metrics, detailed = evaluate_text_readability(\n",
        "        model_name,\n",
        "        config[\"texts\"],\n",
        "        config[\"target_audience\"],\n",
        "        config[\"grade_range\"]\n",
        "    )\n",
        "    evaluation_results[model_name] = {\"metrics\": metrics, \"detailed\": detailed}\n",
        "\n",
        "print(\"âœ… Evaluation completed for all text generators!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEnDdRXymc0N"
      },
      "source": [
        "## 5. Comprehensive Results Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6kSQvCumc0O"
      },
      "outputs": [],
      "source": [
        "# Create comparison table\n",
        "comparison_data = []\n",
        "\n",
        "for model_name, results in evaluation_results.items():\n",
        "    config = model_outputs[model_name]\n",
        "    row = {\n",
        "        \"Model\": model_name,\n",
        "        \"Target Audience\": config[\"target_audience\"],\n",
        "        \"Target Grade\": f\"{config['grade_range'][0]}-{config['grade_range'][1]}\",\n",
        "        \"Avg Reading Ease\": results[\"detailed\"][\"avg_reading_ease\"],\n",
        "        \"Avg Grade Level\": results[\"detailed\"][\"avg_grade_level\"],\n",
        "        \"Std Grade Level\": results[\"detailed\"][\"std_grade_level\"]\n",
        "    }\n",
        "    comparison_data.append(row)\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "print(\"\\nðŸ“Š Model Comparison Results:\")\n",
        "print(\"=\"*90)\n",
        "print(comparison_df.round(2).to_string(index=False))\n",
        "\n",
        "# Analyze readability distribution\n",
        "print(\"\\nðŸ“ˆ Readability Distribution Analysis:\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "for model_name, results in evaluation_results.items():\n",
        "    detailed = results[\"detailed\"]\n",
        "    print(f\"\\n{model_name}:\")\n",
        "\n",
        "    # Reading ease categories\n",
        "    total_texts = sum([\n",
        "        detailed.get('very_easy_texts', 0),\n",
        "        detailed.get('easy_texts', 0),\n",
        "        detailed.get('standard_texts', 0),\n",
        "        detailed.get('difficult_texts', 0),\n",
        "        detailed.get('very_difficult_texts', 0)\n",
        "    ])\n",
        "\n",
        "    if total_texts > 0:\n",
        "        print(f\"  Very Easy: {detailed.get('very_easy_texts', 0)}/{total_texts}\")\n",
        "        print(f\"  Easy: {detailed.get('easy_texts', 0)}/{total_texts}\")\n",
        "        print(f\"  Standard: {detailed.get('standard_texts', 0)}/{total_texts}\")\n",
        "        print(f\"  Difficult: {detailed.get('difficult_texts', 0)}/{total_texts}\")\n",
        "        print(f\"  Very Difficult: {detailed.get('very_difficult_texts', 0)}/{total_texts}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s07clSeamc0O"
      },
      "source": [
        "## 6. Advanced Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRYLAfnzmc0O"
      },
      "outputs": [],
      "source": [
        "# Create comprehensive visualizations\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=2,\n",
        "    subplot_titles=(\n",
        "        \"Reading Ease by Category\",\n",
        "        \"Grade Level Distribution\",\n",
        "        \"Readability Heatmap\",\n",
        "        \"Grade Level vs Reading Ease\"\n",
        "    ),\n",
        "    specs=[\n",
        "        [{\"type\": \"bar\"}, {\"type\": \"box\"}],\n",
        "        [{\"type\": \"heatmap\"}, {\"type\": \"scatter\"}]\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Plot 1: Reading Ease by Category\n",
        "fig.add_trace(\n",
        "    go.Bar(\n",
        "        x=df_texts['category'],\n",
        "        y=df_texts['reading_ease'],\n",
        "        marker_color=['#e74c3c', '#f39c12', '#3498db', '#9b59b6', '#2ecc71', '#1abc9c'],\n",
        "        text=df_texts['reading_ease'].round(1),\n",
        "        textposition='outside'\n",
        "    ),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# Plot 2: Grade Level Distribution (Box Plot)\n",
        "all_grades = []\n",
        "categories = []\n",
        "for _, row in df_texts.iterrows():\n",
        "    # Simulate multiple samples per category\n",
        "    grades = [row['grade_level'] + np.random.normal(0, 0.5) for _ in range(10)]\n",
        "    all_grades.extend(grades)\n",
        "    categories.extend([row['category']] * 10)\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Box(\n",
        "        x=categories,\n",
        "        y=all_grades,\n",
        "        marker_color='#3498db'\n",
        "    ),\n",
        "    row=1, col=2\n",
        ")\n",
        "\n",
        "# Plot 3: Readability Heatmap\n",
        "# Create matrix of different readability metrics\n",
        "readability_matrix = []\n",
        "metric_names = ['Flesch-Kincaid', 'Gunning Fog', 'SMOG', 'Coleman-Liau']\n",
        "\n",
        "for _, row in df_texts.iterrows():\n",
        "    text_metrics = [\n",
        "        row['grade_level'],\n",
        "        textstat.gunning_fog(row['text']),\n",
        "        textstat.smog_index(row['text']) if len(sent_tokenize(row['text'])) >= 30 else 0,\n",
        "        textstat.coleman_liau_index(row['text'])\n",
        "    ]\n",
        "    readability_matrix.append(text_metrics)\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Heatmap(\n",
        "        z=np.array(readability_matrix).T,\n",
        "        x=df_texts['category'],\n",
        "        y=metric_names,\n",
        "        colorscale='RdYlGn_r',\n",
        "        text=np.round(np.array(readability_matrix).T, 1),\n",
        "        texttemplate=\"%{text}\",\n",
        "        textfont={\"size\": 10}\n",
        "    ),\n",
        "    row=2, col=1\n",
        ")\n",
        "\n",
        "# Plot 4: Grade Level vs Reading Ease Scatter\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x=df_texts['grade_level'],\n",
        "        y=df_texts['reading_ease'],\n",
        "        mode='markers+text',\n",
        "        marker=dict(\n",
        "            size=df_texts['word_count']/5,\n",
        "            color=df_texts['grade_level'],\n",
        "            colorscale='Viridis',\n",
        "            showscale=True,\n",
        "            colorbar=dict(x=1.15)\n",
        "        ),\n",
        "        text=df_texts['category'],\n",
        "        textposition=\"top center\"\n",
        "    ),\n",
        "    row=2, col=2\n",
        ")\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(\n",
        "    title_text=\"Comprehensive Readability Analysis\",\n",
        "    showlegend=False,\n",
        "    height=800,\n",
        "    width=1200\n",
        ")\n",
        "\n",
        "fig.update_xaxes(title_text=\"Category\", row=1, col=1)\n",
        "fig.update_yaxes(title_text=\"Reading Ease Score\", row=1, col=1)\n",
        "fig.update_yaxes(title_text=\"Grade Level\", row=1, col=2)\n",
        "fig.update_xaxes(title_text=\"Category\", row=2, col=1)\n",
        "fig.update_xaxes(title_text=\"Grade Level\", row=2, col=2)\n",
        "fig.update_yaxes(title_text=\"Reading Ease\", row=2, col=2)\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7TfV6d-mc0O"
      },
      "source": [
        "## 7. Text Simplification Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFtypQI2mc0O"
      },
      "outputs": [],
      "source": [
        "def simplify_text_analysis(original_text: str, simplified_versions: List[str]):\n",
        "    \"\"\"\n",
        "    Analyze the effectiveness of text simplification.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Analyze original\n",
        "    orig_fre = fk_metrics.flesch_reading_ease(original_text)\n",
        "    orig_fkg = fk_metrics.flesch_kincaid_grade(original_text)\n",
        "\n",
        "    results.append({\n",
        "        'version': 'Original',\n",
        "        'text': original_text[:100] + '...',\n",
        "        'reading_ease': orig_fre,\n",
        "        'grade_level': orig_fkg,\n",
        "        'improvement': 0\n",
        "    })\n",
        "\n",
        "    # Analyze simplified versions\n",
        "    for i, simplified in enumerate(simplified_versions, 1):\n",
        "        simp_fre = fk_metrics.flesch_reading_ease(simplified)\n",
        "        simp_fkg = fk_metrics.flesch_kincaid_grade(simplified)\n",
        "\n",
        "        results.append({\n",
        "            'version': f'Simplified {i}',\n",
        "            'text': simplified[:100] + '...',\n",
        "            'reading_ease': simp_fre,\n",
        "            'grade_level': simp_fkg,\n",
        "            'improvement': simp_fre - orig_fre\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Example: Simplifying a complex academic text\n",
        "complex_text = \"\"\"The implementation of sophisticated machine learning algorithms has\n",
        "revolutionized the methodological approaches utilized in contemporary computational\n",
        "linguistics. Through the systematic application of transformer-based architectures,\n",
        "researchers have demonstrated unprecedented capabilities in natural language\n",
        "understanding and generation.\"\"\"\n",
        "\n",
        "simplified_versions = [\n",
        "    \"\"\"Machine learning has changed how we study language with computers.\n",
        "    New AI models called transformers help computers understand and create\n",
        "    human language better than ever before.\"\"\",\n",
        "\n",
        "    \"\"\"Smart computer programs now help us work with language. These programs\n",
        "    can read and write like humans. This is a big step forward.\"\"\",\n",
        "\n",
        "    \"\"\"Computers can now understand language much better. They use new tools\n",
        "    to read and write text. This helps in many ways.\"\"\"\n",
        "]\n",
        "\n",
        "simplification_df = simplify_text_analysis(complex_text, simplified_versions)\n",
        "\n",
        "print(\"\\nðŸ“ Text Simplification Analysis:\")\n",
        "print(\"=\"*80)\n",
        "print(simplification_df[['version', 'reading_ease', 'grade_level', 'improvement']].round(2).to_string(index=False))\n",
        "\n",
        "# Visualize simplification effectiveness\n",
        "fig_simp = go.Figure()\n",
        "\n",
        "fig_simp.add_trace(go.Bar(\n",
        "    name='Reading Ease',\n",
        "    x=simplification_df['version'],\n",
        "    y=simplification_df['reading_ease'],\n",
        "    marker_color='lightblue',\n",
        "    yaxis='y',\n",
        "))\n",
        "\n",
        "fig_simp.add_trace(go.Scatter(\n",
        "    name='Grade Level',\n",
        "    x=simplification_df['version'],\n",
        "    y=simplification_df['grade_level'],\n",
        "    mode='lines+markers',\n",
        "    marker_color='red',\n",
        "    yaxis='y2'\n",
        "))\n",
        "\n",
        "fig_simp.update_layout(\n",
        "    title='Text Simplification Effectiveness',\n",
        "    xaxis=dict(title='Version'),\n",
        "    yaxis=dict(title='Reading Ease Score', side='left'),\n",
        "    yaxis2=dict(title='Grade Level', overlaying='y', side='right'),\n",
        "    hovermode='x unified',\n",
        "    height=400\n",
        ")\n",
        "\n",
        "fig_simp.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58k1bZvamc0P"
      },
      "source": [
        "## 8. Production-Ready Readability Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5cH24aUmc0P"
      },
      "outputs": [],
      "source": [
        "class ReadabilityEvaluationPipeline:\n",
        "    \"\"\"\n",
        "    Production-ready pipeline for text readability evaluation with MLflow.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, experiment_name=\"readability-evaluation\", tracking_uri=None):\n",
        "        self.experiment_name = experiment_name\n",
        "        if tracking_uri:\n",
        "            mlflow.set_tracking_uri(tracking_uri)\n",
        "        mlflow.set_experiment(experiment_name)\n",
        "\n",
        "        self.fk_metrics = FleschKincaidMetrics()\n",
        "        self.metrics = {\n",
        "            \"reading_ease\": self.fk_metrics.create_reading_ease_metric(),\n",
        "            \"grade_level\": self.fk_metrics.create_grade_level_metric(),\n",
        "            \"comprehensive\": self.fk_metrics.create_comprehensive_readability_metric()\n",
        "        }\n",
        "\n",
        "    def evaluate_content(self,\n",
        "                        content_name: str,\n",
        "                        texts: List[str],\n",
        "                        target_audience: str = \"general\",\n",
        "                        metadata: Dict = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Evaluate content readability with comprehensive metrics.\n",
        "        \"\"\"\n",
        "        with mlflow.start_run(run_name=content_name):\n",
        "            # Log metadata\n",
        "            mlflow.log_param(\"content_name\", content_name)\n",
        "            mlflow.log_param(\"num_texts\", len(texts))\n",
        "            mlflow.log_param(\"target_audience\", target_audience)\n",
        "\n",
        "            if metadata:\n",
        "                for key, value in metadata.items():\n",
        "                    mlflow.log_param(key, value)\n",
        "\n",
        "            # Calculate comprehensive metrics\n",
        "            results = self._calculate_all_metrics(texts)\n",
        "\n",
        "            # Log metrics\n",
        "            for metric_name, metric_value in results[\"aggregate\"].items():\n",
        "                mlflow.log_metric(metric_name, metric_value)\n",
        "\n",
        "            # Generate and log report\n",
        "            report = self._generate_readability_report(texts, results, target_audience)\n",
        "            mlflow.log_dict(report, \"readability_report.json\")\n",
        "\n",
        "            # Log recommendations\n",
        "            recommendations = self._generate_recommendations(results, target_audience)\n",
        "            mlflow.log_text(\"\\n\".join(recommendations), \"recommendations.txt\")\n",
        "\n",
        "            return results\n",
        "\n",
        "    def _calculate_all_metrics(self, texts: List[str]) -> Dict:\n",
        "        \"\"\"\n",
        "        Calculate all readability metrics.\n",
        "        \"\"\"\n",
        "        results = {\n",
        "            \"text_level\": [],\n",
        "            \"aggregate\": {},\n",
        "            \"distribution\": {}\n",
        "        }\n",
        "\n",
        "        for text in texts:\n",
        "            text_metrics = {\n",
        "                'flesch_reading_ease': self.fk_metrics.flesch_reading_ease(text),\n",
        "                'flesch_kincaid_grade': self.fk_metrics.flesch_kincaid_grade(text),\n",
        "                'gunning_fog': textstat.gunning_fog(text),\n",
        "                'smog_index': textstat.smog_index(text) if len(sent_tokenize(text)) >= 30 else 0,\n",
        "                'coleman_liau': textstat.coleman_liau_index(text),\n",
        "                'ari': textstat.automated_readability_index(text),\n",
        "                'dale_chall': textstat.dale_chall_readability_score(text),\n",
        "                'sentence_count': len(sent_tokenize(text)),\n",
        "                'word_count': len(word_tokenize(text)),\n",
        "                'syllable_count': sum(self.fk_metrics.count_syllables(w)\n",
        "                                     for w in word_tokenize(text) if w.isalnum())\n",
        "            }\n",
        "            results[\"text_level\"].append(text_metrics)\n",
        "\n",
        "        # Calculate aggregates\n",
        "        for metric_name in results[\"text_level\"][0].keys():\n",
        "            values = [t[metric_name] for t in results[\"text_level\"]]\n",
        "            results[\"aggregate\"][f\"{metric_name}_mean\"] = np.mean(values)\n",
        "            results[\"aggregate\"][f\"{metric_name}_std\"] = np.std(values)\n",
        "            results[\"aggregate\"][f\"{metric_name}_min\"] = np.min(values)\n",
        "            results[\"aggregate\"][f\"{metric_name}_max\"] = np.max(values)\n",
        "\n",
        "        # Calculate distribution\n",
        "        fre_scores = [t['flesch_reading_ease'] for t in results[\"text_level\"]]\n",
        "        results[\"distribution\"][\"very_easy\"] = sum(1 for s in fre_scores if s >= 90)\n",
        "        results[\"distribution\"][\"easy\"] = sum(1 for s in fre_scores if 80 <= s < 90)\n",
        "        results[\"distribution\"][\"fairly_easy\"] = sum(1 for s in fre_scores if 70 <= s < 80)\n",
        "        results[\"distribution\"][\"standard\"] = sum(1 for s in fre_scores if 60 <= s < 70)\n",
        "        results[\"distribution\"][\"fairly_difficult\"] = sum(1 for s in fre_scores if 50 <= s < 60)\n",
        "        results[\"distribution\"][\"difficult\"] = sum(1 for s in fre_scores if 30 <= s < 50)\n",
        "        results[\"distribution\"][\"very_difficult\"] = sum(1 for s in fre_scores if s < 30)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _generate_readability_report(self, texts: List[str], results: Dict,\n",
        "                                    target_audience: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Generate comprehensive readability report.\n",
        "        \"\"\"\n",
        "        report = {\n",
        "            \"summary\": {\n",
        "                \"total_texts\": len(texts),\n",
        "                \"target_audience\": target_audience,\n",
        "                \"avg_reading_ease\": results[\"aggregate\"][\"flesch_reading_ease_mean\"],\n",
        "                \"avg_grade_level\": results[\"aggregate\"][\"flesch_kincaid_grade_mean\"],\n",
        "                \"readability_category\": self._get_readability_category(\n",
        "                    results[\"aggregate\"][\"flesch_reading_ease_mean\"]\n",
        "                )\n",
        "            },\n",
        "            \"distribution\": results[\"distribution\"],\n",
        "            \"detailed_metrics\": results[\"aggregate\"],\n",
        "            \"sample_analysis\": [\n",
        "                {\n",
        "                    \"text_preview\": text[:200] + \"...\",\n",
        "                    \"metrics\": metrics\n",
        "                }\n",
        "                for text, metrics in zip(texts[:3], results[\"text_level\"][:3])\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        return report\n",
        "\n",
        "    def _get_readability_category(self, reading_ease: float) -> str:\n",
        "        \"\"\"\n",
        "        Get readability category from Flesch Reading Ease score.\n",
        "        \"\"\"\n",
        "        if reading_ease >= 90:\n",
        "            return \"Very Easy\"\n",
        "        elif reading_ease >= 80:\n",
        "            return \"Easy\"\n",
        "        elif reading_ease >= 70:\n",
        "            return \"Fairly Easy\"\n",
        "        elif reading_ease >= 60:\n",
        "            return \"Standard\"\n",
        "        elif reading_ease >= 50:\n",
        "            return \"Fairly Difficult\"\n",
        "        elif reading_ease >= 30:\n",
        "            return \"Difficult\"\n",
        "        else:\n",
        "            return \"Very Difficult\"\n",
        "\n",
        "    def _generate_recommendations(self, results: Dict, target_audience: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Generate recommendations based on readability analysis.\n",
        "        \"\"\"\n",
        "        recommendations = []\n",
        "\n",
        "        avg_fre = results[\"aggregate\"][\"flesch_reading_ease_mean\"]\n",
        "        avg_grade = results[\"aggregate\"][\"flesch_kincaid_grade_mean\"]\n",
        "\n",
        "        # Target audience recommendations\n",
        "        target_ranges = {\n",
        "            \"children\": (90, 100, 1, 5),\n",
        "            \"young_adult\": (70, 90, 6, 9),\n",
        "            \"general\": (60, 80, 8, 12),\n",
        "            \"academic\": (30, 60, 13, 18),\n",
        "            \"professional\": (40, 70, 10, 16)\n",
        "        }\n",
        "\n",
        "        if target_audience in target_ranges:\n",
        "            fre_min, fre_max, grade_min, grade_max = target_ranges[target_audience]\n",
        "\n",
        "            if avg_fre < fre_min:\n",
        "                recommendations.append(\n",
        "                    f\"Content is too difficult for {target_audience} audience. \"\n",
        "                    f\"Consider simplifying sentences and using more common words.\"\n",
        "                )\n",
        "            elif avg_fre > fre_max:\n",
        "                recommendations.append(\n",
        "                    f\"Content may be too simple for {target_audience} audience. \"\n",
        "                    f\"Consider adding more sophisticated vocabulary and complex sentences.\"\n",
        "                )\n",
        "\n",
        "            if avg_grade < grade_min:\n",
        "                recommendations.append(\n",
        "                    f\"Grade level ({avg_grade:.1f}) is below target range ({grade_min}-{grade_max}).\"\n",
        "                )\n",
        "            elif avg_grade > grade_max:\n",
        "                recommendations.append(\n",
        "                    f\"Grade level ({avg_grade:.1f}) is above target range ({grade_min}-{grade_max}).\"\n",
        "                )\n",
        "\n",
        "        # General recommendations\n",
        "        if results[\"aggregate\"][\"word_count_mean\"] / results[\"aggregate\"][\"sentence_count_mean\"] > 20:\n",
        "            recommendations.append(\n",
        "                \"Average sentence length is high. Consider breaking long sentences into shorter ones.\"\n",
        "            )\n",
        "\n",
        "        if results[\"aggregate\"][\"syllable_count_mean\"] / results[\"aggregate\"][\"word_count_mean\"] > 2:\n",
        "            recommendations.append(\n",
        "                \"Words have high syllable count. Consider using simpler vocabulary.\"\n",
        "            )\n",
        "\n",
        "        if not recommendations:\n",
        "            recommendations.append(f\"Content readability is appropriate for {target_audience} audience.\")\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "    def compare_versions(self, versions: Dict[str, List[str]]) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Compare readability across multiple content versions.\n",
        "        \"\"\"\n",
        "        comparison_results = []\n",
        "\n",
        "        for version_name, texts in versions.items():\n",
        "            results = self._calculate_all_metrics(texts)\n",
        "\n",
        "            row = {\n",
        "                \"Version\": version_name,\n",
        "                \"Reading Ease\": results[\"aggregate\"][\"flesch_reading_ease_mean\"],\n",
        "                \"Grade Level\": results[\"aggregate\"][\"flesch_kincaid_grade_mean\"],\n",
        "                \"Gunning Fog\": results[\"aggregate\"][\"gunning_fog_mean\"],\n",
        "                \"SMOG\": results[\"aggregate\"][\"smog_index_mean\"],\n",
        "                \"Category\": self._get_readability_category(\n",
        "                    results[\"aggregate\"][\"flesch_reading_ease_mean\"]\n",
        "                )\n",
        "            }\n",
        "            comparison_results.append(row)\n",
        "\n",
        "        return pd.DataFrame(comparison_results)\n",
        "\n",
        "# Demonstrate the pipeline\n",
        "pipeline = ReadabilityEvaluationPipeline(experiment_name=\"readability-production-pipeline\")\n",
        "\n",
        "# Test with different content versions\n",
        "content_versions = {\n",
        "    \"Original\": [df_texts.iloc[3]['text']],  # Academic paper\n",
        "    \"Simplified_v1\": [simplified_versions[0]],\n",
        "    \"Simplified_v2\": [simplified_versions[1]],\n",
        "    \"Simplified_v3\": [simplified_versions[2]]\n",
        "}\n",
        "\n",
        "# Compare versions\n",
        "version_comparison = pipeline.compare_versions(content_versions)\n",
        "\n",
        "print(\"\\nðŸš€ Production Pipeline Results:\")\n",
        "print(\"=\"*90)\n",
        "print(version_comparison.round(2).to_string(index=False))\n",
        "\n",
        "# Identify best version for general audience\n",
        "general_target = version_comparison[\n",
        "    (version_comparison['Reading Ease'] >= 60) &\n",
        "    (version_comparison['Reading Ease'] <= 80)\n",
        "]\n",
        "\n",
        "if not general_target.empty:\n",
        "    best_version = general_target.iloc[0]\n",
        "    print(f\"\\nðŸ† Best Version for General Audience: {best_version['Version']}\")\n",
        "    print(f\"   - Reading Ease: {best_version['Reading Ease']:.1f}\")\n",
        "    print(f\"   - Grade Level: {best_version['Grade Level']:.1f}\")\n",
        "    print(f\"   - Category: {best_version['Category']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzdqkaC_mc0P"
      },
      "source": [
        "## 9. A/B Testing for Content Readability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuz_0dg1mc0Q"
      },
      "outputs": [],
      "source": [
        "def ab_test_readability(version_a: List[str], version_b: List[str],\n",
        "                        confidence_level: float = 0.95) -> Dict:\n",
        "    \"\"\"\n",
        "    Perform A/B testing on content readability.\n",
        "    \"\"\"\n",
        "    # Calculate metrics for both versions\n",
        "    fk = FleschKincaidMetrics()\n",
        "\n",
        "    scores_a = [fk.flesch_reading_ease(text) for text in version_a]\n",
        "    scores_b = [fk.flesch_reading_ease(text) for text in version_b]\n",
        "\n",
        "    grades_a = [fk.flesch_kincaid_grade(text) for text in version_a]\n",
        "    grades_b = [fk.flesch_kincaid_grade(text) for text in version_b]\n",
        "\n",
        "    # Perform t-tests\n",
        "    reading_ease_ttest = stats.ttest_ind(scores_a, scores_b)\n",
        "    grade_level_ttest = stats.ttest_ind(grades_a, grades_b)\n",
        "\n",
        "    # Calculate effect sizes (Cohen's d)\n",
        "    def cohens_d(group1, group2):\n",
        "        n1, n2 = len(group1), len(group2)\n",
        "        var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)\n",
        "        pooled_std = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))\n",
        "        return (np.mean(group1) - np.mean(group2)) / pooled_std\n",
        "\n",
        "    reading_ease_effect = cohens_d(scores_a, scores_b)\n",
        "    grade_level_effect = cohens_d(grades_a, grades_b)\n",
        "\n",
        "    results = {\n",
        "        \"version_a\": {\n",
        "            \"reading_ease_mean\": np.mean(scores_a),\n",
        "            \"reading_ease_std\": np.std(scores_a),\n",
        "            \"grade_level_mean\": np.mean(grades_a),\n",
        "            \"grade_level_std\": np.std(grades_a)\n",
        "        },\n",
        "        \"version_b\": {\n",
        "            \"reading_ease_mean\": np.mean(scores_b),\n",
        "            \"reading_ease_std\": np.std(scores_b),\n",
        "            \"grade_level_mean\": np.mean(grades_b),\n",
        "            \"grade_level_std\": np.std(grades_b)\n",
        "        },\n",
        "        \"statistical_tests\": {\n",
        "            \"reading_ease_p_value\": reading_ease_ttest.pvalue,\n",
        "            \"reading_ease_significant\": reading_ease_ttest.pvalue < (1 - confidence_level),\n",
        "            \"reading_ease_effect_size\": reading_ease_effect,\n",
        "            \"grade_level_p_value\": grade_level_ttest.pvalue,\n",
        "            \"grade_level_significant\": grade_level_ttest.pvalue < (1 - confidence_level),\n",
        "            \"grade_level_effect_size\": grade_level_effect\n",
        "        },\n",
        "        \"recommendation\": \"\"\n",
        "    }\n",
        "\n",
        "    # Generate recommendation\n",
        "    if results[\"statistical_tests\"][\"reading_ease_significant\"]:\n",
        "        if np.mean(scores_a) > np.mean(scores_b):\n",
        "            results[\"recommendation\"] = \"Version A is significantly easier to read\"\n",
        "        else:\n",
        "            results[\"recommendation\"] = \"Version B is significantly easier to read\"\n",
        "    else:\n",
        "        results[\"recommendation\"] = \"No significant difference in readability\"\n",
        "\n",
        "    return results\n",
        "\n",
        "# Create test versions\n",
        "version_a = [\n",
        "    \"The research shows that exercise improves health. Regular activity helps prevent disease.\",\n",
        "    \"Scientists found new ways to treat cancer. These methods work better than old ones.\",\n",
        "    \"Climate change affects weather patterns. We need to reduce carbon emissions.\"\n",
        "]\n",
        "\n",
        "version_b = [\n",
        "    \"Contemporary investigations demonstrate that physical activity enhances physiological wellbeing.\",\n",
        "    \"Researchers identified innovative therapeutic modalities for oncological interventions.\",\n",
        "    \"Anthropogenic climate modifications influence meteorological phenomena globally.\"\n",
        "]\n",
        "\n",
        "# Run A/B test\n",
        "ab_results = ab_test_readability(version_a, version_b)\n",
        "\n",
        "print(\"\\nðŸ”¬ A/B Testing Results:\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nVersion A:\")\n",
        "print(f\"  Reading Ease: {ab_results['version_a']['reading_ease_mean']:.1f} Â± {ab_results['version_a']['reading_ease_std']:.1f}\")\n",
        "print(f\"  Grade Level: {ab_results['version_a']['grade_level_mean']:.1f} Â± {ab_results['version_a']['grade_level_std']:.1f}\")\n",
        "\n",
        "print(f\"\\nVersion B:\")\n",
        "print(f\"  Reading Ease: {ab_results['version_b']['reading_ease_mean']:.1f} Â± {ab_results['version_b']['reading_ease_std']:.1f}\")\n",
        "print(f\"  Grade Level: {ab_results['version_b']['grade_level_mean']:.1f} Â± {ab_results['version_b']['grade_level_std']:.1f}\")\n",
        "\n",
        "print(f\"\\nStatistical Analysis:\")\n",
        "print(f\"  Reading Ease p-value: {ab_results['statistical_tests']['reading_ease_p_value']:.4f}\")\n",
        "print(f\"  Reading Ease Effect Size: {ab_results['statistical_tests']['reading_ease_effect_size']:.2f}\")\n",
        "print(f\"  Grade Level p-value: {ab_results['statistical_tests']['grade_level_p_value']:.4f}\")\n",
        "print(f\"  Grade Level Effect Size: {ab_results['statistical_tests']['grade_level_effect_size']:.2f}\")\n",
        "\n",
        "print(f\"\\nðŸ“Š Recommendation: {ab_results['recommendation']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AueguvBvmc0Q"
      },
      "source": [
        "## 10. Best Practices and Guidelines\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "1. **Understanding Flesch-Kincaid Metrics**:\n",
        "   - **Reading Ease**: 0-100 scale (higher = easier)\n",
        "   - **Grade Level**: U.S. education grade equivalent\n",
        "   - Both metrics consider sentence length and syllable count\n",
        "   - Complementary metrics provide fuller picture\n",
        "\n",
        "2. **Target Audience Guidelines**:\n",
        "   - **General Public**: 60-70 Reading Ease, Grade 8-9\n",
        "   - **Children**: 90+ Reading Ease, Grade 1-5\n",
        "   - **Young Adults**: 70-80 Reading Ease, Grade 6-8\n",
        "   - **College**: 30-50 Reading Ease, Grade 13+\n",
        "   - **Academic**: 0-30 Reading Ease, Grade 16+\n",
        "\n",
        "3. **Content Optimization Strategies**:\n",
        "   - **Simplify**: Shorter sentences, common words\n",
        "   - **Structure**: Clear paragraphs, logical flow\n",
        "   - **Vocabulary**: Match audience expectations\n",
        "   - **Testing**: A/B test different versions\n",
        "\n",
        "4. **MLflow Integration Benefits**:\n",
        "   - Track readability changes over time\n",
        "   - Compare content versions systematically\n",
        "   - Automate readability monitoring\n",
        "   - Generate actionable recommendations\n",
        "\n",
        "5. **Production Deployment**:\n",
        "   - Set readability thresholds for content\n",
        "   - Automate content review workflows\n",
        "   - Monitor readability drift\n",
        "   - Generate alerts for out-of-range content\n",
        "\n",
        "### Common Pitfalls to Avoid:\n",
        "- Over-simplification losing meaning\n",
        "- Ignoring audience expertise level\n",
        "- Focusing on single metric only\n",
        "- Not considering content purpose\n",
        "- Neglecting cultural/regional differences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExS_Yr83mc0Q"
      },
      "outputs": [],
      "source": [
        "# Final summary and MLflow UI instructions\n",
        "print(\"\\nðŸ“ˆ MLflow Tracking Summary:\")\n",
        "print(\"=\"*60)\n",
        "print(\"To view all experiments and metrics in MLflow UI:\")\n",
        "print(\"\\n1. Run in terminal:\")\n",
        "print(\"   mlflow ui --port 5000\")\n",
        "print(\"\\n2. Open browser:\")\n",
        "print(\"   http://localhost:5000\")\n",
        "print(\"\\n3. Navigate to experiments:\")\n",
        "print(\"   - flesch-kincaid-metrics-demo\")\n",
        "print(\"   - readability-production-pipeline\")\n",
        "print(\"\\nâœ… Demo completed successfully!\")\n",
        "print(\"\\nðŸ”— Additional Resources:\")\n",
        "print(\"   - Flesch Reading Ease: https://en.wikipedia.org/wiki/Flesch_reading_ease\")\n",
        "print(\"   - Textstat Library: https://github.com/shivam5992/textstat\")\n",
        "print(\"   - Plain Language Guidelines: https://www.plainlanguage.gov/\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}