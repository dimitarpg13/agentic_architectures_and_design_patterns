{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Application with Live Web Search using DuckDuckGo and LangGraph\n",
    "\n",
    "This notebook demonstrates a hybrid RAG (Retrieval-Augmented Generation) system that combines:\n",
    "- **Vector Store RAG**: Retrieve from your private knowledge base\n",
    "- **Live Web Search**: Get current information from DuckDuckGo\n",
    "- **LangGraph Orchestration**: Intelligent routing and workflow management\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "User Query ‚Üí Router Agent ‚Üí [Vector Store Retrieval] + [Web Search]\n",
    "                                        ‚Üì\n",
    "                               Hybrid Context Builder\n",
    "                                        ‚Üì\n",
    "                                  LLM Generator\n",
    "                                        ‚Üì\n",
    "                                  Final Answer\n",
    "```\n",
    "\n",
    "## Features\n",
    "- üîç Intelligent query routing (local docs vs web search)\n",
    "- üìö Vector store for private documents\n",
    "- üåê Live web search for current information\n",
    "- üîÑ Hybrid retrieval combining both sources\n",
    "- üéØ Context-aware answer generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q langchain langchain-openai langgraph langchain-community \\\n",
    "    duckduckgo-search chromadb sentence-transformers pypdf \\\n",
    "    tiktoken faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import TypedDict, Annotated, List, Dict, Any, Literal\n",
    "import operator\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Vector store imports\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set Up API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "# Or load from environment\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Sample Documents for RAG Knowledge Base\n",
    "\n",
    "In a real application, you'd load your own documents. For this demo, we'll create sample documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents for the knowledge base\n",
    "sample_documents = [\n",
    "    Document(\n",
    "        page_content=\"\"\"LangChain is a framework for developing applications powered by language models. \n",
    "        It provides tools for prompt management, chains, agents, and memory. LangChain supports \n",
    "        multiple LLM providers including OpenAI, Anthropic, and Hugging Face.\"\"\",\n",
    "        metadata={\"source\": \"langchain_docs\", \"topic\": \"framework\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"LangGraph is a library for building stateful, multi-actor applications with LLMs. \n",
    "        It extends LangChain with graph-based orchestration capabilities, allowing developers to \n",
    "        create complex workflows with cycles, conditional branching, and state management.\"\"\",\n",
    "        metadata={\"source\": \"langgraph_docs\", \"topic\": \"framework\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"RAG (Retrieval-Augmented Generation) is a technique that combines information \n",
    "        retrieval with text generation. It retrieves relevant documents from a knowledge base and \n",
    "        uses them as context for generating more accurate and informed responses.\"\"\",\n",
    "        metadata={\"source\": \"rag_guide\", \"topic\": \"technique\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"Vector databases store embeddings and enable semantic search. Popular options \n",
    "        include Chroma, Pinecone, Weaviate, and FAISS. They use similarity metrics like cosine \n",
    "        similarity to find relevant documents based on semantic meaning rather than exact matches.\"\"\",\n",
    "        metadata={\"source\": \"vector_db_guide\", \"topic\": \"database\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"DuckDuckGo is a privacy-focused search engine that doesn't track users. \n",
    "        It can be integrated into applications for web search capabilities without requiring \n",
    "        an API key, making it ideal for prototyping and development.\"\"\",\n",
    "        metadata={\"source\": \"search_engines\", \"topic\": \"tools\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"Our company policy on remote work: Employees can work remotely up to 3 days per week. \n",
    "        Core hours are 10 AM - 3 PM local time. All team meetings should be scheduled during core hours. \n",
    "        Remote work equipment is provided by the company.\"\"\",\n",
    "        metadata={\"source\": \"company_handbook\", \"topic\": \"policy\", \"date\": \"2024\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"Machine learning embeddings are dense vector representations of text that capture \n",
    "        semantic meaning. Popular embedding models include OpenAI's text-embedding-ada-002, \n",
    "        sentence-transformers, and Google's Universal Sentence Encoder.\"\"\",\n",
    "        metadata={\"source\": \"ml_guide\", \"topic\": \"embeddings\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Created {len(sample_documents)} sample documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Set Up Vector Store with Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings\n",
    "# Option 1: OpenAI embeddings (requires API key, higher quality)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Option 2: Free local embeddings (no API key needed)\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "print(\"‚úÖ Embeddings model initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store from documents\n",
    "# Option 1: Chroma (persistent, easy to use)\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=sample_documents,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"rag_knowledge_base\",\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "# Option 2: FAISS (in-memory, very fast)\n",
    "# vectorstore = FAISS.from_documents(sample_documents, embeddings)\n",
    "\n",
    "print(f\"‚úÖ Vector store created with {vectorstore._collection.count()} documents\")\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}  # Return top 3 most relevant documents\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Retriever configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Vector Store Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test retrieval\n",
    "test_query = \"What is RAG?\"\n",
    "docs = retriever.invoke(test_query)\n",
    "\n",
    "print(f\"Query: {test_query}\\n\")\n",
    "print(\"Retrieved documents:\")\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"\\n{i}. {doc.page_content[:150]}...\")\n",
    "    print(f\"   Source: {doc.metadata.get('source', 'unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialize DuckDuckGo Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DuckDuckGo search\n",
    "web_search = DuckDuckGoSearchResults(\n",
    "    num_results=3,\n",
    "    output_format=\"list\"\n",
    ")\n",
    "\n",
    "# Test search\n",
    "test_results = web_search.run(\"latest AI news 2024\")\n",
    "print(\"‚úÖ DuckDuckGo search initialized\")\n",
    "print(f\"Test search returned {len(test_results) if isinstance(test_results, list) else 1} results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Define RAG State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGState(TypedDict):\n",
    "    \"\"\"State for the RAG workflow.\"\"\"\n",
    "    # Input\n",
    "    query: str\n",
    "    \n",
    "    # Routing decision\n",
    "    use_web_search: bool\n",
    "    use_vector_store: bool\n",
    "    \n",
    "    # Retrieved context\n",
    "    vector_docs: List[Document]\n",
    "    web_results: List[Dict[str, Any]]\n",
    "    \n",
    "    # Combined context\n",
    "    combined_context: str\n",
    "    \n",
    "    # Output\n",
    "    answer: str\n",
    "    sources: List[str]\n",
    "\n",
    "print(\"‚úÖ RAG state defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create Router Node (Decides What to Retrieve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "\n",
    "def route_query(state: RAGState) -> RAGState:\n",
    "    \"\"\"\n",
    "    Determine whether to use vector store, web search, or both.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üîÄ ROUTER NODE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    query = state[\"query\"]\n",
    "    \n",
    "    # Create routing prompt\n",
    "    router_prompt = f\"\"\"Analyze this query and determine the best retrieval strategy:\n",
    "\n",
    "Query: \"{query}\"\n",
    "\n",
    "Decide:\n",
    "1. use_vector_store: True if the query is about internal knowledge, documentation, or company policies\n",
    "2. use_web_search: True if the query requires current/recent information, news, or real-time data\n",
    "\n",
    "Note: Both can be True for queries needing both internal and external information.\n",
    "\n",
    "Examples:\n",
    "- \"What is our remote work policy?\" ‚Üí vector_store: True, web_search: False\n",
    "- \"Latest AI news\" ‚Üí vector_store: False, web_search: True\n",
    "- \"How does LangChain compare to latest AI frameworks?\" ‚Üí vector_store: True, web_search: True\n",
    "\n",
    "Respond with ONLY a JSON object: {{\"use_vector_store\": true/false, \"use_web_search\": true/false}}\"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=router_prompt)])\n",
    "    \n",
    "    # Parse response\n",
    "    import json\n",
    "    try:\n",
    "        decision = json.loads(response.content)\n",
    "        use_vector = decision.get(\"use_vector_store\", True)\n",
    "        use_web = decision.get(\"use_web_search\", False)\n",
    "    except:\n",
    "        # Default fallback\n",
    "        use_vector = True\n",
    "        use_web = False\n",
    "    \n",
    "    print(f\"üìä Routing Decision:\")\n",
    "    print(f\"   Vector Store: {use_vector}\")\n",
    "    print(f\"   Web Search: {use_web}\")\n",
    "    \n",
    "    return {\n",
    "        \"use_vector_store\": use_vector,\n",
    "        \"use_web_search\": use_web\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Router node created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Create Vector Store Retrieval Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_from_vectorstore(state: RAGState) -> RAGState:\n",
    "    \"\"\"\n",
    "    Retrieve relevant documents from vector store.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìö VECTOR STORE RETRIEVAL NODE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not state.get(\"use_vector_store\", False):\n",
    "        print(\"‚è≠Ô∏è  Skipping vector store retrieval\")\n",
    "        return {\"vector_docs\": []}\n",
    "    \n",
    "    query = state[\"query\"]\n",
    "    \n",
    "    try:\n",
    "        # Retrieve documents\n",
    "        docs = retriever.invoke(query)\n",
    "        \n",
    "        print(f\"‚úÖ Retrieved {len(docs)} documents from vector store\")\n",
    "        for i, doc in enumerate(docs, 1):\n",
    "            print(f\"   {i}. {doc.metadata.get('source', 'unknown')} - {doc.page_content[:60]}...\")\n",
    "        \n",
    "        return {\"vector_docs\": docs}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error retrieving from vector store: {e}\")\n",
    "        return {\"vector_docs\": []}\n",
    "\n",
    "print(\"‚úÖ Vector retrieval node created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Create Web Search Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_web(state: RAGState) -> RAGState:\n",
    "    \"\"\"\n",
    "    Search the web using DuckDuckGo.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üåê WEB SEARCH NODE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not state.get(\"use_web_search\", False):\n",
    "        print(\"‚è≠Ô∏è  Skipping web search\")\n",
    "        return {\"web_results\": []}\n",
    "    \n",
    "    query = state[\"query\"]\n",
    "    \n",
    "    try:\n",
    "        # Perform web search\n",
    "        raw_results = web_search.run(query)\n",
    "        \n",
    "        # Parse results\n",
    "        results = []\n",
    "        if isinstance(raw_results, str):\n",
    "            # Parse string results\n",
    "            snippets = raw_results.split('snippet: ')\n",
    "            for snippet in snippets[1:]:\n",
    "                parts = snippet.split('title: ')\n",
    "                if len(parts) > 1:\n",
    "                    title = parts[1].split('link: ')[0].strip()\n",
    "                    link = parts[1].split('link: ')[1].strip() if 'link: ' in parts[1] else \"\"\n",
    "                    results.append({\n",
    "                        \"title\": title,\n",
    "                        \"snippet\": parts[0].strip(),\n",
    "                        \"url\": link\n",
    "                    })\n",
    "        else:\n",
    "            results = raw_results if isinstance(raw_results, list) else []\n",
    "        \n",
    "        print(f\"‚úÖ Found {len(results)} web results\")\n",
    "        for i, result in enumerate(results[:3], 1):\n",
    "            print(f\"   {i}. {result.get('title', 'N/A')[:60]}...\")\n",
    "        \n",
    "        return {\"web_results\": results}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error searching web: {e}\")\n",
    "        return {\"web_results\": []}\n",
    "\n",
    "print(\"‚úÖ Web search node created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Create Context Builder Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context(state: RAGState) -> RAGState:\n",
    "    \"\"\"\n",
    "    Combine vector store docs and web results into unified context.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üî® CONTEXT BUILDER NODE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    context_parts = []\n",
    "    sources = []\n",
    "    \n",
    "    # Add vector store documents\n",
    "    vector_docs = state.get(\"vector_docs\", [])\n",
    "    if vector_docs:\n",
    "        context_parts.append(\"=== INTERNAL KNOWLEDGE BASE ===\")\n",
    "        for i, doc in enumerate(vector_docs, 1):\n",
    "            context_parts.append(f\"\\n[Document {i}]\")\n",
    "            context_parts.append(doc.page_content)\n",
    "            source = doc.metadata.get('source', 'unknown')\n",
    "            sources.append(f\"Internal: {source}\")\n",
    "        print(f\"üìö Added {len(vector_docs)} documents from vector store\")\n",
    "    \n",
    "    # Add web results\n",
    "    web_results = state.get(\"web_results\", [])\n",
    "    if web_results:\n",
    "        context_parts.append(\"\\n\\n=== WEB SEARCH RESULTS ===\")\n",
    "        for i, result in enumerate(web_results, 1):\n",
    "            context_parts.append(f\"\\n[Web Result {i}]\")\n",
    "            context_parts.append(f\"Title: {result.get('title', 'N/A')}\")\n",
    "            context_parts.append(f\"Content: {result.get('snippet', 'N/A')}\")\n",
    "            url = result.get('url', '')\n",
    "            if url:\n",
    "                sources.append(f\"Web: {url}\")\n",
    "        print(f\"üåê Added {len(web_results)} web search results\")\n",
    "    \n",
    "    combined_context = \"\\n\".join(context_parts)\n",
    "    \n",
    "    if not combined_context.strip():\n",
    "        combined_context = \"No relevant context found.\"\n",
    "    \n",
    "    print(f\"\\n‚úÖ Built context with {len(combined_context)} characters\")\n",
    "    \n",
    "    return {\n",
    "        \"combined_context\": combined_context,\n",
    "        \"sources\": sources\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Context builder node created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Create Generator Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(state: RAGState) -> RAGState:\n",
    "    \"\"\"\n",
    "    Generate final answer using retrieved context.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚ú® ANSWER GENERATOR NODE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    query = state[\"query\"]\n",
    "    context = state.get(\"combined_context\", \"\")\n",
    "    \n",
    "    # Create generation prompt\n",
    "    generation_prompt = f\"\"\"You are a helpful assistant answering questions based on provided context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Instructions:\n",
    "1. Answer the question based on the provided context\n",
    "2. If the context contains relevant information, use it to provide a detailed answer\n",
    "3. Distinguish between internal knowledge and web search results if both are present\n",
    "4. If the context doesn't contain enough information, say so\n",
    "5. Be concise but comprehensive\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = llm.invoke([HumanMessage(content=generation_prompt)])\n",
    "        answer = response.content\n",
    "        \n",
    "        print(f\"‚úÖ Generated answer ({len(answer)} characters)\")\n",
    "        \n",
    "        return {\"answer\": answer}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating answer: {e}\")\n",
    "        return {\"answer\": \"I encountered an error generating the answer.\"}\n",
    "\n",
    "print(\"‚úÖ Generator node created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Build the RAG LangGraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the workflow\n",
    "workflow = StateGraph(RAGState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"route\", route_query)\n",
    "workflow.add_node(\"retrieve_vectors\", retrieve_from_vectorstore)\n",
    "workflow.add_node(\"search_web\", search_web)\n",
    "workflow.add_node(\"build_context\", build_context)\n",
    "workflow.add_node(\"generate\", generate_answer)\n",
    "\n",
    "# Define the flow\n",
    "workflow.set_entry_point(\"route\")\n",
    "\n",
    "# Both retrieval operations happen in parallel (conceptually)\n",
    "workflow.add_edge(\"route\", \"retrieve_vectors\")\n",
    "workflow.add_edge(\"route\", \"search_web\")\n",
    "\n",
    "# Both feed into context builder\n",
    "workflow.add_edge(\"retrieve_vectors\", \"build_context\")\n",
    "workflow.add_edge(\"search_web\", \"build_context\")\n",
    "\n",
    "# Context builder feeds into generator\n",
    "workflow.add_edge(\"build_context\", \"generate\")\n",
    "\n",
    "# Generator is the end\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile the graph\n",
    "rag_app = workflow.compile()\n",
    "\n",
    "print(\"\\n‚úÖ RAG Application Ready!\")\n",
    "print(\"\\nüìä Workflow: Route ‚Üí [Vector Retrieval + Web Search] ‚Üí Context Builder ‚Üí Generator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Visualize the RAG Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the workflow\n",
    "try:\n",
    "    from IPython.display import Image, display\n",
    "    display(Image(rag_app.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not visualize graph: {e}\")\n",
    "    print(\"\\nWorkflow Structure:\")\n",
    "    print(\"Route ‚Üí Retrieve Vectors + Search Web ‚Üí Build Context ‚Üí Generate ‚Üí End\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Helper Function to Run RAG Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_rag(query: str, verbose: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run a query through the RAG system.\n",
    "    \n",
    "    Args:\n",
    "        query: The question to ask\n",
    "        verbose: Whether to print detailed output\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with answer and sources\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ü§ñ RAG SYSTEM - QUERY PROCESSING\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\n‚ùì Query: {query}\\n\")\n",
    "    \n",
    "    # Initialize state\n",
    "    initial_state = {\n",
    "        \"query\": query,\n",
    "        \"use_web_search\": False,\n",
    "        \"use_vector_store\": False,\n",
    "        \"vector_docs\": [],\n",
    "        \"web_results\": [],\n",
    "        \"combined_context\": \"\",\n",
    "        \"answer\": \"\",\n",
    "        \"sources\": []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Run the workflow\n",
    "        final_state = rag_app.invoke(initial_state)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"üìù FINAL ANSWER\")\n",
    "            print(\"=\"*80)\n",
    "            print(final_state[\"answer\"])\n",
    "            \n",
    "            if final_state.get(\"sources\"):\n",
    "                print(\"\\n\" + \"=\"*80)\n",
    "                print(\"üìö SOURCES\")\n",
    "                print(\"=\"*80)\n",
    "                for i, source in enumerate(final_state[\"sources\"], 1):\n",
    "                    print(f\"{i}. {source}\")\n",
    "        \n",
    "        return {\n",
    "            \"answer\": final_state[\"answer\"],\n",
    "            \"sources\": final_state.get(\"sources\", []),\n",
    "            \"used_vector_store\": final_state.get(\"use_vector_store\", False),\n",
    "            \"used_web_search\": final_state.get(\"use_web_search\", False)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {\"answer\": f\"Error: {e}\", \"sources\": []}\n",
    "\n",
    "print(\"‚úÖ Helper function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Example 1: Query Using Only Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Internal knowledge query\n",
    "result1 = ask_rag(\"What is LangGraph and what are its capabilities?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Example 2: Query Using Only Web Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Current events query\n",
    "result2 = ask_rag(\"What are the latest developments in AI in 2024?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Example 3: Hybrid Query (Both Sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Hybrid query needing both sources\n",
    "result3 = ask_rag(\"How does RAG compare to the latest retrieval methods in 2024?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. Example 4: Company Policy Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Company-specific query\n",
    "result4 = ask_rag(\"What is our company's remote work policy?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21. Add Your Own Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_documents_to_rag(texts: List[str], metadatas: List[Dict] = None):\n",
    "    \"\"\"\n",
    "    Add new documents to the RAG knowledge base.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of document texts\n",
    "        metadatas: Optional list of metadata dicts for each document\n",
    "    \"\"\"\n",
    "    if metadatas is None:\n",
    "        metadatas = [{\"source\": \"user_added\"} for _ in texts]\n",
    "    \n",
    "    # Create documents\n",
    "    docs = [Document(page_content=text, metadata=meta) \n",
    "            for text, meta in zip(texts, metadatas)]\n",
    "    \n",
    "    # Add to vector store\n",
    "    vectorstore.add_documents(docs)\n",
    "    \n",
    "    print(f\"‚úÖ Added {len(docs)} documents to the knowledge base\")\n",
    "    print(f\"üìä Total documents: {vectorstore._collection.count()}\")\n",
    "\n",
    "# Example: Add a new document\n",
    "new_docs = [\n",
    "    \"\"\"Python 3.12 introduces new features including improved error messages, \n",
    "    a new f-string parser, and performance improvements. The PEP 701 changes make \n",
    "    f-strings more flexible and powerful.\"\"\"\n",
    "]\n",
    "\n",
    "new_metadata = [{\"source\": \"python_docs\", \"version\": \"3.12\", \"topic\": \"programming\"}]\n",
    "\n",
    "add_documents_to_rag(new_docs, new_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22. Load Documents from Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents_from_file(file_path: str, chunk_size: int = 1000) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load and process documents from a file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the file (.txt, .pdf, etc.)\n",
    "        chunk_size: Size of text chunks\n",
    "        \n",
    "    Returns:\n",
    "        List of processed documents\n",
    "    \"\"\"\n",
    "    # Choose loader based on file type\n",
    "    if file_path.endswith('.pdf'):\n",
    "        loader = PyPDFLoader(file_path)\n",
    "    elif file_path.endswith('.txt'):\n",
    "        loader = TextLoader(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_path}\")\n",
    "    \n",
    "    # Load documents\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Split into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=200\n",
    "    )\n",
    "    splits = text_splitter.split_documents(documents)\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(documents)} documents\")\n",
    "    print(f\"üìÑ Split into {len(splits)} chunks\")\n",
    "    \n",
    "    return splits\n",
    "\n",
    "# Example usage (uncomment to use with your own files):\n",
    "# docs = load_documents_from_file(\"your_document.pdf\")\n",
    "# vectorstore.add_documents(docs)\n",
    "# print(f\"Total documents in store: {vectorstore._collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23. Advanced: Hybrid Search with Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_results(query: str, documents: List[Document], web_results: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Rerank combined results based on relevance to query.\n",
    "    Simple implementation using LLM scoring.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    # Convert documents to dict format\n",
    "    for doc in documents:\n",
    "        all_results.append({\n",
    "            \"content\": doc.page_content,\n",
    "            \"source\": doc.metadata.get(\"source\", \"vector_store\"),\n",
    "            \"type\": \"vector\"\n",
    "        })\n",
    "    \n",
    "    # Add web results\n",
    "    for result in web_results:\n",
    "        all_results.append({\n",
    "            \"content\": result.get(\"snippet\", \"\"),\n",
    "            \"source\": result.get(\"url\", \"web\"),\n",
    "            \"type\": \"web\"\n",
    "        })\n",
    "    \n",
    "    # Simple reranking based on content length and keyword matching\n",
    "    # In production, use a proper reranking model\n",
    "    query_terms = query.lower().split()\n",
    "    \n",
    "    for result in all_results:\n",
    "        content_lower = result[\"content\"].lower()\n",
    "        score = sum(1 for term in query_terms if term in content_lower)\n",
    "        result[\"relevance_score\"] = score\n",
    "    \n",
    "    # Sort by relevance\n",
    "    ranked = sorted(all_results, key=lambda x: x[\"relevance_score\"], reverse=True)\n",
    "    \n",
    "    return ranked[:5]  # Return top 5\n",
    "\n",
    "print(\"‚úÖ Reranking function created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24. Interactive RAG Chat Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_rag_chat():\n",
    "    \"\"\"\n",
    "    Interactive chat interface for the RAG system.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üí¨ INTERACTIVE RAG CHAT\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Ask questions and get answers from both the knowledge base and web!\")\n",
    "    print(\"Commands: 'quit' to exit, 'stats' for system stats\\n\")\n",
    "    \n",
    "    conversation_history = []\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"\\n‚ùì You: \")\n",
    "        \n",
    "        if query.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"\\nüëã Thanks for chatting! Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if query.lower() == 'stats':\n",
    "            print(f\"\\nüìä System Statistics:\")\n",
    "            print(f\"   Knowledge Base Documents: {vectorstore._collection.count()}\")\n",
    "            print(f\"   Queries Processed: {len(conversation_history)}\")\n",
    "            continue\n",
    "        \n",
    "        if not query.strip():\n",
    "            continue\n",
    "        \n",
    "        # Process query\n",
    "        result = ask_rag(query, verbose=False)\n",
    "        \n",
    "        print(f\"\\nü§ñ Assistant: {result['answer']}\")\n",
    "        \n",
    "        if result.get('sources'):\n",
    "            print(f\"\\nüìö Sources: {len(result['sources'])} total\")\n",
    "        \n",
    "        conversation_history.append({\n",
    "            \"query\": query,\n",
    "            \"answer\": result['answer'],\n",
    "            \"sources\": result.get('sources', [])\n",
    "        })\n",
    "\n",
    "# Uncomment to start interactive mode:\n",
    "# interactive_rag_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25. Batch Processing Multiple Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch process multiple queries\n",
    "queries = [\n",
    "    \"What is RAG?\",\n",
    "    \"What are the latest AI developments?\",\n",
    "    \"Explain our remote work policy\",\n",
    "    \"How does LangChain work?\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üì¶ BATCH PROCESSING QUERIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = []\n",
    "for i, query in enumerate(queries, 1):\n",
    "    print(f\"\\n[{i}/{len(queries)}] Processing: {query}\")\n",
    "    result = ask_rag(query, verbose=False)\n",
    "    results.append(result)\n",
    "    print(f\"‚úÖ Answer preview: {result['answer'][:100]}...\")\n",
    "\n",
    "print(f\"\\n‚úÖ Processed {len(queries)} queries successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 26. Performance Monitoring and Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def benchmark_rag(query: str, num_runs: int = 3) -> Dict:\n",
    "    \"\"\"\n",
    "    Benchmark RAG system performance.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüèÉ Running benchmark for: '{query}'\")\n",
    "    print(f\"Number of runs: {num_runs}\\n\")\n",
    "    \n",
    "    times = []\n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        start = time.time()\n",
    "        result = ask_rag(query, verbose=False)\n",
    "        end = time.time()\n",
    "        \n",
    "        elapsed = end - start\n",
    "        times.append(elapsed)\n",
    "        print(f\"Run {i+1}: {elapsed:.2f}s\")\n",
    "    \n",
    "    avg_time = sum(times) / len(times)\n",
    "    \n",
    "    print(f\"\\nüìä Benchmark Results:\")\n",
    "    print(f\"   Average Time: {avg_time:.2f}s\")\n",
    "    print(f\"   Min Time: {min(times):.2f}s\")\n",
    "    print(f\"   Max Time: {max(times):.2f}s\")\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"avg_time\": avg_time,\n",
    "        \"min_time\": min(times),\n",
    "        \"max_time\": max(times),\n",
    "        \"runs\": num_runs\n",
    "    }\n",
    "\n",
    "# Run benchmark\n",
    "benchmark_result = benchmark_rag(\"What is LangGraph?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 27. Best Practices and Tips\n",
    "\n",
    "### Document Management:\n",
    "1. **Chunking Strategy**: Use appropriate chunk sizes (500-1500 tokens) with overlap\n",
    "2. **Metadata**: Add rich metadata (source, date, category) for better filtering\n",
    "3. **Updates**: Regularly update vector store with new information\n",
    "4. **Cleaning**: Remove duplicates and outdated documents\n",
    "\n",
    "### Retrieval Optimization:\n",
    "1. **k Parameter**: Adjust number of retrieved documents (3-5 is usually good)\n",
    "2. **Similarity Threshold**: Filter out low-relevance results\n",
    "3. **Hybrid Search**: Combine semantic and keyword search\n",
    "4. **Reranking**: Use reranking models for better result ordering\n",
    "\n",
    "### Web Search Integration:\n",
    "1. **Query Refinement**: Optimize search queries before sending to DuckDuckGo\n",
    "2. **Result Filtering**: Filter and validate web results\n",
    "3. **Caching**: Cache frequent search results\n",
    "4. **Rate Limiting**: Respect search API limits\n",
    "\n",
    "### LLM Generation:\n",
    "1. **Context Length**: Don't exceed model's context window\n",
    "2. **Prompt Engineering**: Craft clear, specific prompts\n",
    "3. **Citation**: Ask LLM to cite sources in responses\n",
    "4. **Fact-Checking**: Validate generated content when possible\n",
    "\n",
    "### Production Considerations:\n",
    "1. **Error Handling**: Comprehensive error handling and fallbacks\n",
    "2. **Monitoring**: Track query patterns and system performance\n",
    "3. **Caching**: Implement response caching for common queries\n",
    "4. **Security**: Sanitize inputs and validate sources\n",
    "5. **Scalability**: Use managed vector databases (Pinecone, Weaviate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 28. Advanced Features to Implement\n",
    "\n",
    "### 1. Conversational RAG:\n",
    "- Add conversation history to context\n",
    "- Track follow-up questions\n",
    "- Maintain user session state\n",
    "\n",
    "### 2. Multi-Modal RAG:\n",
    "- Process images and PDFs\n",
    "- Extract text from various formats\n",
    "- Handle structured data (tables, charts)\n",
    "\n",
    "### 3. Advanced Retrieval:\n",
    "```python\n",
    "# Hypothetical Document Embeddings (HyDE)\n",
    "# Parent-Child Document Retrieval\n",
    "# Multi-Vector Retrieval\n",
    "```\n",
    "\n",
    "### 4. Quality Control:\n",
    "- Confidence scoring\n",
    "- Answer validation\n",
    "- Source verification\n",
    "- Hallucination detection\n",
    "\n",
    "### 5. User Feedback:\n",
    "- Collect user ratings\n",
    "- Learn from feedback\n",
    "- Improve over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 29. Troubleshooting Common Issues\n",
    "\n",
    "### Issue: Poor Retrieval Quality\n",
    "**Solutions:**\n",
    "- Adjust chunk size and overlap\n",
    "- Try different embedding models\n",
    "- Increase number of retrieved documents (k)\n",
    "- Add metadata filtering\n",
    "\n",
    "### Issue: Slow Performance\n",
    "**Solutions:**\n",
    "- Use faster embedding models\n",
    "- Implement caching\n",
    "- Use approximate nearest neighbor search (FAISS with IVF)\n",
    "- Reduce context length\n",
    "\n",
    "### Issue: Irrelevant Web Results\n",
    "**Solutions:**\n",
    "- Refine search queries\n",
    "- Add result filtering\n",
    "- Increase relevance threshold\n",
    "- Use better search tools (Tavily, Brave)\n",
    "\n",
    "### Issue: Out of Context Window\n",
    "**Solutions:**\n",
    "- Reduce number of retrieved documents\n",
    "- Summarize retrieved content\n",
    "- Use models with larger context windows\n",
    "- Implement smart context truncation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30. Resources and Next Steps\n",
    "\n",
    "### Documentation:\n",
    "- [LangChain RAG Tutorial](https://python.langchain.com/docs/use_cases/question_answering/)\n",
    "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
    "- [Vector Databases Comparison](https://www.pinecone.io/learn/vector-database/)\n",
    "\n",
    "### Alternative Tools:\n",
    "- **Embeddings**: Cohere, Voyage AI, Jina AI\n",
    "- **Vector DBs**: Pinecone, Weaviate, Qdrant, Milvus\n",
    "- **Search**: Tavily (LLM-optimized), Brave Search, SerpAPI\n",
    "- **Reranking**: Cohere Rerank, Jina Reranker\n",
    "\n",
    "### Next Steps:\n",
    "1. Deploy with FastAPI backend\n",
    "2. Build web UI with Streamlit/Gradio\n",
    "3. Add authentication and rate limiting\n",
    "4. Implement A/B testing for different strategies\n",
    "5. Set up monitoring and analytics\n",
    "6. Create evaluation pipeline\n",
    "7. Scale with production-grade vector DB\n",
    "\n",
    "### Evaluation:\n",
    "- Test retrieval quality (precision, recall)\n",
    "- Measure generation quality (faithfulness, relevance)\n",
    "- Track latency and throughput\n",
    "- Collect user feedback\n",
    "\n",
    "Happy building! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
