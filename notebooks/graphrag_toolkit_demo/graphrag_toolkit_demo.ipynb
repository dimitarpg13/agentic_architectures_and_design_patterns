{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphRAG Toolkit: Complete Implementation Guide\n",
    "\n",
    "This notebook demonstrates a comprehensive GraphRAG (Graph-based Retrieval Augmented Generation) workflow using modern tools and libraries. GraphRAG enhances traditional RAG by building knowledge graphs from documents, enabling more contextual and relationship-aware retrieval.\n",
    "\n",
    "## Key Features Demonstrated:\n",
    "- Document processing and chunking\n",
    "- Knowledge graph construction\n",
    "- Entity and relationship extraction\n",
    "- Graph-based retrieval\n",
    "- Context-aware generation\n",
    "- Visualization and analysis tools\n",
    "\n",
    "**Author**: GraphRAG Implementation Guide  \n",
    "**Last Updated**: December 2024  \n",
    "**Environment**: Google Colab Compatible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Setup\n",
    "\n",
    "Install required packages for GraphRAG implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core installations for GraphRAG\n",
    "!pip install -q --upgrade pip\n",
    "!pip install -q langchain langchain-community langchain-openai\n",
    "!pip install -q networkx pyvis\n",
    "!pip install -q sentence-transformers chromadb\n",
    "!pip install -q spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install -q pandas numpy\n",
    "!pip install -q python-dotenv\n",
    "!pip install -q tiktoken\n",
    "!pip install -q beautifulsoup4 requests\n",
    "!pip install -q matplotlib seaborn\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLP and ML\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# LangChain components\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Graph components\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and API Setup\n",
    "\n",
    "Configure API keys and model parameters. For Google Colab, we'll use environment variables or Colab secrets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration class for GraphRAG\n",
    "@dataclass\n",
    "class GraphRAGConfig:\n",
    "    \"\"\"Configuration for GraphRAG pipeline\"\"\"\n",
    "    \n",
    "    # Model configuration\n",
    "    llm_model: str = \"gpt-4o-mini\"\n",
    "    embedding_model: str = \"text-embedding-3-small\"\n",
    "    temperature: float = 0.7\n",
    "    max_tokens: int = 2000\n",
    "    \n",
    "    # Chunking configuration\n",
    "    chunk_size: int = 1000\n",
    "    chunk_overlap: int = 200\n",
    "    \n",
    "    # Graph configuration\n",
    "    max_entities_per_chunk: int = 10\n",
    "    max_relationships_per_chunk: int = 15\n",
    "    min_entity_confidence: float = 0.7\n",
    "    \n",
    "    # Retrieval configuration\n",
    "    top_k_documents: int = 5\n",
    "    top_k_graph_nodes: int = 10\n",
    "    similarity_threshold: float = 0.7\n",
    "    \n",
    "    # Vector store configuration\n",
    "    collection_name: str = \"graphrag_demo\"\n",
    "    persist_directory: str = \"./chroma_db\"\n",
    "\n",
    "# Initialize configuration\n",
    "config = GraphRAGConfig()\n",
    "\n",
    "# Setup API keys (use Colab secrets or environment variables)\n",
    "def setup_api_keys():\n",
    "    \"\"\"Setup API keys from Colab secrets or manual input\"\"\"\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "        print(\"‚úÖ API key loaded from Colab secrets\")\n",
    "    except:\n",
    "        # Manual input fallback\n",
    "        import getpass\n",
    "        api_key = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "        os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "        print(\"‚úÖ API key set manually\")\n",
    "\n",
    "setup_api_keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Core GraphRAG Components\n",
    "\n",
    "Implementation of the core components for the GraphRAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Entity:\n",
    "    \"\"\"Represents an entity in the knowledge graph\"\"\"\n",
    "    name: str\n",
    "    type: str\n",
    "    description: str\n",
    "    properties: Dict[str, Any] = field(default_factory=dict)\n",
    "    confidence: float = 1.0\n",
    "    source_chunks: List[str] = field(default_factory=list)\n",
    "\n",
    "@dataclass\n",
    "class Relationship:\n",
    "    \"\"\"Represents a relationship between entities\"\"\"\n",
    "    source: str\n",
    "    target: str\n",
    "    type: str\n",
    "    description: str\n",
    "    properties: Dict[str, Any] = field(default_factory=dict)\n",
    "    confidence: float = 1.0\n",
    "    source_chunks: List[str] = field(default_factory=list)\n",
    "\n",
    "class KnowledgeGraph:\n",
    "    \"\"\"Knowledge graph for storing entities and relationships\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.graph = nx.DiGraph()\n",
    "        self.entities: Dict[str, Entity] = {}\n",
    "        self.relationships: List[Relationship] = []\n",
    "        \n",
    "    def add_entity(self, entity: Entity):\n",
    "        \"\"\"Add an entity to the graph\"\"\"\n",
    "        self.entities[entity.name] = entity\n",
    "        self.graph.add_node(\n",
    "            entity.name,\n",
    "            type=entity.type,\n",
    "            description=entity.description,\n",
    "            properties=entity.properties,\n",
    "            confidence=entity.confidence\n",
    "        )\n",
    "        \n",
    "    def add_relationship(self, relationship: Relationship):\n",
    "        \"\"\"Add a relationship to the graph\"\"\"\n",
    "        self.relationships.append(relationship)\n",
    "        self.graph.add_edge(\n",
    "            relationship.source,\n",
    "            relationship.target,\n",
    "            type=relationship.type,\n",
    "            description=relationship.description,\n",
    "            properties=relationship.properties,\n",
    "            confidence=relationship.confidence\n",
    "        )\n",
    "        \n",
    "    def get_subgraph(self, node: str, depth: int = 2) -> nx.DiGraph:\n",
    "        \"\"\"Get subgraph around a specific node\"\"\"\n",
    "        nodes = nx.single_source_shortest_path_length(\n",
    "            self.graph.to_undirected(), node, cutoff=depth\n",
    "        ).keys()\n",
    "        return self.graph.subgraph(nodes)\n",
    "    \n",
    "    def get_entity_context(self, entity_name: str) -> Dict:\n",
    "        \"\"\"Get context information for an entity\"\"\"\n",
    "        if entity_name not in self.entities:\n",
    "            return {}\n",
    "            \n",
    "        entity = self.entities[entity_name]\n",
    "        neighbors = list(self.graph.neighbors(entity_name))\n",
    "        predecessors = list(self.graph.predecessors(entity_name))\n",
    "        \n",
    "        return {\n",
    "            \"entity\": entity,\n",
    "            \"neighbors\": neighbors,\n",
    "            \"predecessors\": predecessors,\n",
    "            \"degree\": self.graph.degree(entity_name),\n",
    "            \"pagerank\": nx.pagerank(self.graph).get(entity_name, 0)\n",
    "        }\n",
    "    \n",
    "    def visualize(self, subset_nodes: Optional[List[str]] = None, \n",
    "                  height: str = \"600px\", width: str = \"100%\"):\n",
    "        \"\"\"Visualize the knowledge graph\"\"\"\n",
    "        net = Network(height=height, width=width, directed=True)\n",
    "        \n",
    "        # Select nodes to visualize\n",
    "        if subset_nodes:\n",
    "            subgraph = self.graph.subgraph(subset_nodes)\n",
    "        else:\n",
    "            subgraph = self.graph\n",
    "            \n",
    "        # Add nodes and edges\n",
    "        for node, data in subgraph.nodes(data=True):\n",
    "            net.add_node(\n",
    "                node,\n",
    "                label=f\"{node}\\n({data.get('type', 'Unknown')})\",\n",
    "                title=data.get('description', ''),\n",
    "                color=self._get_node_color(data.get('type', 'Unknown'))\n",
    "            )\n",
    "            \n",
    "        for source, target, data in subgraph.edges(data=True):\n",
    "            net.add_edge(\n",
    "                source, target,\n",
    "                title=f\"{data.get('type', 'relates_to')}: {data.get('description', '')}\",\n",
    "                label=data.get('type', 'relates_to')\n",
    "            )\n",
    "            \n",
    "        # Generate HTML\n",
    "        net.toggle_physics(True)\n",
    "        net.show_buttons(filter_=['physics'])\n",
    "        return net.generate_html()\n",
    "    \n",
    "    def _get_node_color(self, node_type: str) -> str:\n",
    "        \"\"\"Get color for node based on type\"\"\"\n",
    "        color_map = {\n",
    "            \"Person\": \"#FF6B6B\",\n",
    "            \"Organization\": \"#4ECDC4\",\n",
    "            \"Location\": \"#45B7D1\",\n",
    "            \"Event\": \"#96CEB4\",\n",
    "            \"Concept\": \"#DDA0DD\",\n",
    "            \"Product\": \"#F7DC6F\",\n",
    "            \"Unknown\": \"#95A5A6\"\n",
    "        }\n",
    "        return color_map.get(node_type, color_map[\"Unknown\"])\n",
    "\n",
    "print(\"‚úÖ Core GraphRAG components defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entity and Relationship Extraction\n",
    "\n",
    "Implement extraction of entities and relationships from text using LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityRelationshipExtractor:\n",
    "    \"\"\"Extract entities and relationships from text using LLMs\"\"\"\n",
    "    \n",
    "    def __init__(self, config: GraphRAGConfig):\n",
    "        self.config = config\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=config.llm_model,\n",
    "            temperature=config.temperature,\n",
    "            max_tokens=config.max_tokens\n",
    "        )\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "    def extract_entities_relationships(self, text: str) -> Tuple[List[Entity], List[Relationship]]:\n",
    "        \"\"\"Extract entities and relationships from text\"\"\"\n",
    "        \n",
    "        # Define extraction prompt\n",
    "        extraction_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are an expert at extracting entities and relationships from text.\n",
    "            Extract key entities (people, organizations, locations, concepts, events, products) and their relationships.\n",
    "            \n",
    "            Format your response as JSON with the following structure:\n",
    "            {{\n",
    "                \"entities\": [\n",
    "                    {{\n",
    "                        \"name\": \"Entity Name\",\n",
    "                        \"type\": \"Person|Organization|Location|Event|Concept|Product\",\n",
    "                        \"description\": \"Brief description\",\n",
    "                        \"confidence\": 0.0-1.0\n",
    "                    }}\n",
    "                ],\n",
    "                \"relationships\": [\n",
    "                    {{\n",
    "                        \"source\": \"Source Entity Name\",\n",
    "                        \"target\": \"Target Entity Name\",\n",
    "                        \"type\": \"relationship_type\",\n",
    "                        \"description\": \"Description of relationship\",\n",
    "                        \"confidence\": 0.0-1.0\n",
    "                    }}\n",
    "                ]\n",
    "            }}\n",
    "            \n",
    "            Be comprehensive but accurate. Only extract entities and relationships that are clearly present in the text.\"\"\"),\n",
    "            (\"user\", \"Extract entities and relationships from the following text:\\n\\n{text}\")\n",
    "        ])\n",
    "        \n",
    "        try:\n",
    "            # Create chain and extract\n",
    "            chain = LLMChain(llm=self.llm, prompt=extraction_prompt)\n",
    "            response = chain.run(text=text)\n",
    "            \n",
    "            # Parse JSON response\n",
    "            extracted_data = json.loads(response)\n",
    "            \n",
    "            # Create Entity objects\n",
    "            entities = []\n",
    "            for e in extracted_data.get(\"entities\", []):\n",
    "                if e.get(\"confidence\", 0) >= self.config.min_entity_confidence:\n",
    "                    entities.append(Entity(\n",
    "                        name=e[\"name\"],\n",
    "                        type=e[\"type\"],\n",
    "                        description=e[\"description\"],\n",
    "                        confidence=e.get(\"confidence\", 1.0),\n",
    "                        source_chunks=[text[:100]]  # Store reference to source\n",
    "                    ))\n",
    "            \n",
    "            # Create Relationship objects\n",
    "            relationships = []\n",
    "            entity_names = {e.name for e in entities}\n",
    "            for r in extracted_data.get(\"relationships\", []):\n",
    "                # Only add relationships where both entities exist\n",
    "                if r[\"source\"] in entity_names and r[\"target\"] in entity_names:\n",
    "                    relationships.append(Relationship(\n",
    "                        source=r[\"source\"],\n",
    "                        target=r[\"target\"],\n",
    "                        type=r[\"type\"],\n",
    "                        description=r[\"description\"],\n",
    "                        confidence=r.get(\"confidence\", 1.0),\n",
    "                        source_chunks=[text[:100]]\n",
    "                    ))\n",
    "            \n",
    "            # Limit to configured maximums\n",
    "            entities = entities[:self.config.max_entities_per_chunk]\n",
    "            relationships = relationships[:self.config.max_relationships_per_chunk]\n",
    "            \n",
    "            return entities, relationships\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting entities/relationships: {e}\")\n",
    "            # Fallback to spaCy NER\n",
    "            return self._fallback_extraction(text)\n",
    "    \n",
    "    def _fallback_extraction(self, text: str) -> Tuple[List[Entity], List[Relationship]]:\n",
    "        \"\"\"Fallback extraction using spaCy NER\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        entities = []\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            entity_type_map = {\n",
    "                \"PERSON\": \"Person\",\n",
    "                \"ORG\": \"Organization\",\n",
    "                \"GPE\": \"Location\",\n",
    "                \"LOC\": \"Location\",\n",
    "                \"EVENT\": \"Event\",\n",
    "                \"PRODUCT\": \"Product\"\n",
    "            }\n",
    "            \n",
    "            if ent.label_ in entity_type_map:\n",
    "                entities.append(Entity(\n",
    "                    name=ent.text,\n",
    "                    type=entity_type_map[ent.label_],\n",
    "                    description=f\"{ent.label_} entity from text\",\n",
    "                    confidence=0.7,\n",
    "                    source_chunks=[text[:100]]\n",
    "                ))\n",
    "        \n",
    "        return entities[:self.config.max_entities_per_chunk], []\n",
    "\n",
    "print(\"‚úÖ Entity and Relationship Extractor defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. GraphRAG Pipeline Implementation\n",
    "\n",
    "Complete GraphRAG pipeline combining all components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphRAGPipeline:\n",
    "    \"\"\"Complete GraphRAG pipeline for document processing and retrieval\"\"\"\n",
    "    \n",
    "    def __init__(self, config: GraphRAGConfig):\n",
    "        self.config = config\n",
    "        self.knowledge_graph = KnowledgeGraph()\n",
    "        self.extractor = EntityRelationshipExtractor(config)\n",
    "        \n",
    "        # Initialize embeddings and vector store\n",
    "        self.embeddings = OpenAIEmbeddings(model=config.embedding_model)\n",
    "        self.vector_store = None\n",
    "        self.documents = []\n",
    "        \n",
    "        # Initialize text splitter\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=config.chunk_size,\n",
    "            chunk_overlap=config.chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    "        )\n",
    "        \n",
    "        # Initialize LLM for generation\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=config.llm_model,\n",
    "            temperature=config.temperature,\n",
    "            max_tokens=config.max_tokens\n",
    "        )\n",
    "        \n",
    "        # Metrics tracking\n",
    "        self.metrics = {\n",
    "            \"total_documents\": 0,\n",
    "            \"total_chunks\": 0,\n",
    "            \"total_entities\": 0,\n",
    "            \"total_relationships\": 0,\n",
    "            \"processing_time\": 0\n",
    "        }\n",
    "    \n",
    "    def process_documents(self, texts: List[str], metadata: Optional[List[Dict]] = None):\n",
    "        \"\"\"Process documents to build knowledge graph and vector store\"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        logger.info(f\"Processing {len(texts)} documents...\")\n",
    "        \n",
    "        # Create documents with metadata\n",
    "        if metadata is None:\n",
    "            metadata = [{\"source\": f\"doc_{i}\"} for i in range(len(texts))]\n",
    "            \n",
    "        all_chunks = []\n",
    "        \n",
    "        for i, (text, meta) in enumerate(zip(texts, metadata)):\n",
    "            # Split into chunks\n",
    "            chunks = self.text_splitter.split_text(text)\n",
    "            \n",
    "            for j, chunk in enumerate(chunks):\n",
    "                chunk_meta = {**meta, \"chunk_id\": f\"{meta.get('source', f'doc_{i}')}_{j}\"}\n",
    "                all_chunks.append(Document(page_content=chunk, metadata=chunk_meta))\n",
    "                \n",
    "                # Extract entities and relationships from chunk\n",
    "                entities, relationships = self.extractor.extract_entities_relationships(chunk)\n",
    "                \n",
    "                # Add to knowledge graph\n",
    "                for entity in entities:\n",
    "                    self.knowledge_graph.add_entity(entity)\n",
    "                    \n",
    "                for relationship in relationships:\n",
    "                    self.knowledge_graph.add_relationship(relationship)\n",
    "                    \n",
    "                self.metrics[\"total_entities\"] += len(entities)\n",
    "                self.metrics[\"total_relationships\"] += len(relationships)\n",
    "            \n",
    "            self.metrics[\"total_documents\"] += 1\n",
    "            logger.info(f\"Processed document {i+1}/{len(texts)}\")\n",
    "        \n",
    "        # Create vector store\n",
    "        self.documents = all_chunks\n",
    "        self.metrics[\"total_chunks\"] = len(all_chunks)\n",
    "        \n",
    "        self.vector_store = Chroma.from_documents(\n",
    "            documents=all_chunks,\n",
    "            embedding=self.embeddings,\n",
    "            collection_name=self.config.collection_name,\n",
    "            persist_directory=self.config.persist_directory\n",
    "        )\n",
    "        \n",
    "        self.metrics[\"processing_time\"] = (datetime.now() - start_time).total_seconds()\n",
    "        logger.info(f\"Processing complete in {self.metrics['processing_time']:.2f} seconds\")\n",
    "        \n",
    "        return self.metrics\n",
    "    \n",
    "    def retrieve(self, query: str) -> Dict:\n",
    "        \"\"\"Retrieve relevant context using both vector search and graph traversal\"\"\"\n",
    "        \n",
    "        # 1. Vector similarity search\n",
    "        vector_results = self.vector_store.similarity_search_with_score(\n",
    "            query, k=self.config.top_k_documents\n",
    "        )\n",
    "        \n",
    "        # 2. Extract entities from query\n",
    "        query_entities, _ = self.extractor.extract_entities_relationships(query)\n",
    "        \n",
    "        # 3. Graph-based retrieval\n",
    "        graph_context = {}\n",
    "        for entity in query_entities:\n",
    "            if entity.name in self.knowledge_graph.entities:\n",
    "                context = self.knowledge_graph.get_entity_context(entity.name)\n",
    "                graph_context[entity.name] = context\n",
    "        \n",
    "        # 4. Find related entities through graph traversal\n",
    "        related_entities = set()\n",
    "        for entity_name in graph_context.keys():\n",
    "            subgraph = self.knowledge_graph.get_subgraph(entity_name, depth=2)\n",
    "            related_entities.update(subgraph.nodes())\n",
    "        \n",
    "        # 5. Combine results\n",
    "        return {\n",
    "            \"vector_results\": vector_results,\n",
    "            \"query_entities\": query_entities,\n",
    "            \"graph_context\": graph_context,\n",
    "            \"related_entities\": list(related_entities)[:self.config.top_k_graph_nodes]\n",
    "        }\n",
    "    \n",
    "    def generate_response(self, query: str, retrieval_results: Dict) -> str:\n",
    "        \"\"\"Generate response using retrieved context\"\"\"\n",
    "        \n",
    "        # Build context from vector results\n",
    "        vector_context = \"\\n\\n\".join([\n",
    "            f\"[Score: {score:.3f}] {doc.page_content}\"\n",
    "            for doc, score in retrieval_results[\"vector_results\"]\n",
    "        ])\n",
    "        \n",
    "        # Build context from graph\n",
    "        graph_context_str = \"\"\n",
    "        for entity_name, context in retrieval_results[\"graph_context\"].items():\n",
    "            entity = context[\"entity\"]\n",
    "            graph_context_str += f\"\\n\\nEntity: {entity_name} ({entity.type})\\n\"\n",
    "            graph_context_str += f\"Description: {entity.description}\\n\"\n",
    "            graph_context_str += f\"Connected to: {', '.join(context['neighbors'][:5])}\\n\"\n",
    "            graph_context_str += f\"PageRank Score: {context['pagerank']:.4f}\"\n",
    "        \n",
    "        # Build prompt\n",
    "        generation_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are a helpful assistant that answers questions using provided context.\n",
    "            Use both the document context and knowledge graph information to provide comprehensive answers.\n",
    "            Cite specific entities and relationships when relevant.\n",
    "            If the context doesn't contain enough information, say so.\"\"\"),\n",
    "            (\"user\", \"\"\"Question: {query}\n",
    "            \n",
    "            Document Context:\n",
    "            {vector_context}\n",
    "            \n",
    "            Knowledge Graph Context:\n",
    "            {graph_context}\n",
    "            \n",
    "            Related Entities: {related_entities}\n",
    "            \n",
    "            Please provide a comprehensive answer based on the above context.\"\"\")\n",
    "        ])\n",
    "        \n",
    "        # Generate response\n",
    "        chain = LLMChain(llm=self.llm, prompt=generation_prompt)\n",
    "        response = chain.run(\n",
    "            query=query,\n",
    "            vector_context=vector_context,\n",
    "            graph_context=graph_context_str,\n",
    "            related_entities=\", \".join(retrieval_results[\"related_entities\"][:10])\n",
    "        )\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def query(self, query: str) -> Dict:\n",
    "        \"\"\"End-to-end query processing\"\"\"\n",
    "        logger.info(f\"Processing query: {query}\")\n",
    "        \n",
    "        # Retrieve context\n",
    "        retrieval_results = self.retrieve(query)\n",
    "        \n",
    "        # Generate response\n",
    "        response = self.generate_response(query, retrieval_results)\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"response\": response,\n",
    "            \"retrieval_results\": retrieval_results,\n",
    "            \"metrics\": self.get_query_metrics(retrieval_results)\n",
    "        }\n",
    "    \n",
    "    def get_query_metrics(self, retrieval_results: Dict) -> Dict:\n",
    "        \"\"\"Calculate query metrics\"\"\"\n",
    "        return {\n",
    "            \"num_documents_retrieved\": len(retrieval_results[\"vector_results\"]),\n",
    "            \"num_entities_found\": len(retrieval_results[\"query_entities\"]),\n",
    "            \"num_graph_contexts\": len(retrieval_results[\"graph_context\"]),\n",
    "            \"num_related_entities\": len(retrieval_results[\"related_entities\"]),\n",
    "            \"avg_similarity_score\": np.mean([\n",
    "                score for _, score in retrieval_results[\"vector_results\"]\n",
    "            ]) if retrieval_results[\"vector_results\"] else 0\n",
    "        }\n",
    "    \n",
    "    def visualize_graph(self, max_nodes: int = 50):\n",
    "        \"\"\"Visualize the knowledge graph\"\"\"\n",
    "        # Get top nodes by PageRank\n",
    "        if len(self.knowledge_graph.graph.nodes()) > max_nodes:\n",
    "            pagerank = nx.pagerank(self.knowledge_graph.graph)\n",
    "            top_nodes = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:max_nodes]\n",
    "            subset_nodes = [node for node, _ in top_nodes]\n",
    "        else:\n",
    "            subset_nodes = None\n",
    "            \n",
    "        return self.knowledge_graph.visualize(subset_nodes=subset_nodes)\n",
    "    \n",
    "    def get_statistics(self) -> pd.DataFrame:\n",
    "        \"\"\"Get detailed statistics about the knowledge graph\"\"\"\n",
    "        stats = {\n",
    "            \"Metric\": [\n",
    "                \"Total Documents\",\n",
    "                \"Total Chunks\",\n",
    "                \"Total Entities\",\n",
    "                \"Total Relationships\",\n",
    "                \"Graph Nodes\",\n",
    "                \"Graph Edges\",\n",
    "                \"Average Node Degree\",\n",
    "                \"Graph Density\",\n",
    "                \"Number of Components\",\n",
    "                \"Processing Time (seconds)\"\n",
    "            ],\n",
    "            \"Value\": [\n",
    "                self.metrics[\"total_documents\"],\n",
    "                self.metrics[\"total_chunks\"],\n",
    "                self.metrics[\"total_entities\"],\n",
    "                self.metrics[\"total_relationships\"],\n",
    "                self.knowledge_graph.graph.number_of_nodes(),\n",
    "                self.knowledge_graph.graph.number_of_edges(),\n",
    "                np.mean(list(dict(self.knowledge_graph.graph.degree()).values())) \n",
    "                    if self.knowledge_graph.graph.number_of_nodes() > 0 else 0,\n",
    "                nx.density(self.knowledge_graph.graph) \n",
    "                    if self.knowledge_graph.graph.number_of_nodes() > 0 else 0,\n",
    "                nx.number_weakly_connected_components(self.knowledge_graph.graph)\n",
    "                    if self.knowledge_graph.graph.number_of_nodes() > 0 else 0,\n",
    "                self.metrics[\"processing_time\"]\n",
    "            ]\n",
    "        }\n",
    "        return pd.DataFrame(stats)\n",
    "\n",
    "print(\"‚úÖ GraphRAG Pipeline implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Example Usage with Sample Data\n",
    "\n",
    "Demonstrate the GraphRAG pipeline with sample documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents for demonstration\n",
    "sample_documents = [\n",
    "    \"\"\"Apple Inc. is an American multinational technology company headquartered in Cupertino, California. \n",
    "    Tim Cook has been the CEO of Apple since 2011, succeeding Steve Jobs. Apple designs, develops, and sells \n",
    "    consumer electronics, computer software, and online services. The company's hardware products include \n",
    "    the iPhone smartphone, the iPad tablet computer, the Mac personal computer, the Apple Watch smartwatch, \n",
    "    and the Apple TV digital media player. Apple's software includes the macOS and iOS operating systems, \n",
    "    the iTunes media player, and the Safari web browser. The company was founded by Steve Jobs, Steve Wozniak, \n",
    "    and Ronald Wayne in April 1976. Apple became the first publicly traded U.S. company to be valued at over \n",
    "    $1 trillion in 2018.\"\"\",\n",
    "    \n",
    "    \"\"\"Microsoft Corporation is an American multinational technology company with headquarters in Redmond, Washington. \n",
    "    Satya Nadella has been the CEO since 2014, succeeding Steve Ballmer. Microsoft develops, manufactures, licenses, \n",
    "    supports, and sells computer software, consumer electronics, personal computers, and related services. Its best \n",
    "    known software products are the Microsoft Windows line of operating systems, the Microsoft Office suite, and \n",
    "    the Internet Explorer and Edge web browsers. Microsoft was founded by Bill Gates and Paul Allen on April 4, 1975. \n",
    "    The company has made numerous acquisitions, including LinkedIn for $26.2 billion in 2016 and GitHub for \n",
    "    $7.5 billion in 2018. Microsoft's Azure cloud platform competes directly with Amazon Web Services.\"\"\",\n",
    "    \n",
    "    \"\"\"Google, officially known as Alphabet Inc. since 2015, is an American multinational conglomerate headquartered \n",
    "    in Mountain View, California. Sundar Pichai serves as CEO of both Google and Alphabet. The company was originally \n",
    "    founded by Larry Page and Sergey Brin while they were Ph.D. students at Stanford University in 1998. Google's \n",
    "    rapid growth since incorporation has triggered a chain of products, acquisitions, and partnerships beyond Google's \n",
    "    core search engine. The company offers services designed for work and productivity (Google Docs, Google Sheets, \n",
    "    and Google Slides), email (Gmail), scheduling and time management (Google Calendar), cloud storage (Google Drive), \n",
    "    instant messaging and video chat (Google Duo, Google Chat, and Google Meet), language translation (Google Translate), \n",
    "    mapping and navigation (Google Maps, Waze, Google Earth, and Street View), and video sharing (YouTube).\"\"\",\n",
    "    \n",
    "    \"\"\"Amazon.com, Inc. is an American multinational technology company based in Seattle, Washington, which focuses on \n",
    "    e-commerce, cloud computing, digital streaming, and artificial intelligence. Andy Jassy became CEO in 2021, \n",
    "    succeeding founder Jeff Bezos. Amazon is one of the Big Five companies in the U.S. information technology industry, \n",
    "    along with Google, Apple, Microsoft, and Facebook. The company has been referred to as one of the most influential \n",
    "    economic and cultural forces in the world. Amazon was founded by Jeff Bezos in 1994 as an online marketplace for books \n",
    "    but later expanded to sell electronics, software, video games, apparel, furniture, food, toys, and jewelry. \n",
    "    Amazon Web Services (AWS) provides on-demand cloud computing platforms and APIs to individuals, companies, and governments.\"\"\"\n",
    "]\n",
    "\n",
    "# Metadata for documents\n",
    "sample_metadata = [\n",
    "    {\"source\": \"apple_overview\", \"company\": \"Apple\", \"date\": \"2024-12\"},\n",
    "    {\"source\": \"microsoft_overview\", \"company\": \"Microsoft\", \"date\": \"2024-12\"},\n",
    "    {\"source\": \"google_overview\", \"company\": \"Google\", \"date\": \"2024-12\"},\n",
    "    {\"source\": \"amazon_overview\", \"company\": \"Amazon\", \"date\": \"2024-12\"}\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(sample_documents)} sample documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GraphRAG pipeline\n",
    "print(\"Initializing GraphRAG pipeline...\")\n",
    "graphrag = GraphRAGPipeline(config)\n",
    "\n",
    "# Process documents\n",
    "print(\"\\nProcessing documents...\")\n",
    "metrics = graphrag.process_documents(sample_documents, sample_metadata)\n",
    "\n",
    "# Display processing metrics\n",
    "print(\"\\nüìä Processing Metrics:\")\n",
    "display(graphrag.get_statistics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interactive Knowledge Graph Visualization\n",
    "\n",
    "Visualize the constructed knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate interactive graph visualization\n",
    "print(\"Generating knowledge graph visualization...\")\n",
    "graph_html = graphrag.visualize_graph(max_nodes=30)\n",
    "\n",
    "# Display the graph\n",
    "display(HTML(f\"\"\"\n",
    "<div style=\"border: 2px solid #ddd; padding: 10px; border-radius: 5px;\">\n",
    "    <h3>Knowledge Graph Visualization</h3>\n",
    "    <p>Interactive graph showing entities and relationships extracted from documents.</p>\n",
    "    <p><em>Note: Showing top 30 nodes by PageRank score. Use mouse to drag nodes and zoom.</em></p>\n",
    "    {graph_html}\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Query Examples and Response Generation\n",
    "\n",
    "Demonstrate querying the GraphRAG system with various questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example queries\n",
    "test_queries = [\n",
    "    \"Who are the current CEOs of major tech companies?\",\n",
    "    \"What products does Apple make?\",\n",
    "    \"Tell me about the founders of these tech companies.\",\n",
    "    \"Which companies compete in cloud computing?\",\n",
    "    \"What acquisitions have these companies made?\"\n",
    "]\n",
    "\n",
    "# Process each query\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Query {i}: {query}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Execute query\n",
    "    result = graphrag.query(query)\n",
    "    \n",
    "    # Display response\n",
    "    print(f\"\\nüìù Response:\\n{result['response']}\")\n",
    "    \n",
    "    # Display metrics\n",
    "    print(f\"\\nüìä Query Metrics:\")\n",
    "    for metric, value in result['metrics'].items():\n",
    "        print(f\"  ‚Ä¢ {metric}: {value}\")\n",
    "    \n",
    "    # Display found entities\n",
    "    if result['retrieval_results']['query_entities']:\n",
    "        print(f\"\\nüîç Entities found in query:\")\n",
    "        for entity in result['retrieval_results']['query_entities']:\n",
    "            print(f\"  ‚Ä¢ {entity.name} ({entity.type}): {entity.description}\")\n",
    "    \n",
    "    # Display related entities from graph\n",
    "    if result['retrieval_results']['related_entities']:\n",
    "        print(f\"\\nüï∏Ô∏è Related entities from graph:\")\n",
    "        print(f\"  {', '.join(result['retrieval_results']['related_entities'][:10])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced Analysis and Insights\n",
    "\n",
    "Perform advanced analysis on the knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph analysis\n",
    "print(\"üî¨ Advanced Graph Analysis\\n\")\n",
    "\n",
    "# Calculate PageRank\n",
    "if graphrag.knowledge_graph.graph.number_of_nodes() > 0:\n",
    "    pagerank = nx.pagerank(graphrag.knowledge_graph.graph)\n",
    "    top_entities = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    print(\"Top 10 Entities by PageRank:\")\n",
    "    for i, (entity, score) in enumerate(top_entities, 1):\n",
    "        entity_obj = graphrag.knowledge_graph.entities.get(entity)\n",
    "        entity_type = entity_obj.type if entity_obj else \"Unknown\"\n",
    "        print(f\"{i:2}. {entity:30} ({entity_type:15}) - Score: {score:.4f}\")\n",
    "\n",
    "# Analyze connectivity\n",
    "print(\"\\nüìä Graph Connectivity Analysis:\")\n",
    "if graphrag.knowledge_graph.graph.number_of_nodes() > 0:\n",
    "    # Degree centrality\n",
    "    degree_centrality = nx.degree_centrality(graphrag.knowledge_graph.graph)\n",
    "    avg_centrality = np.mean(list(degree_centrality.values()))\n",
    "    \n",
    "    # Clustering coefficient\n",
    "    clustering = nx.average_clustering(graphrag.knowledge_graph.graph.to_undirected())\n",
    "    \n",
    "    print(f\"  ‚Ä¢ Average Degree Centrality: {avg_centrality:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Average Clustering Coefficient: {clustering:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Number of Connected Components: {nx.number_weakly_connected_components(graphrag.knowledge_graph.graph)}\")\n",
    "    \n",
    "    # Find most connected entities\n",
    "    degrees = dict(graphrag.knowledge_graph.graph.degree())\n",
    "    top_connected = sorted(degrees.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    \n",
    "    print(f\"\\n  Most Connected Entities:\")\n",
    "    for entity, degree in top_connected:\n",
    "        print(f\"    ‚Ä¢ {entity}: {degree} connections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize graph statistics\n",
    "if graphrag.knowledge_graph.graph.number_of_nodes() > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Degree distribution\n",
    "    degrees = [d for n, d in graphrag.knowledge_graph.graph.degree()]\n",
    "    axes[0, 0].hist(degrees, bins=20, edgecolor='black', alpha=0.7)\n",
    "    axes[0, 0].set_title('Degree Distribution')\n",
    "    axes[0, 0].set_xlabel('Degree')\n",
    "    axes[0, 0].set_ylabel('Count')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Entity type distribution\n",
    "    entity_types = [e.type for e in graphrag.knowledge_graph.entities.values()]\n",
    "    type_counts = pd.Series(entity_types).value_counts()\n",
    "    axes[0, 1].bar(type_counts.index, type_counts.values, color='skyblue', edgecolor='black')\n",
    "    axes[0, 1].set_title('Entity Type Distribution')\n",
    "    axes[0, 1].set_xlabel('Entity Type')\n",
    "    axes[0, 1].set_ylabel('Count')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 3. PageRank distribution\n",
    "    pagerank_scores = list(pagerank.values())\n",
    "    axes[1, 0].hist(pagerank_scores, bins=20, edgecolor='black', alpha=0.7, color='coral')\n",
    "    axes[1, 0].set_title('PageRank Score Distribution')\n",
    "    axes[1, 0].set_xlabel('PageRank Score')\n",
    "    axes[1, 0].set_ylabel('Count')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Top entities by PageRank\n",
    "    top_10_pagerank = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    entities_pr = [e[0][:15] + '...' if len(e[0]) > 15 else e[0] for e in top_10_pagerank]\n",
    "    scores_pr = [e[1] for e in top_10_pagerank]\n",
    "    axes[1, 1].barh(entities_pr, scores_pr, color='lightgreen', edgecolor='black')\n",
    "    axes[1, 1].set_title('Top 10 Entities by PageRank')\n",
    "    axes[1, 1].set_xlabel('PageRank Score')\n",
    "    axes[1, 1].invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ Graph analysis visualizations complete!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No nodes in graph to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export and Save Results\n",
    "\n",
    "Export the knowledge graph and results for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# Create export directory\n",
    "export_dir = \"/content/graphrag_export\"\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "# 1. Export knowledge graph\n",
    "graph_file = os.path.join(export_dir, \"knowledge_graph.gpickle\")\n",
    "nx.write_gpickle(graphrag.knowledge_graph.graph, graph_file)\n",
    "print(f\"‚úÖ Knowledge graph exported to {graph_file}\")\n",
    "\n",
    "# 2. Export entities and relationships as JSON\n",
    "entities_data = {\n",
    "    \"entities\": [\n",
    "        {\n",
    "            \"name\": e.name,\n",
    "            \"type\": e.type,\n",
    "            \"description\": e.description,\n",
    "            \"confidence\": e.confidence\n",
    "        } for e in graphrag.knowledge_graph.entities.values()\n",
    "    ],\n",
    "    \"relationships\": [\n",
    "        {\n",
    "            \"source\": r.source,\n",
    "            \"target\": r.target,\n",
    "            \"type\": r.type,\n",
    "            \"description\": r.description,\n",
    "            \"confidence\": r.confidence\n",
    "        } for r in graphrag.knowledge_graph.relationships\n",
    "    ]\n",
    "}\n",
    "\n",
    "json_file = os.path.join(export_dir, \"entities_relationships.json\")\n",
    "with open(json_file, 'w') as f:\n",
    "    json.dump(entities_data, f, indent=2)\n",
    "print(f\"‚úÖ Entities and relationships exported to {json_file}\")\n",
    "\n",
    "# 3. Export metrics and statistics\n",
    "stats_df = graphrag.get_statistics()\n",
    "stats_file = os.path.join(export_dir, \"graphrag_statistics.csv\")\n",
    "stats_df.to_csv(stats_file, index=False)\n",
    "print(f\"‚úÖ Statistics exported to {stats_file}\")\n",
    "\n",
    "# 4. Save the entire pipeline (optional - for reloading later)\n",
    "pipeline_file = os.path.join(export_dir, \"graphrag_pipeline.pkl\")\n",
    "with open(pipeline_file, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        \"config\": config,\n",
    "        \"metrics\": graphrag.metrics,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }, f)\n",
    "print(f\"‚úÖ Pipeline configuration saved to {pipeline_file}\")\n",
    "\n",
    "print(f\"\\nüì¶ All exports complete! Files saved to {export_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Advanced Querying with Custom Prompts\n",
    "\n",
    "Demonstrate advanced querying capabilities with custom prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_query(graphrag_pipeline, query: str, custom_instructions: str = None):\n",
    "    \"\"\"Execute a query with custom instructions for response generation\"\"\"\n",
    "    \n",
    "    # Retrieve context\n",
    "    retrieval_results = graphrag_pipeline.retrieve(query)\n",
    "    \n",
    "    # Build custom prompt\n",
    "    if custom_instructions:\n",
    "        system_prompt = f\"\"\"\n",
    "        You are a helpful assistant that answers questions using provided context.\n",
    "        {custom_instructions}\n",
    "        \"\"\"\n",
    "    else:\n",
    "        system_prompt = \"You are a helpful assistant that answers questions using provided context.\"\n",
    "    \n",
    "    # Generate response with custom prompt\n",
    "    vector_context = \"\\n\\n\".join([\n",
    "        f\"{doc.page_content}\" for doc, _ in retrieval_results[\"vector_results\"]\n",
    "    ])\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"user\", \"\"\"Query: {query}\n",
    "        \n",
    "        Context: {context}\n",
    "        \n",
    "        Entities mentioned: {entities}\n",
    "        \n",
    "        Please provide your response based on the context.\"\"\")\n",
    "    ])\n",
    "    \n",
    "    chain = LLMChain(llm=graphrag_pipeline.llm, prompt=prompt)\n",
    "    response = chain.run(\n",
    "        query=query,\n",
    "        context=vector_context,\n",
    "        entities=\", \".join(retrieval_results[\"related_entities\"][:10])\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Example custom queries\n",
    "print(\"üéØ Custom Query Examples\\n\")\n",
    "\n",
    "# Query 1: Comparative analysis\n",
    "print(\"Query 1: Comparative Analysis\")\n",
    "print(\"=\"*50)\n",
    "response = custom_query(\n",
    "    graphrag,\n",
    "    \"Compare the founding stories of Apple and Microsoft\",\n",
    "    \"Provide a detailed comparison highlighting similarities and differences. Use bullet points for clarity.\"\n",
    ")\n",
    "print(response)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Query 2: Timeline extraction\n",
    "print(\"Query 2: Timeline Extraction\")\n",
    "print(\"=\"*50)\n",
    "response = custom_query(\n",
    "    graphrag,\n",
    "    \"What are the key dates and milestones mentioned?\",\n",
    "    \"Extract all dates and associated events. Format as a chronological timeline.\"\n",
    ")\n",
    "print(response)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Query 3: Relationship mapping\n",
    "print(\"Query 3: Relationship Mapping\")\n",
    "print(\"=\"*50)\n",
    "response = custom_query(\n",
    "    graphrag,\n",
    "    \"What are the relationships between the CEOs and their companies?\",\n",
    "    \"Focus on succession patterns and leadership transitions. Include dates where available.\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Performance Optimization Tips\n",
    "\n",
    "Best practices and optimization strategies for GraphRAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display optimization recommendations\n",
    "optimization_tips = \"\"\"\n",
    "# GraphRAG Performance Optimization Guide\n",
    "\n",
    "## 1. Document Processing Optimization\n",
    "- **Batch Processing**: Process documents in batches of 10-20 for optimal memory usage\n",
    "- **Chunk Size**: Adjust chunk_size (500-1500) based on document complexity\n",
    "- **Overlap**: Use 10-20% overlap for better context preservation\n",
    "\n",
    "## 2. Entity Extraction Optimization\n",
    "- **Confidence Threshold**: Set min_entity_confidence to 0.7-0.8 to filter noise\n",
    "- **Entity Limits**: Limit entities per chunk (10-15) to prevent graph explosion\n",
    "- **Caching**: Cache entity extractions for repeated documents\n",
    "\n",
    "## 3. Graph Structure Optimization\n",
    "- **Pruning**: Remove nodes with degree < 2 for cleaner graphs\n",
    "- **Merging**: Merge similar entities (using string similarity or embeddings)\n",
    "- **Indexing**: Create indexes on frequently queried node properties\n",
    "\n",
    "## 4. Retrieval Optimization\n",
    "- **Hybrid Search**: Combine vector search with BM25 for better results\n",
    "- **Graph Traversal Depth**: Limit to 2-3 hops for efficiency\n",
    "- **Caching**: Cache frequent queries and their results\n",
    "\n",
    "## 5. Scalability Considerations\n",
    "- **Graph Database**: Use Neo4j or ArangoDB for graphs > 100K nodes\n",
    "- **Vector Database**: Use Pinecone or Weaviate for production\n",
    "- **Distributed Processing**: Use Ray or Dask for large document sets\n",
    "\n",
    "## 6. Cost Optimization\n",
    "- **Model Selection**: Use smaller models for entity extraction (gpt-3.5-turbo)\n",
    "- **Batch API Calls**: Batch multiple chunks in single API calls\n",
    "- **Local Models**: Consider using local models (Llama, Mistral) for extraction\n",
    "\n",
    "## 7. Quality Improvements\n",
    "- **Validation**: Implement entity/relationship validation logic\n",
    "- **Feedback Loop**: Track query success and refine extraction prompts\n",
    "- **Human-in-the-loop**: Add review step for critical entities\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(optimization_tips))\n",
    "\n",
    "# Performance benchmarking function\n",
    "def benchmark_graphrag(graphrag_pipeline, test_queries: List[str]):\n",
    "    \"\"\"Benchmark GraphRAG performance\"\"\"\n",
    "    import time\n",
    "    \n",
    "    results = []\n",
    "    for query in test_queries:\n",
    "        start = time.time()\n",
    "        result = graphrag_pipeline.query(query)\n",
    "        end = time.time()\n",
    "        \n",
    "        results.append({\n",
    "            \"query\": query[:50] + \"...\",\n",
    "            \"response_time\": end - start,\n",
    "            \"entities_found\": len(result['retrieval_results']['query_entities']),\n",
    "            \"docs_retrieved\": len(result['retrieval_results']['vector_results']),\n",
    "            \"graph_nodes\": len(result['retrieval_results']['related_entities'])\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run benchmark\n",
    "print(\"\\nüìä Performance Benchmark Results:\")\n",
    "benchmark_queries = [\n",
    "    \"Who founded Apple?\",\n",
    "    \"What are Microsoft's main products?\",\n",
    "    \"Compare cloud services offered by these companies\"\n",
    "]\n",
    "benchmark_df = benchmark_graphrag(graphrag, benchmark_queries)\n",
    "display(benchmark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated a comprehensive GraphRAG implementation featuring:\n",
    "\n",
    "### ‚úÖ Key Capabilities Implemented:\n",
    "- **Document Processing**: Chunking and metadata management\n",
    "- **Entity & Relationship Extraction**: LLM-based extraction with fallback\n",
    "- **Knowledge Graph Construction**: NetworkX-based graph with rich properties\n",
    "- **Hybrid Retrieval**: Combining vector search and graph traversal\n",
    "- **Context-Aware Generation**: Enhanced responses using graph context\n",
    "- **Visualization**: Interactive graph visualization with Pyvis\n",
    "- **Analysis Tools**: PageRank, centrality, and connectivity metrics\n",
    "- **Export Capabilities**: Save graphs and results for reuse\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "1. **Scale Up**: Process larger document collections\n",
    "2. **Enhance Extraction**: Fine-tune prompts for your domain\n",
    "3. **Production Deployment**: Migrate to graph databases (Neo4j)\n",
    "4. **Advanced Features**: Add temporal reasoning, multi-hop queries\n",
    "5. **Integration**: Connect with existing data pipelines\n",
    "\n",
    "### üìö Resources:\n",
    "- [LangChain Documentation](https://python.langchain.com/)\n",
    "- [NetworkX Documentation](https://networkx.org/)\n",
    "- [OpenAI API Reference](https://platform.openai.com/docs/)\n",
    "- [Graph Database Options](https://neo4j.com/)\n",
    "\n",
    "This implementation provides a solid foundation for building production-ready GraphRAG systems that combine the power of knowledge graphs with modern retrieval-augmented generation techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}