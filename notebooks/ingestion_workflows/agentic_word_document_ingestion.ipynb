{
 "cells": [
   {
      "cell_type": "markdown",
      "id": "6a44f001",
      "metadata": {
        "id": "6a44f001"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dimitarpg13/agentic_architectures_and_design_patterns/blob/main/notebooks/observability/braintrust_agentic_observability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic Workflow for Word Document Ingestion\n",
    "\n",
    "This notebook demonstrates a multi-agent system for intelligently ingesting Word documents and using them as context for LLM interactions.\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "The system consists of multiple specialized agents:\n",
    "- **Document Parser Agent**: Extracts and structures content from Word documents\n",
    "- **Content Analyzer Agent**: Analyzes document structure, identifies key sections, and creates metadata\n",
    "- **Chunking Strategy Agent**: Determines optimal chunking strategy based on document type\n",
    "- **Context Builder Agent**: Assembles relevant context for LLM queries\n",
    "- **Supervisor Agent**: Orchestrates the workflow and handles user queries\n",
    "\n",
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-docx langgraph langchain langchain-anthropic langchain-community tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import TypedDict, Annotated, List, Dict, Any, Optional\n",
    "from docx import Document\n",
    "from docx.table import Table\n",
    "from docx.text.paragraph import Paragraph\n",
    "import json\n",
    "from operator import add\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "import tiktoken\n",
    "\n",
    "# Set your API key\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"your-api-key-here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define State Schema\n",
    "\n",
    "The state is shared across all agents in the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"State passed between agents in the workflow.\"\"\"\n",
    "    \n",
    "    # Input\n",
    "    document_path: str\n",
    "    user_query: Optional[str]\n",
    "    \n",
    "    # Document parsing results\n",
    "    raw_text: str\n",
    "    paragraphs: List[str]\n",
    "    tables: List[Dict[str, Any]]\n",
    "    \n",
    "    # Analysis results\n",
    "    document_type: str\n",
    "    key_sections: List[Dict[str, str]]\n",
    "    metadata: Dict[str, Any]\n",
    "    \n",
    "    # Chunking results\n",
    "    chunks: List[Dict[str, Any]]\n",
    "    chunking_strategy: str\n",
    "    \n",
    "    # Context building\n",
    "    relevant_context: str\n",
    "    context_metadata: Dict[str, Any]\n",
    "    \n",
    "    # Final output\n",
    "    response: str\n",
    "    \n",
    "    # Agent messages\n",
    "    messages: Annotated[List[str], add]\n",
    "    \n",
    "    # Control flow\n",
    "    next_agent: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text: str, model: str = \"cl100k_base\") -> int:\n",
    "    \"\"\"Count tokens in text using tiktoken.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def extract_table_data(table: Table) -> Dict[str, Any]:\n",
    "    \"\"\"Extract data from a Word table.\"\"\"\n",
    "    data = []\n",
    "    for row in table.rows:\n",
    "        row_data = [cell.text.strip() for cell in row.cells]\n",
    "        data.append(row_data)\n",
    "    \n",
    "    # Try to identify headers\n",
    "    headers = data[0] if data else []\n",
    "    rows = data[1:] if len(data) > 1 else []\n",
    "    \n",
    "    return {\n",
    "        \"headers\": headers,\n",
    "        \"rows\": rows,\n",
    "        \"raw_data\": data\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent 1: Document Parser\n",
    "\n",
    "Extracts content from Word documents including text, tables, and structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_parser_agent(state: AgentState) -> AgentState:\n",
    "    \"\"\"Parse Word document and extract structured content.\"\"\"\n",
    "    \n",
    "    print(\"üìÑ Document Parser Agent: Parsing document...\")\n",
    "    \n",
    "    try:\n",
    "        doc = Document(state[\"document_path\"])\n",
    "        \n",
    "        # Extract paragraphs\n",
    "        paragraphs = []\n",
    "        raw_text = []\n",
    "        \n",
    "        for para in doc.paragraphs:\n",
    "            text = para.text.strip()\n",
    "            if text:\n",
    "                paragraphs.append({\n",
    "                    \"text\": text,\n",
    "                    \"style\": para.style.name,\n",
    "                    \"is_heading\": para.style.name.startswith('Heading')\n",
    "                })\n",
    "                raw_text.append(text)\n",
    "        \n",
    "        # Extract tables\n",
    "        tables = []\n",
    "        for table in doc.tables:\n",
    "            table_data = extract_table_data(table)\n",
    "            tables.append(table_data)\n",
    "        \n",
    "        state[\"raw_text\"] = \"\\n\".join(raw_text)\n",
    "        state[\"paragraphs\"] = paragraphs\n",
    "        state[\"tables\"] = tables\n",
    "        state[\"messages\"] = [f\"Parsed document: {len(paragraphs)} paragraphs, {len(tables)} tables\"]\n",
    "        state[\"next_agent\"] = \"analyzer\"\n",
    "        \n",
    "        print(f\"  ‚úì Extracted {len(paragraphs)} paragraphs and {len(tables)} tables\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        state[\"messages\"] = [f\"Error parsing document: {str(e)}\"]\n",
    "        state[\"next_agent\"] = \"end\"\n",
    "        print(f\"  ‚úó Error: {str(e)}\")\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent 2: Content Analyzer\n",
    "\n",
    "Analyzes document structure and identifies key sections using LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_analyzer_agent(state: AgentState) -> AgentState:\n",
    "    \"\"\"Analyze document content and identify structure.\"\"\"\n",
    "    \n",
    "    print(\"üîç Content Analyzer Agent: Analyzing document structure...\")\n",
    "    \n",
    "    llm = ChatAnthropic(model=\"claude-sonnet-4-20250514\", temperature=0)\n",
    "    \n",
    "    # Create a preview of the document\n",
    "    preview_paragraphs = state[\"paragraphs\"][:20]  # First 20 paragraphs\n",
    "    preview_text = \"\\n\\n\".join([p[\"text\"] for p in preview_paragraphs])\n",
    "    \n",
    "    # Identify headings\n",
    "    headings = [p for p in state[\"paragraphs\"] if p[\"is_heading\"]]\n",
    "    \n",
    "    analysis_prompt = f\"\"\"\n",
    "    Analyze this document and provide:\n",
    "    1. Document type (e.g., technical report, business proposal, research paper, manual, etc.)\n",
    "    2. Key sections and their purposes\n",
    "    3. Main topics covered\n",
    "    4. Suggested chunking approach (semantic, fixed-size, section-based)\n",
    "    \n",
    "    Headings found:\n",
    "    {json.dumps([h['text'] for h in headings[:10]], indent=2)}\n",
    "    \n",
    "    Document preview:\n",
    "    {preview_text[:2000]}\n",
    "    \n",
    "    Respond in JSON format:\n",
    "    {{\n",
    "      \"document_type\": \"...\",\n",
    "      \"key_sections\": [\n",
    "        {{\"title\": \"...\", \"purpose\": \"...\"}}\n",
    "      ],\n",
    "      \"main_topics\": [\"...\"],\n",
    "      \"chunking_strategy\": \"semantic|fixed|section\",\n",
    "      \"reasoning\": \"...\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = llm.invoke([\n",
    "            SystemMessage(content=\"You are a document analysis expert. Provide concise, structured analysis.\"),\n",
    "            HumanMessage(content=analysis_prompt)\n",
    "        ])\n",
    "        \n",
    "        # Parse JSON response\n",
    "        analysis = json.loads(response.content)\n",
    "        \n",
    "        state[\"document_type\"] = analysis.get(\"document_type\", \"unknown\")\n",
    "        state[\"key_sections\"] = analysis.get(\"key_sections\", [])\n",
    "        state[\"chunking_strategy\"] = analysis.get(\"chunking_strategy\", \"semantic\")\n",
    "        state[\"metadata\"] = {\n",
    "            \"main_topics\": analysis.get(\"main_topics\", []),\n",
    "            \"analysis_reasoning\": analysis.get(\"reasoning\", \"\"),\n",
    "            \"num_paragraphs\": len(state[\"paragraphs\"]),\n",
    "            \"num_tables\": len(state[\"tables\"]),\n",
    "            \"token_count\": count_tokens(state[\"raw_text\"])\n",
    "        }\n",
    "        \n",
    "        state[\"messages\"] = [f\"Analyzed document: {state['document_type']}\"]\n",
    "        state[\"next_agent\"] = \"chunker\"\n",
    "        \n",
    "        print(f\"  ‚úì Document type: {state['document_type']}\")\n",
    "        print(f\"  ‚úì Chunking strategy: {state['chunking_strategy']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Error in analysis: {str(e)}\")\n",
    "        # Fallback to basic analysis\n",
    "        state[\"document_type\"] = \"document\"\n",
    "        state[\"chunking_strategy\"] = \"semantic\"\n",
    "        state[\"key_sections\"] = []\n",
    "        state[\"metadata\"] = {\"token_count\": count_tokens(state[\"raw_text\"])}\n",
    "        state[\"messages\"] = [\"Used fallback analysis\"]\n",
    "        state[\"next_agent\"] = \"chunker\"\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent 3: Chunking Strategy Agent\n",
    "\n",
    "Implements intelligent chunking based on document analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunking_agent(state: AgentState) -> AgentState:\n",
    "    \"\"\"Chunk document based on optimal strategy.\"\"\"\n",
    "    \n",
    "    print(f\"‚úÇÔ∏è  Chunking Agent: Using {state['chunking_strategy']} strategy...\")\n",
    "    \n",
    "    chunks = []\n",
    "    strategy = state[\"chunking_strategy\"]\n",
    "    \n",
    "    if strategy == \"section\":\n",
    "        # Chunk by sections (headings)\n",
    "        current_chunk = []\n",
    "        current_heading = \"Introduction\"\n",
    "        \n",
    "        for para in state[\"paragraphs\"]:\n",
    "            if para[\"is_heading\"]:\n",
    "                if current_chunk:\n",
    "                    chunks.append({\n",
    "                        \"text\": \"\\n\".join(current_chunk),\n",
    "                        \"section\": current_heading,\n",
    "                        \"type\": \"section\",\n",
    "                        \"token_count\": count_tokens(\"\\n\".join(current_chunk))\n",
    "                    })\n",
    "                current_chunk = []\n",
    "                current_heading = para[\"text\"]\n",
    "            current_chunk.append(para[\"text\"])\n",
    "        \n",
    "        # Add final chunk\n",
    "        if current_chunk:\n",
    "            chunks.append({\n",
    "                \"text\": \"\\n\".join(current_chunk),\n",
    "                \"section\": current_heading,\n",
    "                \"type\": \"section\",\n",
    "                \"token_count\": count_tokens(\"\\n\".join(current_chunk))\n",
    "            })\n",
    "    \n",
    "    elif strategy == \"fixed\":\n",
    "        # Fixed-size chunking with overlap\n",
    "        chunk_size = 500  # tokens\n",
    "        overlap = 50\n",
    "        \n",
    "        all_text = state[\"raw_text\"]\n",
    "        words = all_text.split()\n",
    "        \n",
    "        # Simple word-based chunking (approximation)\n",
    "        words_per_chunk = chunk_size // 1.3  # rough estimate\n",
    "        overlap_words = overlap // 1.3\n",
    "        \n",
    "        for i in range(0, len(words), int(words_per_chunk - overlap_words)):\n",
    "            chunk_text = \" \".join(words[i:i+int(words_per_chunk)])\n",
    "            if chunk_text:\n",
    "                chunks.append({\n",
    "                    \"text\": chunk_text,\n",
    "                    \"section\": f\"Chunk {len(chunks)+1}\",\n",
    "                    \"type\": \"fixed\",\n",
    "                    \"token_count\": count_tokens(chunk_text)\n",
    "                })\n",
    "    \n",
    "    else:  # semantic (default)\n",
    "        # Semantic chunking - group by topic/meaning\n",
    "        # For simplicity, we'll use paragraph-level with small groups\n",
    "        current_chunk = []\n",
    "        current_tokens = 0\n",
    "        max_chunk_tokens = 800\n",
    "        \n",
    "        for para in state[\"paragraphs\"]:\n",
    "            para_tokens = count_tokens(para[\"text\"])\n",
    "            \n",
    "            if current_tokens + para_tokens > max_chunk_tokens and current_chunk:\n",
    "                chunks.append({\n",
    "                    \"text\": \"\\n\".join([p[\"text\"] for p in current_chunk]),\n",
    "                    \"section\": current_chunk[0][\"text\"][:50] + \"...\",\n",
    "                    \"type\": \"semantic\",\n",
    "                    \"token_count\": current_tokens\n",
    "                })\n",
    "                current_chunk = []\n",
    "                current_tokens = 0\n",
    "            \n",
    "            current_chunk.append(para)\n",
    "            current_tokens += para_tokens\n",
    "        \n",
    "        # Add final chunk\n",
    "        if current_chunk:\n",
    "            chunks.append({\n",
    "                \"text\": \"\\n\".join([p[\"text\"] for p in current_chunk]),\n",
    "                \"section\": current_chunk[0][\"text\"][:50] + \"...\",\n",
    "                \"type\": \"semantic\",\n",
    "                \"token_count\": current_tokens\n",
    "            })\n",
    "    \n",
    "    # Add tables as separate chunks\n",
    "    for i, table in enumerate(state[\"tables\"]):\n",
    "        table_text = \"\\n\".join([\"|\" + \"|\".join(row) + \"|\" for row in table[\"raw_data\"]])\n",
    "        chunks.append({\n",
    "            \"text\": table_text,\n",
    "            \"section\": f\"Table {i+1}\",\n",
    "            \"type\": \"table\",\n",
    "            \"token_count\": count_tokens(table_text)\n",
    "        })\n",
    "    \n",
    "    state[\"chunks\"] = chunks\n",
    "    state[\"messages\"] = [f\"Created {len(chunks)} chunks\"]\n",
    "    state[\"next_agent\"] = \"context_builder\"\n",
    "    \n",
    "    print(f\"  ‚úì Created {len(chunks)} chunks\")\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent 4: Context Builder\n",
    "\n",
    "Selects and assembles relevant context for user queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_builder_agent(state: AgentState) -> AgentState:\n",
    "    \"\"\"Build relevant context based on user query.\"\"\"\n",
    "    \n",
    "    print(\"üèóÔ∏è  Context Builder Agent: Assembling context...\")\n",
    "    \n",
    "    if not state.get(\"user_query\"):\n",
    "        # No query - return full document summary\n",
    "        context = f\"\"\"Document Type: {state['document_type']}\n",
    "        \n",
    "Key Sections:\n",
    "{json.dumps(state['key_sections'], indent=2)}\n",
    "\n",
    "Full Content:\n",
    "{state['raw_text'][:5000]}...\n",
    "\"\"\"\n",
    "        state[\"relevant_context\"] = context\n",
    "        state[\"context_metadata\"] = {\"strategy\": \"full_document\"}\n",
    "        state[\"next_agent\"] = \"supervisor\"\n",
    "        return state\n",
    "    \n",
    "    # Use LLM to identify relevant chunks\n",
    "    llm = ChatAnthropic(model=\"claude-sonnet-4-20250514\", temperature=0)\n",
    "    \n",
    "    # Create chunk summaries for selection\n",
    "    chunk_summaries = []\n",
    "    for i, chunk in enumerate(state[\"chunks\"]):\n",
    "        chunk_summaries.append({\n",
    "            \"id\": i,\n",
    "            \"section\": chunk[\"section\"],\n",
    "            \"preview\": chunk[\"text\"][:200],\n",
    "            \"tokens\": chunk[\"token_count\"]\n",
    "        })\n",
    "    \n",
    "    selection_prompt = f\"\"\"\n",
    "    Given the user query: \"{state['user_query']}\"\n",
    "    \n",
    "    Select the most relevant chunks from this document:\n",
    "    {json.dumps(chunk_summaries, indent=2)}\n",
    "    \n",
    "    Respond with JSON:\n",
    "    {{\n",
    "      \"selected_chunk_ids\": [0, 3, 5],\n",
    "      \"reasoning\": \"why these chunks are relevant\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = llm.invoke([\n",
    "            SystemMessage(content=\"You are an expert at identifying relevant document sections.\"),\n",
    "            HumanMessage(content=selection_prompt)\n",
    "        ])\n",
    "        \n",
    "        selection = json.loads(response.content)\n",
    "        selected_ids = selection.get(\"selected_chunk_ids\", [])\n",
    "        \n",
    "        # Assemble context from selected chunks\n",
    "        relevant_chunks = [state[\"chunks\"][i] for i in selected_ids if i < len(state[\"chunks\"])]\n",
    "        \n",
    "        context_parts = [f\"Document Type: {state['document_type']}\\n\"]\n",
    "        for chunk in relevant_chunks:\n",
    "            context_parts.append(f\"\\n--- {chunk['section']} ---\\n{chunk['text']}\\n\")\n",
    "        \n",
    "        state[\"relevant_context\"] = \"\\n\".join(context_parts)\n",
    "        state[\"context_metadata\"] = {\n",
    "            \"strategy\": \"query_based\",\n",
    "            \"selected_chunks\": len(relevant_chunks),\n",
    "            \"reasoning\": selection.get(\"reasoning\", \"\")\n",
    "        }\n",
    "        \n",
    "        print(f\"  ‚úì Selected {len(relevant_chunks)} relevant chunks\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Error in selection: {str(e)}\")\n",
    "        # Fallback - use first few chunks\n",
    "        context_parts = [f\"Document Type: {state['document_type']}\\n\"]\n",
    "        for chunk in state[\"chunks\"][:3]:\n",
    "            context_parts.append(f\"\\n--- {chunk['section']} ---\\n{chunk['text']}\\n\")\n",
    "        state[\"relevant_context\"] = \"\\n\".join(context_parts)\n",
    "        state[\"context_metadata\"] = {\"strategy\": \"fallback\"}\n",
    "    \n",
    "    state[\"messages\"] = [\"Context assembled\"]\n",
    "    state[\"next_agent\"] = \"supervisor\"\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent 5: Supervisor\n",
    "\n",
    "Orchestrates the workflow and generates final responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervisor_agent(state: AgentState) -> AgentState:\n",
    "    \"\"\"Supervise workflow and generate final response.\"\"\"\n",
    "    \n",
    "    print(\"üëî Supervisor Agent: Generating response...\")\n",
    "    \n",
    "    llm = ChatAnthropic(model=\"claude-sonnet-4-20250514\", temperature=0.7)\n",
    "    \n",
    "    if not state.get(\"user_query\"):\n",
    "        # No query - provide document summary\n",
    "        summary_prompt = f\"\"\"\n",
    "        Provide a comprehensive summary of this document:\n",
    "        \n",
    "        {state['relevant_context']}\n",
    "        \n",
    "        Include:\n",
    "        - Document type and purpose\n",
    "        - Key topics and sections\n",
    "        - Main takeaways\n",
    "        \"\"\"\n",
    "        \n",
    "        response = llm.invoke([\n",
    "            SystemMessage(content=\"You are a document summarization expert.\"),\n",
    "            HumanMessage(content=summary_prompt)\n",
    "        ])\n",
    "        \n",
    "        state[\"response\"] = response.content\n",
    "    else:\n",
    "        # Answer user query with context\n",
    "        query_prompt = f\"\"\"\n",
    "        Based on the following document context, answer the user's question.\n",
    "        \n",
    "        Context:\n",
    "        {state['relevant_context']}\n",
    "        \n",
    "        User Question: {state['user_query']}\n",
    "        \n",
    "        Provide a detailed, accurate answer based on the document. If the information\n",
    "        isn't in the document, say so.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = llm.invoke([\n",
    "            SystemMessage(content=\"You are a helpful assistant with access to document context.\"),\n",
    "            HumanMessage(content=query_prompt)\n",
    "        ])\n",
    "        \n",
    "        state[\"response\"] = response.content\n",
    "    \n",
    "    state[\"messages\"] = [\"Response generated\"]\n",
    "    state[\"next_agent\"] = \"end\"\n",
    "    \n",
    "    print(\"  ‚úì Response generated\")\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the LangGraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_workflow():\n",
    "    \"\"\"Create the agentic workflow graph.\"\"\"\n",
    "    \n",
    "    workflow = StateGraph(AgentState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"parser\", document_parser_agent)\n",
    "    workflow.add_node(\"analyzer\", content_analyzer_agent)\n",
    "    workflow.add_node(\"chunker\", chunking_agent)\n",
    "    workflow.add_node(\"context_builder\", context_builder_agent)\n",
    "    workflow.add_node(\"supervisor\", supervisor_agent)\n",
    "    \n",
    "    # Add edges\n",
    "    workflow.set_entry_point(\"parser\")\n",
    "    \n",
    "    # Conditional routing based on next_agent\n",
    "    def route(state: AgentState):\n",
    "        next_agent = state.get(\"next_agent\", \"end\")\n",
    "        if next_agent == \"end\":\n",
    "            return END\n",
    "        return next_agent\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"parser\",\n",
    "        route,\n",
    "        {\"analyzer\": \"analyzer\", \"end\": END}\n",
    "    )\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"analyzer\",\n",
    "        route,\n",
    "        {\"chunker\": \"chunker\", \"end\": END}\n",
    "    )\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"chunker\",\n",
    "        route,\n",
    "        {\"context_builder\": \"context_builder\", \"end\": END}\n",
    "    )\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"context_builder\",\n",
    "        route,\n",
    "        {\"supervisor\": \"supervisor\", \"end\": END}\n",
    "    )\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"supervisor\",\n",
    "        route,\n",
    "        {\"end\": END}\n",
    "    )\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# Create the workflow\n",
    "app = create_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "### 1. Process Document Without Query (Summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample Word document for testing\n",
    "from docx import Document\n",
    "\n",
    "doc = Document()\n",
    "doc.add_heading('Sample Technical Report', 0)\n",
    "doc.add_heading('Introduction', 1)\n",
    "doc.add_paragraph('This is a sample document demonstrating the agentic workflow.')\n",
    "doc.add_paragraph('It contains multiple sections with different types of content.')\n",
    "\n",
    "doc.add_heading('Methodology', 1)\n",
    "doc.add_paragraph('Our approach consists of three main phases:')\n",
    "doc.add_paragraph('1. Data collection and preprocessing')\n",
    "doc.add_paragraph('2. Model training and validation')\n",
    "doc.add_paragraph('3. Deployment and monitoring')\n",
    "\n",
    "doc.add_heading('Results', 1)\n",
    "doc.add_paragraph('The system achieved 95% accuracy on the test set.')\n",
    "\n",
    "# Add a table\n",
    "table = doc.add_table(rows=3, cols=3)\n",
    "table.cell(0, 0).text = 'Metric'\n",
    "table.cell(0, 1).text = 'Training'\n",
    "table.cell(0, 2).text = 'Testing'\n",
    "table.cell(1, 0).text = 'Accuracy'\n",
    "table.cell(1, 1).text = '98%'\n",
    "table.cell(1, 2).text = '95%'\n",
    "table.cell(2, 0).text = 'F1 Score'\n",
    "table.cell(2, 1).text = '0.97'\n",
    "table.cell(2, 2).text = '0.94'\n",
    "\n",
    "doc.add_heading('Conclusion', 1)\n",
    "doc.add_paragraph('The results demonstrate the effectiveness of our approach.')\n",
    "\n",
    "doc.save('sample_document.docx')\n",
    "print(\"‚úì Sample document created: sample_document.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run workflow for document summary\n",
    "initial_state = {\n",
    "    \"document_path\": \"sample_document.docx\",\n",
    "    \"user_query\": None,\n",
    "    \"messages\": []\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"WORKFLOW EXECUTION: DOCUMENT SUMMARY\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "result = app.invoke(initial_state)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nResponse:\\n{result['response']}\")\n",
    "print(f\"\\nMetadata:\")\n",
    "print(f\"  Document Type: {result.get('document_type', 'N/A')}\")\n",
    "print(f\"  Chunks Created: {len(result.get('chunks', []))}\")\n",
    "print(f\"  Chunking Strategy: {result.get('chunking_strategy', 'N/A')}\")\n",
    "print(f\"  Token Count: {result.get('metadata', {}).get('token_count', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Process Document With User Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run workflow with a specific query\n",
    "query_state = {\n",
    "    \"document_path\": \"sample_document.docx\",\n",
    "    \"user_query\": \"What were the accuracy results?\",\n",
    "    \"messages\": []\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"WORKFLOW EXECUTION: QUERY-BASED\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "result = app.invoke(query_state)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nQuery: {query_state['user_query']}\")\n",
    "print(f\"\\nResponse:\\n{result['response']}\")\n",
    "print(f\"\\nContext Metadata:\")\n",
    "print(json.dumps(result.get('context_metadata', {}), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Interactive Query Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_document_qa(document_path: str):\n",
    "    \"\"\"Interactive Q&A session with a document.\"\"\"\n",
    "    \n",
    "    # First, process the document\n",
    "    print(\"Processing document...\\n\")\n",
    "    initial_state = {\n",
    "        \"document_path\": document_path,\n",
    "        \"user_query\": None,\n",
    "        \"messages\": []\n",
    "    }\n",
    "    \n",
    "    # Get initial processing done\n",
    "    base_result = app.invoke(initial_state)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Document processed! You can now ask questions.\")\n",
    "    print(\"Type 'quit' to exit.\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"\\nYour question: \")\n",
    "        \n",
    "        if query.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Process query with existing chunks\n",
    "        query_state = {\n",
    "            **base_result,\n",
    "            \"user_query\": query,\n",
    "            \"messages\": [],\n",
    "            \"next_agent\": \"context_builder\"  # Skip to context building\n",
    "        }\n",
    "        \n",
    "        # Create a mini-workflow for querying only\n",
    "        query_workflow = StateGraph(AgentState)\n",
    "        query_workflow.add_node(\"context_builder\", context_builder_agent)\n",
    "        query_workflow.add_node(\"supervisor\", supervisor_agent)\n",
    "        query_workflow.set_entry_point(\"context_builder\")\n",
    "        query_workflow.add_edge(\"context_builder\", \"supervisor\")\n",
    "        query_workflow.add_edge(\"supervisor\", END)\n",
    "        query_app = query_workflow.compile()\n",
    "        \n",
    "        result = query_app.invoke(query_state)\n",
    "        \n",
    "        print(f\"\\nAnswer:\\n{result['response']}\\n\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "# Uncomment to run interactively\n",
    "# interactive_document_qa(\"sample_document.docx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Workflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    # This requires graphviz to be installed\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display graph: {e}\")\n",
    "    print(\"\\nWorkflow structure:\")\n",
    "    print(\"parser -> analyzer -> chunker -> context_builder -> supervisor -> END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Enhancements\n",
    "\n",
    "For production use, consider adding:\n",
    "\n",
    "1. **Vector Storage**: Store chunks in a vector database (Pinecone, Weaviate, Chroma) for semantic search\n",
    "2. **Caching**: Cache parsed documents and embeddings\n",
    "3. **Error Handling**: More robust error handling and retry logic\n",
    "4. **Monitoring**: Add logging and metrics tracking\n",
    "5. **Multi-document**: Support for processing multiple documents\n",
    "6. **Streaming**: Stream responses for better UX\n",
    "7. **Memory**: Add conversation memory for follow-up questions\n",
    "8. **Security**: Validate and sanitize document inputs\n",
    "9. **Optimization**: Parallel processing where possible\n",
    "10. **Advanced RAG**: Implement hybrid search, re-ranking, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension: Add Vector Search\n",
    "\n",
    "Here's how you'd add vector search for better retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with ChromaDB (uncomment to use)\n",
    "\"\"\"\n",
    "!pip install chromadb sentence-transformers\n",
    "\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Initialize vector store\n",
    "client = chromadb.Client()\n",
    "embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "collection = client.create_collection(\n",
    "    name=\"document_chunks\",\n",
    "    embedding_function=embedding_function\n",
    ")\n",
    "\n",
    "def add_chunks_to_vectordb(chunks, document_id):\n",
    "    \"\"\"Add chunks to vector database.\"\"\"\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        collection.add(\n",
    "            documents=[chunk['text']],\n",
    "            metadatas=[{\n",
    "                'document_id': document_id,\n",
    "                'chunk_id': i,\n",
    "                'section': chunk['section']\n",
    "            }],\n",
    "            ids=[f\"{document_id}_chunk_{i}\"]\n",
    "        )\n",
    "\n",
    "def semantic_search(query, top_k=3):\n",
    "    \"\"\"Perform semantic search on chunks.\"\"\"\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    return results\n",
    "\"\"\"\n",
    "print(\"Vector search extension example (commented out)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
