{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dimitarpg13/agentic_architectures_and_design_patterns/blob/main/notebooks/model_evaluation/mlflow_bleu_metric_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jT2-wLr0icsA"
      },
      "source": [
        "# MLflow BLEU Metric Demonstration\n",
        "\n",
        "This notebook provides a comprehensive guide to using MLflow's BLEU (Bilingual Evaluation Understudy) metric for evaluating machine translation and text generation models.\n",
        "\n",
        "## What is BLEU?\n",
        "\n",
        "BLEU is a metric for automatically evaluating machine-translated text by comparing it to one or more reference translations. Key characteristics:\n",
        "\n",
        "- **Precision-based**: Measures how many n-grams in the candidate appear in the reference\n",
        "- **Brevity Penalty**: Penalizes translations that are too short\n",
        "- **N-gram Matching**: Typically uses 1-grams through 4-grams\n",
        "- **Corpus-level**: Designed for corpus-level evaluation but can be adapted for sentence-level\n",
        "\n",
        "### BLEU Score Range:\n",
        "- 0.0 to 1.0 (often reported as 0-100)\n",
        "- Higher scores indicate better translation quality\n",
        "- Human translations typically score 0.6-0.8\n",
        "- Good MT systems: 0.3-0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ySoY5o5icsB"
      },
      "source": [
        "## 1. Installation and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-83AXlx1icsB"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q mlflow nltk sacrebleu pandas numpy matplotlib seaborn plotly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ie5f8KywicsC"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import mlflow.metrics\n",
        "from mlflow.metrics import make_metric, MetricValue\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# BLEU implementations\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
        "import sacrebleu\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "\n",
        "print(f\"MLflow version: {mlflow.__version__}\")\n",
        "print(f\"SacreBLEU version: {sacrebleu.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaAfYdU2icsC"
      },
      "source": [
        "## 2. Creating Custom BLEU Metrics for MLflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fipue3OicsC"
      },
      "outputs": [],
      "source": [
        "class BLEUMetrics:\n",
        "    \"\"\"\n",
        "    Comprehensive BLEU metric implementations for MLflow.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def create_bleu_metric(n_gram=4, smoothing=True, use_sacrebleu=True):\n",
        "        \"\"\"\n",
        "        Create a BLEU metric for MLflow evaluation.\n",
        "\n",
        "        Args:\n",
        "            n_gram: Maximum n-gram order (typically 4)\n",
        "            smoothing: Apply smoothing for sentence-level BLEU\n",
        "            use_sacrebleu: Use SacreBLEU (more standardized) vs NLTK\n",
        "        \"\"\"\n",
        "        def bleu_metric(predictions, targets):\n",
        "            if not isinstance(predictions, list):\n",
        "                predictions = [predictions]\n",
        "            if not isinstance(targets, list):\n",
        "                targets = [targets]\n",
        "\n",
        "            scores = []\n",
        "\n",
        "            if use_sacrebleu:\n",
        "                # SacreBLEU implementation\n",
        "                for pred, target in zip(predictions, targets):\n",
        "                    # SacreBLEU expects references as list\n",
        "                    score = sacrebleu.sentence_bleu(pred, [target])\n",
        "                    scores.append(score.score / 100.0)  # Normalize to 0-1\n",
        "            else:\n",
        "                # NLTK implementation\n",
        "                smoothing_fn = SmoothingFunction().method1 if smoothing else None\n",
        "\n",
        "                for pred, target in zip(predictions, targets):\n",
        "                    # Tokenize if strings\n",
        "                    if isinstance(pred, str):\n",
        "                        pred = pred.split()\n",
        "                    if isinstance(target, str):\n",
        "                        target = target.split()\n",
        "\n",
        "                    weights = tuple([1/n_gram] * n_gram)\n",
        "                    score = sentence_bleu(\n",
        "                        [target], pred,\n",
        "                        weights=weights,\n",
        "                        smoothing_function=smoothing_fn\n",
        "                    )\n",
        "                    scores.append(score)\n",
        "\n",
        "            return MetricValue(\n",
        "                aggregate_results={f\"bleu_{n_gram}\": np.mean(scores)},\n",
        "                scores=scores\n",
        "            )\n",
        "\n",
        "        return make_metric(\n",
        "            eval_fn=bleu_metric,\n",
        "            greater_is_better=True,\n",
        "            name=f\"bleu_{n_gram}_{'sacre' if use_sacrebleu else 'nltk'}\"\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def create_multi_bleu_metric():\n",
        "        \"\"\"\n",
        "        Create a metric that computes BLEU-1 through BLEU-4.\n",
        "        \"\"\"\n",
        "        def multi_bleu_metric(predictions, targets):\n",
        "            if not isinstance(predictions, list):\n",
        "                predictions = [predictions]\n",
        "            if not isinstance(targets, list):\n",
        "                targets = [targets]\n",
        "\n",
        "            all_scores = {f\"bleu_{i}\": [] for i in range(1, 5)}\n",
        "\n",
        "            for pred, target in zip(predictions, targets):\n",
        "                # Calculate BLEU-1 through BLEU-4\n",
        "                for n in range(1, 5):\n",
        "                    score = sacrebleu.sentence_bleu(\n",
        "                        pred, [target],\n",
        "                        max_ngram_order=n\n",
        "                    )\n",
        "                    all_scores[f\"bleu_{n}\"].append(score.score / 100.0)\n",
        "\n",
        "            aggregate_results = {\n",
        "                key: np.mean(values)\n",
        "                for key, values in all_scores.items()\n",
        "            }\n",
        "\n",
        "            # Add geometric mean (similar to BLEU-4 but explicit)\n",
        "            aggregate_results[\"bleu_geometric_mean\"] = np.exp(\n",
        "                np.mean([np.log(aggregate_results[f\"bleu_{i}\"]) for i in range(1, 5)])\n",
        "            )\n",
        "\n",
        "            return MetricValue(\n",
        "                aggregate_results=aggregate_results,\n",
        "                scores=all_scores[\"bleu_4\"]  # Return BLEU-4 as primary score\n",
        "            )\n",
        "\n",
        "        return make_metric(\n",
        "            eval_fn=multi_bleu_metric,\n",
        "            greater_is_better=True,\n",
        "            name=\"multi_bleu\"\n",
        "        )\n",
        "\n",
        "# Create metric instances\n",
        "bleu_4_sacre = BLEUMetrics.create_bleu_metric(n_gram=4, use_sacrebleu=True)\n",
        "bleu_4_nltk = BLEUMetrics.create_bleu_metric(n_gram=4, use_sacrebleu=False)\n",
        "multi_bleu = BLEUMetrics.create_multi_bleu_metric()\n",
        "\n",
        "print(\"âœ… Custom BLEU metrics created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9qKu3fkicsD"
      },
      "source": [
        "## 3. Preparing Sample Translation Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9UaRWQsicsD"
      },
      "outputs": [],
      "source": [
        "# Create diverse translation examples\n",
        "translation_data = [\n",
        "    {\n",
        "        \"source\": \"The weather is beautiful today.\",\n",
        "        \"reference\": \"Das Wetter ist heute schÃ¶n.\",\n",
        "        \"model_good\": \"Das Wetter ist heute schÃ¶n.\",\n",
        "        \"model_medium\": \"Das Wetter ist schÃ¶n heute.\",\n",
        "        \"model_poor\": \"Wetter gut heute.\"\n",
        "    },\n",
        "    {\n",
        "        \"source\": \"Machine learning revolutionizes data analysis.\",\n",
        "        \"reference\": \"Maschinelles Lernen revolutioniert die Datenanalyse.\",\n",
        "        \"model_good\": \"Maschinelles Lernen revolutioniert die Datenanalyse.\",\n",
        "        \"model_medium\": \"Machine Learning revolutioniert Datenanalyse.\",\n",
        "        \"model_poor\": \"Computer lernen Daten.\"\n",
        "    },\n",
        "    {\n",
        "        \"source\": \"The cat sits on the comfortable mat.\",\n",
        "        \"reference\": \"Die Katze sitzt auf der bequemen Matte.\",\n",
        "        \"model_good\": \"Die Katze sitzt auf der gemÃ¼tlichen Matte.\",\n",
        "        \"model_medium\": \"Die Katze ist auf der Matte.\",\n",
        "        \"model_poor\": \"Katze Matte.\"\n",
        "    },\n",
        "    {\n",
        "        \"source\": \"Artificial intelligence transforms modern businesses.\",\n",
        "        \"reference\": \"KÃ¼nstliche Intelligenz transformiert moderne Unternehmen.\",\n",
        "        \"model_good\": \"KÃ¼nstliche Intelligenz verwandelt moderne Unternehmen.\",\n",
        "        \"model_medium\": \"KI transformiert Unternehmen.\",\n",
        "        \"model_poor\": \"Computer Ã¤ndern GeschÃ¤ft.\"\n",
        "    },\n",
        "    {\n",
        "        \"source\": \"The quick brown fox jumps over the lazy dog.\",\n",
        "        \"reference\": \"Der schnelle braune Fuchs springt Ã¼ber den faulen Hund.\",\n",
        "        \"model_good\": \"Der schnelle braune Fuchs springt Ã¼ber den trÃ¤gen Hund.\",\n",
        "        \"model_medium\": \"Der braune Fuchs springt Ã¼ber den Hund.\",\n",
        "        \"model_poor\": \"Fuchs springt Hund.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_translations = pd.DataFrame(translation_data)\n",
        "\n",
        "print(f\"ðŸ“Š Created {len(df_translations)} translation examples\")\n",
        "print(\"\\nExample translations:\")\n",
        "for idx, row in df_translations.head(2).iterrows():\n",
        "    print(f\"\\n{idx+1}. Source: {row['source']}\")\n",
        "    print(f\"   Reference: {row['reference']}\")\n",
        "    print(f\"   Good Model: {row['model_good']}\")\n",
        "    print(f\"   Poor Model: {row['model_poor']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUMAtJ5ZicsD"
      },
      "source": [
        "## 4. Evaluating Translation Models with MLflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDzI8GcCicsD"
      },
      "outputs": [],
      "source": [
        "# Set up MLflow experiment\n",
        "mlflow.set_experiment(\"bleu-metrics-demo\")\n",
        "\n",
        "def evaluate_translation_model(model_name, predictions, references, model_config=None):\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation of a translation model using BLEU metrics.\n",
        "    \"\"\"\n",
        "    with mlflow.start_run(run_name=model_name):\n",
        "        # Log model configuration\n",
        "        mlflow.log_param(\"model_name\", model_name)\n",
        "        mlflow.log_param(\"num_samples\", len(predictions))\n",
        "\n",
        "        if model_config:\n",
        "            for key, value in model_config.items():\n",
        "                mlflow.log_param(key, value)\n",
        "\n",
        "        # Create evaluation dataframe\n",
        "        eval_df = pd.DataFrame({\n",
        "            \"predictions\": predictions,\n",
        "            \"targets\": references\n",
        "        })\n",
        "\n",
        "        # Evaluate with BLEU metrics\n",
        "        metrics = [bleu_4_sacre, multi_bleu]\n",
        "\n",
        "        results = mlflow.evaluate(\n",
        "            data=eval_df,\n",
        "            targets=\"targets\",\n",
        "            predictions=\"predictions\",\n",
        "            extra_metrics=metrics,\n",
        "            evaluators=\"default\"\n",
        "        )\n",
        "\n",
        "        # Calculate additional detailed metrics\n",
        "        detailed_scores = calculate_detailed_bleu(predictions, references)\n",
        "\n",
        "        # Log detailed metrics\n",
        "        for key, value in detailed_scores.items():\n",
        "            mlflow.log_metric(key, value)\n",
        "\n",
        "        return results.metrics, detailed_scores\n",
        "\n",
        "def calculate_detailed_bleu(predictions, references):\n",
        "    \"\"\"\n",
        "    Calculate detailed BLEU scores including n-gram precisions.\n",
        "    \"\"\"\n",
        "    scores = {}\n",
        "\n",
        "    # Calculate individual n-gram scores\n",
        "    for n in range(1, 5):\n",
        "        bleu_scores = []\n",
        "        for pred, ref in zip(predictions, references):\n",
        "            score = sacrebleu.sentence_bleu(pred, [ref], max_ngram_order=n)\n",
        "            bleu_scores.append(score.score / 100.0)\n",
        "\n",
        "        scores[f\"bleu_{n}_mean\"] = np.mean(bleu_scores)\n",
        "        scores[f\"bleu_{n}_std\"] = np.std(bleu_scores)\n",
        "        scores[f\"bleu_{n}_min\"] = np.min(bleu_scores)\n",
        "        scores[f\"bleu_{n}_max\"] = np.max(bleu_scores)\n",
        "\n",
        "    # Calculate corpus-level BLEU\n",
        "    corpus_score = sacrebleu.corpus_bleu(predictions, [references])\n",
        "    scores[\"corpus_bleu\"] = corpus_score.score / 100.0\n",
        "\n",
        "    # Add brevity penalty info\n",
        "    scores[\"brevity_penalty\"] = corpus_score.bp\n",
        "\n",
        "    return scores\n",
        "\n",
        "# Evaluate all three models\n",
        "models_config = {\n",
        "    \"High-Quality NMT\": {\n",
        "        \"predictions\": df_translations[\"model_good\"].tolist(),\n",
        "        \"config\": {\"architecture\": \"transformer\", \"params\": \"60M\", \"training_data\": \"10M_pairs\"}\n",
        "    },\n",
        "    \"Medium-Quality NMT\": {\n",
        "        \"predictions\": df_translations[\"model_medium\"].tolist(),\n",
        "        \"config\": {\"architecture\": \"lstm\", \"params\": \"10M\", \"training_data\": \"1M_pairs\"}\n",
        "    },\n",
        "    \"Low-Quality Baseline\": {\n",
        "        \"predictions\": df_translations[\"model_poor\"].tolist(),\n",
        "        \"config\": {\"architecture\": \"word-by-word\", \"params\": \"1M\", \"training_data\": \"100K_pairs\"}\n",
        "    }\n",
        "}\n",
        "\n",
        "evaluation_results = {}\n",
        "for model_name, config in models_config.items():\n",
        "    metrics, detailed = evaluate_translation_model(\n",
        "        model_name,\n",
        "        config[\"predictions\"],\n",
        "        df_translations[\"reference\"].tolist(),\n",
        "        config[\"config\"]\n",
        "    )\n",
        "    evaluation_results[model_name] = {\"metrics\": metrics, \"detailed\": detailed}\n",
        "\n",
        "print(\"âœ… Evaluation completed for all models!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUsGdK6sicsE"
      },
      "source": [
        "## 5. Comprehensive Results Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9v3SMlSDicsE"
      },
      "outputs": [],
      "source": [
        "# Create comprehensive comparison table\n",
        "comparison_data = []\n",
        "\n",
        "for model_name, results in evaluation_results.items():\n",
        "    row = {\"Model\": model_name}\n",
        "\n",
        "    # Add main BLEU scores\n",
        "    for n in range(1, 5):\n",
        "        key = f\"bleu_{n}_mean\"\n",
        "        if key in results[\"detailed\"]:\n",
        "            row[f\"BLEU-{n}\"] = results[\"detailed\"][key]\n",
        "\n",
        "    # Add corpus BLEU and brevity penalty\n",
        "    row[\"Corpus BLEU\"] = results[\"detailed\"].get(\"corpus_bleu\", 0)\n",
        "    row[\"Brevity Penalty\"] = results[\"detailed\"].get(\"brevity_penalty\", 0)\n",
        "\n",
        "    comparison_data.append(row)\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "# Display formatted results\n",
        "print(\"\\nðŸ“Š BLEU Score Comparison (0-1 scale):\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Format for display\n",
        "display_df = comparison_df.copy()\n",
        "for col in display_df.columns:\n",
        "    if col != \"Model\":\n",
        "        display_df[col] = display_df[col].apply(lambda x: f\"{x:.4f}\")\n",
        "\n",
        "print(display_df.to_string(index=False))\n",
        "\n",
        "# Calculate improvements\n",
        "print(\"\\nðŸ“ˆ Performance Analysis:\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "baseline_bleu4 = comparison_df[comparison_df[\"Model\"] == \"Low-Quality Baseline\"][\"BLEU-4\"].values[0]\n",
        "for _, row in comparison_df.iterrows():\n",
        "    if row[\"Model\"] != \"Low-Quality Baseline\":\n",
        "        improvement = ((row[\"BLEU-4\"] - baseline_bleu4) / baseline_bleu4) * 100\n",
        "        print(f\"{row['Model']:25s}: {improvement:+.1f}% improvement over baseline\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_i-3c06yicsE"
      },
      "source": [
        "## 6. Advanced Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQqMxPBgicsE"
      },
      "outputs": [],
      "source": [
        "# Create interactive visualizations with Plotly\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=2,\n",
        "    subplot_titles=(\n",
        "        \"BLEU Scores by N-gram Order\",\n",
        "        \"Model Performance Radar Chart\",\n",
        "        \"Score Distribution\",\n",
        "        \"Sentence-Level BLEU Heatmap\"\n",
        "    ),\n",
        "    specs=[\n",
        "        [{\"type\": \"bar\"}, {\"type\": \"scatterpolar\"}],\n",
        "        [{\"type\": \"box\"}, {\"type\": \"heatmap\"}]\n",
        "    ]\n",
        ")\n",
        "\n",
        "colors = {\n",
        "    \"High-Quality NMT\": \"#2ecc71\",\n",
        "    \"Medium-Quality NMT\": \"#f39c12\",\n",
        "    \"Low-Quality Baseline\": \"#e74c3c\"\n",
        "}\n",
        "\n",
        "# Plot 1: BLEU Scores by N-gram\n",
        "for model_name in comparison_df[\"Model\"]:\n",
        "    row_data = comparison_df[comparison_df[\"Model\"] == model_name]\n",
        "    bleu_scores = [row_data[f\"BLEU-{n}\"].values[0] for n in range(1, 5)]\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Bar(\n",
        "            name=model_name,\n",
        "            x=[\"BLEU-1\", \"BLEU-2\", \"BLEU-3\", \"BLEU-4\"],\n",
        "            y=bleu_scores,\n",
        "            marker_color=colors[model_name]\n",
        "        ),\n",
        "        row=1, col=1\n",
        "    )\n",
        "\n",
        "# Plot 2: Radar Chart\n",
        "for model_name in comparison_df[\"Model\"]:\n",
        "    row_data = comparison_df[comparison_df[\"Model\"] == model_name]\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatterpolar(\n",
        "            r=[\n",
        "                row_data[\"BLEU-1\"].values[0],\n",
        "                row_data[\"BLEU-2\"].values[0],\n",
        "                row_data[\"BLEU-3\"].values[0],\n",
        "                row_data[\"BLEU-4\"].values[0],\n",
        "                row_data[\"Brevity Penalty\"].values[0]\n",
        "            ],\n",
        "            theta=[\"BLEU-1\", \"BLEU-2\", \"BLEU-3\", \"BLEU-4\", \"Brevity\\nPenalty\"],\n",
        "            fill='toself',\n",
        "            name=model_name,\n",
        "            marker_color=colors[model_name]\n",
        "        ),\n",
        "        row=1, col=2\n",
        "    )\n",
        "\n",
        "# Plot 3: Score Distribution (Box Plot)\n",
        "for model_name, config in models_config.items():\n",
        "    # Calculate sentence-level BLEU-4 scores\n",
        "    sentence_scores = []\n",
        "    for pred, ref in zip(config[\"predictions\"], df_translations[\"reference\"].tolist()):\n",
        "        score = sacrebleu.sentence_bleu(pred, [ref])\n",
        "        sentence_scores.append(score.score / 100.0)\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Box(\n",
        "            y=sentence_scores,\n",
        "            name=model_name,\n",
        "            marker_color=colors[model_name]\n",
        "        ),\n",
        "        row=2, col=1\n",
        "    )\n",
        "\n",
        "# Plot 4: Heatmap of sentence-level scores\n",
        "heatmap_data = []\n",
        "for model_name, config in models_config.items():\n",
        "    sentence_scores = []\n",
        "    for pred, ref in zip(config[\"predictions\"], df_translations[\"reference\"].tolist()):\n",
        "        score = sacrebleu.sentence_bleu(pred, [ref])\n",
        "        sentence_scores.append(score.score / 100.0)\n",
        "    heatmap_data.append(sentence_scores)\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Heatmap(\n",
        "        z=heatmap_data,\n",
        "        x=[f\"Sent {i+1}\" for i in range(len(df_translations))],\n",
        "        y=[\"High-Quality\", \"Medium\", \"Low-Quality\"],\n",
        "        colorscale=\"RdYlGn\",\n",
        "        text=np.round(heatmap_data, 3),\n",
        "        texttemplate=\"%{text}\",\n",
        "        textfont={\"size\": 10}\n",
        "    ),\n",
        "    row=2, col=2\n",
        ")\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(\n",
        "    title_text=\"Comprehensive BLEU Metric Analysis\",\n",
        "    showlegend=True,\n",
        "    height=800,\n",
        "    width=1200\n",
        ")\n",
        "\n",
        "fig.update_xaxes(title_text=\"N-gram Order\", row=1, col=1)\n",
        "fig.update_yaxes(title_text=\"BLEU Score\", row=1, col=1)\n",
        "fig.update_yaxes(title_text=\"BLEU-4 Score\", row=2, col=1)\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wniqy2zBicsE"
      },
      "source": [
        "## 7. Production-Ready BLEU Evaluation Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W61w-CIYicsE"
      },
      "outputs": [],
      "source": [
        "class BLEUEvaluationPipeline:\n",
        "    \"\"\"\n",
        "    Production-ready BLEU evaluation pipeline with MLflow integration.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, experiment_name=\"bleu-evaluation\", tracking_uri=None):\n",
        "        self.experiment_name = experiment_name\n",
        "        if tracking_uri:\n",
        "            mlflow.set_tracking_uri(tracking_uri)\n",
        "        mlflow.set_experiment(experiment_name)\n",
        "\n",
        "        # Initialize metrics\n",
        "        self.metrics = {\n",
        "            \"bleu_4_sacre\": BLEUMetrics.create_bleu_metric(4, use_sacrebleu=True),\n",
        "            \"bleu_4_nltk\": BLEUMetrics.create_bleu_metric(4, use_sacrebleu=False),\n",
        "            \"multi_bleu\": BLEUMetrics.create_multi_bleu_metric()\n",
        "        }\n",
        "\n",
        "    def evaluate_model(self,\n",
        "                       model_name: str,\n",
        "                       predictions: List[str],\n",
        "                       references: List[str],\n",
        "                       metadata: Dict = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Evaluate a single model with comprehensive BLEU metrics.\n",
        "        \"\"\"\n",
        "        with mlflow.start_run(run_name=model_name):\n",
        "            # Log metadata\n",
        "            mlflow.log_param(\"model_name\", model_name)\n",
        "            mlflow.log_param(\"num_samples\", len(predictions))\n",
        "\n",
        "            if metadata:\n",
        "                for key, value in metadata.items():\n",
        "                    mlflow.log_param(key, value)\n",
        "\n",
        "            # Calculate all BLEU variants\n",
        "            results = self._calculate_comprehensive_bleu(predictions, references)\n",
        "\n",
        "            # Log metrics\n",
        "            for metric_name, metric_value in results[\"aggregate\"].items():\n",
        "                mlflow.log_metric(metric_name, metric_value)\n",
        "\n",
        "            # Log artifacts\n",
        "            self._log_evaluation_artifacts(predictions, references, results)\n",
        "\n",
        "            return results\n",
        "\n",
        "    def _calculate_comprehensive_bleu(self,\n",
        "                                     predictions: List[str],\n",
        "                                     references: List[str]) -> Dict:\n",
        "        \"\"\"\n",
        "        Calculate comprehensive BLEU metrics.\n",
        "        \"\"\"\n",
        "        results = {\n",
        "            \"sentence_level\": [],\n",
        "            \"aggregate\": {},\n",
        "            \"n_gram_precision\": {}\n",
        "        }\n",
        "\n",
        "        # Sentence-level scores\n",
        "        for pred, ref in zip(predictions, references):\n",
        "            sentence_metrics = {}\n",
        "\n",
        "            # Calculate BLEU-1 through BLEU-4\n",
        "            for n in range(1, 5):\n",
        "                score = sacrebleu.sentence_bleu(pred, [ref], max_ngram_order=n)\n",
        "                sentence_metrics[f\"bleu_{n}\"] = score.score / 100.0\n",
        "\n",
        "            # Add detailed scores\n",
        "            full_score = sacrebleu.sentence_bleu(pred, [ref])\n",
        "            sentence_metrics[\"brevity_penalty\"] = full_score.bp\n",
        "            sentence_metrics[\"length_ratio\"] = full_score.sys_len / full_score.ref_len\n",
        "\n",
        "            results[\"sentence_level\"].append(sentence_metrics)\n",
        "\n",
        "        # Aggregate scores\n",
        "        for metric_name in results[\"sentence_level\"][0].keys():\n",
        "            values = [s[metric_name] for s in results[\"sentence_level\"]]\n",
        "            results[\"aggregate\"][f\"{metric_name}_mean\"] = np.mean(values)\n",
        "            results[\"aggregate\"][f\"{metric_name}_std\"] = np.std(values)\n",
        "            results[\"aggregate\"][f\"{metric_name}_min\"] = np.min(values)\n",
        "            results[\"aggregate\"][f\"{metric_name}_max\"] = np.max(values)\n",
        "\n",
        "        # Corpus-level BLEU\n",
        "        corpus_score = sacrebleu.corpus_bleu(predictions, [references])\n",
        "        results[\"aggregate\"][\"corpus_bleu\"] = corpus_score.score / 100.0\n",
        "        results[\"aggregate\"][\"corpus_brevity_penalty\"] = corpus_score.bp\n",
        "\n",
        "        # N-gram precision breakdown\n",
        "        for n in range(1, 5):\n",
        "            corpus_n = sacrebleu.corpus_bleu(\n",
        "                predictions, [references],\n",
        "                max_ngram_order=n\n",
        "            )\n",
        "            results[\"n_gram_precision\"][f\"precision_{n}_gram\"] = corpus_n.precisions[n-1] / 100.0\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _log_evaluation_artifacts(self,\n",
        "                                 predictions: List[str],\n",
        "                                 references: List[str],\n",
        "                                 results: Dict):\n",
        "        \"\"\"\n",
        "        Log evaluation artifacts to MLflow.\n",
        "        \"\"\"\n",
        "        # Create evaluation report\n",
        "        report = {\n",
        "            \"summary\": results[\"aggregate\"],\n",
        "            \"n_gram_precision\": results[\"n_gram_precision\"],\n",
        "            \"sample_analysis\": [\n",
        "                {\n",
        "                    \"prediction\": pred,\n",
        "                    \"reference\": ref,\n",
        "                    \"scores\": scores\n",
        "                }\n",
        "                for pred, ref, scores in zip(\n",
        "                    predictions[:5],  # Log first 5 samples\n",
        "                    references[:5],\n",
        "                    results[\"sentence_level\"][:5]\n",
        "                )\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        mlflow.log_dict(report, \"evaluation_report.json\")\n",
        "\n",
        "    def compare_models(self,\n",
        "                      models: Dict[str, Tuple[List[str], Dict]],\n",
        "                      references: List[str]) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Compare multiple models.\n",
        "\n",
        "        Args:\n",
        "            models: Dict of model_name -> (predictions, metadata)\n",
        "            references: Reference translations\n",
        "        \"\"\"\n",
        "        comparison_results = []\n",
        "\n",
        "        for model_name, (predictions, metadata) in models.items():\n",
        "            results = self.evaluate_model(\n",
        "                model_name, predictions, references, metadata\n",
        "            )\n",
        "\n",
        "            # Extract key metrics for comparison\n",
        "            row = {\n",
        "                \"Model\": model_name,\n",
        "                \"BLEU-1\": results[\"aggregate\"][\"bleu_1_mean\"],\n",
        "                \"BLEU-2\": results[\"aggregate\"][\"bleu_2_mean\"],\n",
        "                \"BLEU-3\": results[\"aggregate\"][\"bleu_3_mean\"],\n",
        "                \"BLEU-4\": results[\"aggregate\"][\"bleu_4_mean\"],\n",
        "                \"Corpus BLEU\": results[\"aggregate\"][\"corpus_bleu\"],\n",
        "                \"Avg Length Ratio\": results[\"aggregate\"][\"length_ratio_mean\"]\n",
        "            }\n",
        "            comparison_results.append(row)\n",
        "\n",
        "        return pd.DataFrame(comparison_results)\n",
        "\n",
        "# Demonstrate the pipeline\n",
        "pipeline = BLEUEvaluationPipeline(experiment_name=\"bleu-production-pipeline\")\n",
        "\n",
        "# Prepare models for comparison\n",
        "models_to_compare = {\n",
        "    \"Transformer-Large\": (\n",
        "        df_translations[\"model_good\"].tolist(),\n",
        "        {\"architecture\": \"transformer\", \"params\": \"175M\", \"beam_size\": 5}\n",
        "    ),\n",
        "    \"LSTM-Attention\": (\n",
        "        df_translations[\"model_medium\"].tolist(),\n",
        "        {\"architecture\": \"lstm_attention\", \"params\": \"50M\", \"beam_size\": 3}\n",
        "    ),\n",
        "    \"Statistical-MT\": (\n",
        "        df_translations[\"model_poor\"].tolist(),\n",
        "        {\"architecture\": \"phrase_based\", \"params\": \"10M\", \"beam_size\": 1}\n",
        "    )\n",
        "}\n",
        "\n",
        "# Run comparison\n",
        "comparison_results = pipeline.compare_models(\n",
        "    models_to_compare,\n",
        "    df_translations[\"reference\"].tolist()\n",
        ")\n",
        "\n",
        "print(\"\\nðŸš€ Production Pipeline Results:\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_results.round(4).to_string(index=False))\n",
        "\n",
        "# Identify best model\n",
        "best_model = comparison_results.loc[comparison_results[\"BLEU-4\"].idxmax()]\n",
        "print(f\"\\nðŸ† Best Model: {best_model['Model']}\")\n",
        "print(f\"   - BLEU-4: {best_model['BLEU-4']:.4f}\")\n",
        "print(f\"   - Corpus BLEU: {best_model['Corpus BLEU']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl2HHl36icsF"
      },
      "source": [
        "## 8. Statistical Significance Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHUVenjIicsF"
      },
      "outputs": [],
      "source": [
        "def bootstrap_bleu_significance(predictions_a, predictions_b, references,\n",
        "                                n_bootstrap=1000, confidence=0.95):\n",
        "    \"\"\"\n",
        "    Perform bootstrap resampling to test statistical significance of BLEU differences.\n",
        "    \"\"\"\n",
        "    n_samples = len(predictions_a)\n",
        "\n",
        "    # Calculate original BLEU scores\n",
        "    score_a = sacrebleu.corpus_bleu(predictions_a, [references]).score / 100.0\n",
        "    score_b = sacrebleu.corpus_bleu(predictions_b, [references]).score / 100.0\n",
        "    original_diff = score_a - score_b\n",
        "\n",
        "    # Bootstrap resampling\n",
        "    bootstrap_diffs = []\n",
        "\n",
        "    for _ in range(n_bootstrap):\n",
        "        # Sample with replacement\n",
        "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
        "\n",
        "        boot_preds_a = [predictions_a[i] for i in indices]\n",
        "        boot_preds_b = [predictions_b[i] for i in indices]\n",
        "        boot_refs = [references[i] for i in indices]\n",
        "\n",
        "        boot_score_a = sacrebleu.corpus_bleu(boot_preds_a, [boot_refs]).score / 100.0\n",
        "        boot_score_b = sacrebleu.corpus_bleu(boot_preds_b, [boot_refs]).score / 100.0\n",
        "\n",
        "        bootstrap_diffs.append(boot_score_a - boot_score_b)\n",
        "\n",
        "    # Calculate confidence interval\n",
        "    alpha = 1 - confidence\n",
        "    lower_percentile = (alpha/2) * 100\n",
        "    upper_percentile = (1 - alpha/2) * 100\n",
        "\n",
        "    ci_lower = np.percentile(bootstrap_diffs, lower_percentile)\n",
        "    ci_upper = np.percentile(bootstrap_diffs, upper_percentile)\n",
        "\n",
        "    # Check if significant (CI doesn't include 0)\n",
        "    is_significant = (ci_lower > 0) or (ci_upper < 0)\n",
        "\n",
        "    return {\n",
        "        \"model_a_bleu\": score_a,\n",
        "        \"model_b_bleu\": score_b,\n",
        "        \"difference\": original_diff,\n",
        "        \"ci_lower\": ci_lower,\n",
        "        \"ci_upper\": ci_upper,\n",
        "        \"is_significant\": is_significant,\n",
        "        \"p_value_estimate\": np.mean([d <= 0 for d in bootstrap_diffs]) * 2  # Two-tailed\n",
        "    }\n",
        "\n",
        "# Test significance between models\n",
        "print(\"\\nðŸ“Š Statistical Significance Testing (Bootstrap)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Compare High-Quality vs Medium-Quality\n",
        "sig_test_1 = bootstrap_bleu_significance(\n",
        "    df_translations[\"model_good\"].tolist(),\n",
        "    df_translations[\"model_medium\"].tolist(),\n",
        "    df_translations[\"reference\"].tolist()\n",
        ")\n",
        "\n",
        "print(\"\\nHigh-Quality NMT vs Medium-Quality NMT:\")\n",
        "print(f\"  High-Quality BLEU: {sig_test_1['model_a_bleu']:.4f}\")\n",
        "print(f\"  Medium-Quality BLEU: {sig_test_1['model_b_bleu']:.4f}\")\n",
        "print(f\"  Difference: {sig_test_1['difference']:.4f}\")\n",
        "print(f\"  95% CI: [{sig_test_1['ci_lower']:.4f}, {sig_test_1['ci_upper']:.4f}]\")\n",
        "print(f\"  Significant: {'Yes âœ“' if sig_test_1['is_significant'] else 'No âœ—'}\")\n",
        "print(f\"  P-value (estimate): {sig_test_1['p_value_estimate']:.4f}\")\n",
        "\n",
        "# Compare Medium-Quality vs Low-Quality\n",
        "sig_test_2 = bootstrap_bleu_significance(\n",
        "    df_translations[\"model_medium\"].tolist(),\n",
        "    df_translations[\"model_poor\"].tolist(),\n",
        "    df_translations[\"reference\"].tolist()\n",
        ")\n",
        "\n",
        "print(\"\\nMedium-Quality NMT vs Low-Quality Baseline:\")\n",
        "print(f\"  Medium-Quality BLEU: {sig_test_2['model_a_bleu']:.4f}\")\n",
        "print(f\"  Low-Quality BLEU: {sig_test_2['model_b_bleu']:.4f}\")\n",
        "print(f\"  Difference: {sig_test_2['difference']:.4f}\")\n",
        "print(f\"  95% CI: [{sig_test_2['ci_lower']:.4f}, {sig_test_2['ci_upper']:.4f}]\")\n",
        "print(f\"  Significant: {'Yes âœ“' if sig_test_2['is_significant'] else 'No âœ—'}\")\n",
        "print(f\"  P-value (estimate): {sig_test_2['p_value_estimate']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmr_6Et2icsF"
      },
      "source": [
        "## 9. Multi-Reference BLEU Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtO5O_f8icsF"
      },
      "outputs": [],
      "source": [
        "# Create multi-reference data\n",
        "multi_ref_data = [\n",
        "    {\n",
        "        \"source\": \"The weather is beautiful today.\",\n",
        "        \"references\": [\n",
        "            \"Das Wetter ist heute schÃ¶n.\",\n",
        "            \"Heute ist das Wetter wunderschÃ¶n.\",\n",
        "            \"Es ist schÃ¶nes Wetter heute.\"\n",
        "        ],\n",
        "        \"prediction\": \"Das Wetter ist heute schÃ¶n.\"\n",
        "    },\n",
        "    {\n",
        "        \"source\": \"Machine learning revolutionizes data analysis.\",\n",
        "        \"references\": [\n",
        "            \"Maschinelles Lernen revolutioniert die Datenanalyse.\",\n",
        "            \"Machine Learning verÃ¤ndert die Datenanalyse grundlegend.\",\n",
        "            \"Datenanalyse wird durch maschinelles Lernen revolutioniert.\"\n",
        "        ],\n",
        "        \"prediction\": \"Maschinelles Lernen revolutioniert die Datenanalyse.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "def evaluate_multi_reference_bleu(predictions, multi_references):\n",
        "    \"\"\"\n",
        "    Evaluate BLEU with multiple references per source.\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "\n",
        "    for pred, refs in zip(predictions, multi_references):\n",
        "        # SacreBLEU handles multiple references naturally\n",
        "        score = sacrebleu.sentence_bleu(pred, refs)\n",
        "        scores.append(score.score / 100.0)\n",
        "\n",
        "    # Calculate corpus-level score with multiple references\n",
        "    # Transpose references for corpus_bleu format\n",
        "    refs_transposed = list(map(list, zip(*multi_references)))\n",
        "    corpus_score = sacrebleu.corpus_bleu(predictions, refs_transposed)\n",
        "\n",
        "    return {\n",
        "        \"sentence_scores\": scores,\n",
        "        \"average_score\": np.mean(scores),\n",
        "        \"corpus_score\": corpus_score.score / 100.0\n",
        "    }\n",
        "\n",
        "# Evaluate with multi-reference\n",
        "predictions = [item[\"prediction\"] for item in multi_ref_data]\n",
        "multi_refs = [item[\"references\"] for item in multi_ref_data]\n",
        "\n",
        "multi_ref_results = evaluate_multi_reference_bleu(predictions, multi_refs)\n",
        "\n",
        "print(\"\\nðŸŽ¯ Multi-Reference BLEU Evaluation:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Average Sentence BLEU: {multi_ref_results['average_score']:.4f}\")\n",
        "print(f\"Corpus BLEU: {multi_ref_results['corpus_score']:.4f}\")\n",
        "print(\"\\nSentence-level scores:\")\n",
        "for i, score in enumerate(multi_ref_results['sentence_scores']):\n",
        "    print(f\"  Sentence {i+1}: {score:.4f}\")\n",
        "\n",
        "print(\"\\nðŸ’¡ Note: Multi-reference BLEU typically yields higher scores\")\n",
        "print(\"   as it credits translations matching any reference variant.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SZhP64GicsF"
      },
      "source": [
        "## 10. Best Practices and Recommendations\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "1. **BLEU Metric Selection**:\n",
        "   - **BLEU-4**: Standard for machine translation evaluation\n",
        "   - **BLEU-1**: Good for content word coverage\n",
        "   - **Multi-BLEU**: Provides comprehensive n-gram analysis\n",
        "   - **SacreBLEU**: Use for standardized, reproducible scores\n",
        "\n",
        "2. **Evaluation Considerations**:\n",
        "   - Always report corpus-level BLEU for overall system performance\n",
        "   - Use sentence-level BLEU with smoothing for fine-grained analysis\n",
        "   - Consider brevity penalty impact on short translations\n",
        "   - Use multiple references when available\n",
        "\n",
        "3. **Statistical Significance**:\n",
        "   - Always test significance when comparing models\n",
        "   - Use bootstrap resampling for confidence intervals\n",
        "   - Report p-values alongside BLEU scores\n",
        "\n",
        "4. **MLflow Integration Benefits**:\n",
        "   - Automatic experiment tracking and versioning\n",
        "   - Easy A/B testing and model comparison\n",
        "   - Reproducible evaluation pipelines\n",
        "   - Centralized metric storage\n",
        "\n",
        "5. **Production Tips**:\n",
        "   - Cache tokenization for large-scale evaluation\n",
        "   - Use parallel processing for sentence-level scores\n",
        "   - Implement early stopping based on BLEU thresholds\n",
        "   - Monitor BLEU drift in production\n",
        "\n",
        "### BLEU Score Interpretation:\n",
        "- **< 0.1**: Poor quality, barely usable\n",
        "- **0.1-0.2**: Low quality, major improvements needed\n",
        "- **0.2-0.3**: Reasonable quality for some applications\n",
        "- **0.3-0.4**: Good quality for production MT systems\n",
        "- **0.4-0.5**: Very good quality, approaching human performance\n",
        "- **> 0.5**: Excellent quality, often human-level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUEaP3fSicsG"
      },
      "outputs": [],
      "source": [
        "# Final MLflow tracking summary\n",
        "print(\"\\nðŸ“ˆ MLflow Tracking Summary:\")\n",
        "print(\"=\"*60)\n",
        "print(\"To view all experiments and metrics in MLflow UI:\")\n",
        "print(\"\\n1. Run in terminal:\")\n",
        "print(\"   mlflow ui --port 5000\")\n",
        "print(\"\\n2. Open browser:\")\n",
        "print(\"   http://localhost:5000\")\n",
        "print(\"\\n3. Navigate to experiments:\")\n",
        "print(\"   - bleu-metrics-demo\")\n",
        "print(\"   - bleu-production-pipeline\")\n",
        "print(\"\\nâœ… Demo completed successfully!\")\n",
        "print(\"\\nðŸ”— Additional Resources:\")\n",
        "print(\"   - MLflow Docs: https://mlflow.org/docs/latest/\")\n",
        "print(\"   - SacreBLEU: https://github.com/mjpost/sacrebleu\")\n",
        "print(\"   - BLEU Paper: https://aclanthology.org/P02-1040/\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}