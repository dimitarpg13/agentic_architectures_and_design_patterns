{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DuckDuckGo Web Search with LangGraph\n",
    "\n",
    "This notebook demonstrates how to integrate DuckDuckGo search capabilities with LangGraph to create an intelligent agent that can perform live web searches.\n",
    "\n",
    "## Overview\n",
    "- Use DuckDuckGo as a search tool (no API key required!)\n",
    "- Create a LangGraph agent with search capabilities\n",
    "- Handle multi-step reasoning with web search\n",
    "\n",
    "## Prerequisites\n",
    "You'll need an OpenAI API key (or another LLM provider supported by LangChain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q langchain langchain-openai langgraph duckduckgo-search langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set Up API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "# Or load from environment\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize DuckDuckGo Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DuckDuckGo search tool\n",
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "# Test the search tool\n",
    "print(\"Testing DuckDuckGo search...\")\n",
    "result = search.run(\"latest AI news 2024\")\n",
    "print(f\"\\nSearch result preview: {result[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define the Agent State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"The state of our agent.\"\"\"\n",
    "    messages: Annotated[list[BaseMessage], operator.add]\n",
    "    # You can add more state fields as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create the LangGraph Agent with Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "\n",
    "# Bind the search tool to the LLM\n",
    "tools = [search]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# Define the agent node\n",
    "def call_agent(state: AgentState) -> AgentState:\n",
    "    \"\"\"Call the LLM with tools.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Define should continue function\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    \"\"\"Determine if we should continue or end.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # If there are no tool calls, we finish\n",
    "    if not last_message.tool_calls:\n",
    "        return \"end\"\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "# Create the graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"agent\", call_agent)\n",
    "workflow.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# Add conditional edges\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"tools\",\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Add edge from tools back to agent\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"âœ… LangGraph agent with DuckDuckGo search created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize the Graph (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to visualize the graph\n",
    "# try:\n",
    "#     from IPython.display import Image, display\n",
    "#     display(Image(app.get_graph().draw_mermaid_png()))\n",
    "# except Exception as e:\n",
    "#     print(f\"Could not visualize graph: {e}\")\n",
    "#     print(\"Install pygraphviz for visualization: pip install pygraphviz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Example 1: Simple Web Search Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(query: str):\n",
    "    \"\"\"Helper function to run the agent with a query.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    inputs = {\"messages\": [HumanMessage(content=query)]}\n",
    "    \n",
    "    for output in app.stream(inputs):\n",
    "        for key, value in output.items():\n",
    "            print(f\"Node '{key}':\")\n",
    "            if \"messages\" in value:\n",
    "                for msg in value[\"messages\"]:\n",
    "                    if hasattr(msg, 'content') and msg.content:\n",
    "                        print(f\"  {msg.content}\")\n",
    "                    if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "                        print(f\"  Tool calls: {msg.tool_calls}\")\n",
    "    \n",
    "    # Get final response\n",
    "    final_state = app.invoke(inputs)\n",
    "    final_message = final_state[\"messages\"][-1]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Final Answer:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(final_message.content)\n",
    "    return final_message.content\n",
    "\n",
    "# Example 1: Current events\n",
    "run_agent(\"What are the latest developments in artificial intelligence this week?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Example 2: Comparative Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Comparison query\n",
    "run_agent(\"Compare the latest iPhone and Samsung Galaxy flagship phones. What are the key differences?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Example 3: Multi-Step Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Multi-step reasoning\n",
    "run_agent(\"Who won the latest Nobel Prize in Physics and what was their contribution?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Interactive Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive mode - uncomment to use\n",
    "# while True:\n",
    "#     user_input = input(\"\\nYou: \")\n",
    "#     if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "#         print(\"Goodbye!\")\n",
    "#         break\n",
    "#     \n",
    "#     run_agent(user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Advanced: Custom Search Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "\n",
    "# Create search tool with custom parameters\n",
    "custom_search = DuckDuckGoSearchResults(\n",
    "    num_results=5,\n",
    "    output_format=\"list\"  # Can be 'list' or 'snippet'\n",
    ")\n",
    "\n",
    "# Test custom search\n",
    "results = custom_search.run(\"LangChain LangGraph tutorial\")\n",
    "print(\"Custom search results:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Advanced: Adding Memory to the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Create agent with memory\n",
    "memory = MemorySaver()\n",
    "app_with_memory = workflow.compile(checkpointer=memory)\n",
    "\n",
    "# Example with conversation history\n",
    "thread_id = \"conversation_1\"\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "# First query\n",
    "print(\"First query with memory:\")\n",
    "inputs1 = {\"messages\": [HumanMessage(content=\"Search for information about Python 3.12 new features\")]}\n",
    "result1 = app_with_memory.invoke(inputs1, config)\n",
    "print(result1[\"messages\"][-1].content)\n",
    "\n",
    "# Follow-up query (agent remembers previous context)\n",
    "print(\"\\n\\nFollow-up query:\")\n",
    "inputs2 = {\"messages\": [HumanMessage(content=\"Which of those features is most useful for async programming?\")]}\n",
    "result2 = app_with_memory.invoke(inputs2, config)\n",
    "print(result2[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Tips and Best Practices\n",
    "\n",
    "### Search Query Tips:\n",
    "1. **Be specific**: More specific queries yield better results\n",
    "2. **Use recent dates**: Include year/month for time-sensitive info\n",
    "3. **Combine keywords**: Use multiple relevant keywords\n",
    "\n",
    "### Agent Configuration:\n",
    "1. **Temperature**: Use 0 for factual queries, higher for creative tasks\n",
    "2. **Model selection**: GPT-4 for complex reasoning, GPT-3.5 for speed\n",
    "3. **Tool usage**: Monitor tool calls to optimize performance\n",
    "\n",
    "### DuckDuckGo Limitations:\n",
    "- No API key required (great for prototyping!)\n",
    "- Rate limiting may apply with heavy usage\n",
    "- Results may be less comprehensive than paid search APIs\n",
    "- For production, consider alternatives like Brave Search, SerpAPI, or Tavily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Troubleshooting\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "1. **Import errors**: Make sure all packages are installed\n",
    "```bash\n",
    "pip install langchain langchain-openai langgraph duckduckgo-search langchain-community\n",
    "```\n",
    "\n",
    "2. **API key errors**: Verify your OpenAI API key is set correctly\n",
    "\n",
    "3. **Rate limiting**: If you get rate limit errors, add delays between requests\n",
    "\n",
    "4. **Search timeouts**: DuckDuckGo may timeout on slow connections; adjust timeout settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Add more tools**: Combine with calculators, code execution, etc.\n",
    "2. **Implement streaming**: Stream responses for better UX\n",
    "3. **Add error handling**: Implement retry logic and fallbacks\n",
    "4. **Deploy**: Package as a web app with Streamlit or FastAPI\n",
    "5. **Experiment**: Try different LLMs (Anthropic Claude, local models, etc.)\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [LangChain Documentation](https://python.langchain.com/)\n",
    "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
    "- [DuckDuckGo Search](https://github.com/deedy5/duckduckgo_search)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
