{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic Workflow for Confluence Search using Google AI Development Kit\n",
    "\n",
    "This notebook implements a production-ready agentic workflow for semantic search through Confluence pages using Google's AI ecosystem:\n",
    "- **Google Gemini API** for embeddings and LLM capabilities\n",
    "- **Vertex AI Vector Search** for scalable similarity search\n",
    "- **Document AI** for advanced document processing\n",
    "- **AlloyDB** for vector storage with pgvector\n",
    "- **LangGraph + Gemini** for agentic orchestration\n",
    "- **Google Cloud Logging** for observability\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "Confluence Root URL → Crawler → Document AI Processor → Gemini Embeddings\n",
    "                                        ↓                       ↓\n",
    "                              Structured Extraction      Vector Search (Vertex AI)\n",
    "                                        ↓                       ↓\n",
    "User Query → Gemini Agent → Tool Selection → Semantic Search → Response\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q google-generativeai \\\n",
    "              google-cloud-aiplatform \\\n",
    "              google-cloud-documentai \\\n",
    "              google-cloud-logging \\\n",
    "              google-cloud-alloydb-connector \\\n",
    "              atlassian-python-api \\\n",
    "              langchain-google-genai \\\n",
    "              langchain-google-vertexai \\\n",
    "              langgraph \\\n",
    "              chromadb \\\n",
    "              pgvector \\\n",
    "              asyncpg \\\n",
    "              beautifulsoup4 \\\n",
    "              lxml \\\n",
    "              python-dotenv \\\n",
    "              numpy \\\n",
    "              pandas \\\n",
    "              tqdm \\\n",
    "              tenacity \\\n",
    "              pytz \\\n",
    "              aiohttp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import asyncio\n",
    "from typing import List, Dict, Any, Optional, Tuple, TypedDict, Annotated, Sequence\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# External imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "from bs4 import BeautifulSoup\n",
    "from atlassian import Confluence\n",
    "from dotenv import load_dotenv\n",
    "import aiohttp\n",
    "\n",
    "# Google imports\n",
    "import google.generativeai as genai\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import documentai\n",
    "from google.cloud import logging as cloud_logging\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# LangChain + Google imports\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain_google_vertexai import VertexAI, VertexAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.tools import Tool, StructuredTool\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage, ToolMessage\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "@dataclass\n",
    "class GoogleAIConfig:\n",
    "    \"\"\"Configuration for Google AI-based Confluence search\"\"\"\n",
    "    \n",
    "    # Google Cloud settings\n",
    "    project_id: str = os.getenv('GOOGLE_CLOUD_PROJECT', '')\n",
    "    location: str = os.getenv('GOOGLE_CLOUD_LOCATION', 'us-central1')\n",
    "    credentials_path: str = os.getenv('GOOGLE_APPLICATION_CREDENTIALS', '')\n",
    "    \n",
    "    # Gemini API settings\n",
    "    gemini_api_key: str = os.getenv('GEMINI_API_KEY', '')\n",
    "    gemini_model: str = 'gemini-1.5-pro'\n",
    "    embedding_model: str = 'text-embedding-004'  # Latest Gemini embedding model\n",
    "    \n",
    "    # Confluence settings\n",
    "    confluence_url: str = os.getenv('CONFLUENCE_URL', 'https://your-domain.atlassian.net')\n",
    "    confluence_username: str = os.getenv('CONFLUENCE_USERNAME', '')\n",
    "    confluence_api_token: str = os.getenv('CONFLUENCE_API_TOKEN', '')\n",
    "    \n",
    "    # Vector Search settings\n",
    "    vector_search_index: str = 'confluence-search-index'\n",
    "    use_vertex_ai_search: bool = True  # Use Vertex AI Vector Search\n",
    "    use_alloydb: bool = False  # Alternative: Use AlloyDB with pgvector\n",
    "    \n",
    "    # AlloyDB settings (if used)\n",
    "    alloydb_instance: str = os.getenv('ALLOYDB_INSTANCE', '')\n",
    "    alloydb_database: str = os.getenv('ALLOYDB_DATABASE', 'confluence_search')\n",
    "    alloydb_user: str = os.getenv('ALLOYDB_USER', 'postgres')\n",
    "    alloydb_password: str = os.getenv('ALLOYDB_PASSWORD', '')\n",
    "    \n",
    "    # Document AI settings\n",
    "    use_document_ai: bool = True\n",
    "    document_processor_id: str = os.getenv('DOCUMENT_PROCESSOR_ID', '')\n",
    "    \n",
    "    # Crawling settings\n",
    "    max_depth: int = 3\n",
    "    max_pages: int = 100\n",
    "    batch_size: int = 10\n",
    "    max_workers: int = 5\n",
    "    \n",
    "    # Text processing\n",
    "    chunk_size: int = 1000\n",
    "    chunk_overlap: int = 200\n",
    "    \n",
    "    # Search settings\n",
    "    top_k: int = 5\n",
    "    score_threshold: float = 0.7\n",
    "    \n",
    "    # Agent settings\n",
    "    agent_temperature: float = 0.1\n",
    "    max_iterations: int = 10\n",
    "    \n",
    "    # File Search (Gemini's RAG feature)\n",
    "    use_gemini_file_search: bool = True\n",
    "    file_search_corpus: str = 'confluence-corpus'\n",
    "\n",
    "config = GoogleAIConfig()\n",
    "\n",
    "# Initialize Google Cloud\n",
    "if config.project_id:\n",
    "    aiplatform.init(project=config.project_id, location=config.location)\n",
    "    \n",
    "# Initialize Gemini\n",
    "if config.gemini_api_key:\n",
    "    genai.configure(api_key=config.gemini_api_key)\n",
    "else:\n",
    "    logger.warning(\"Gemini API key not configured. Please set GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Google Cloud Logging Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleCloudObservability:\n",
    "    \"\"\"Observability using Google Cloud Logging and Monitoring\"\"\"\n",
    "    \n",
    "    def __init__(self, config: GoogleAIConfig):\n",
    "        self.config = config\n",
    "        self.client = None\n",
    "        \n",
    "        if config.project_id:\n",
    "            try:\n",
    "                self.client = cloud_logging.Client(project=config.project_id)\n",
    "                self.client.setup_logging()\n",
    "                self.logger = self.client.logger('confluence-search')\n",
    "                logger.info(\"Google Cloud Logging initialized\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to initialize Cloud Logging: {e}\")\n",
    "    \n",
    "    def log_search(self, query: str, results: List[Dict], latency: float, metadata: Dict = None):\n",
    "        \"\"\"Log search interaction to Cloud Logging\"\"\"\n",
    "        if not self.client:\n",
    "            return\n",
    "        \n",
    "        log_entry = {\n",
    "            'type': 'search',\n",
    "            'query': query,\n",
    "            'results_count': len(results),\n",
    "            'latency_ms': latency * 1000,\n",
    "            'timestamp': datetime.utcnow().isoformat(),\n",
    "            **(metadata or {})\n",
    "        }\n",
    "        \n",
    "        self.logger.log_struct(log_entry, severity='INFO')\n",
    "    \n",
    "    def log_crawl(self, pages_count: int, duration: float, errors: int = 0):\n",
    "        \"\"\"Log crawling metrics\"\"\"\n",
    "        if not self.client:\n",
    "            return\n",
    "        \n",
    "        log_entry = {\n",
    "            'type': 'crawl',\n",
    "            'pages_crawled': pages_count,\n",
    "            'duration_seconds': duration,\n",
    "            'errors': errors,\n",
    "            'pages_per_second': pages_count / duration if duration > 0 else 0,\n",
    "            'timestamp': datetime.utcnow().isoformat()\n",
    "        }\n",
    "        \n",
    "        self.logger.log_struct(log_entry, severity='INFO')\n",
    "    \n",
    "    def log_error(self, error_type: str, error_message: str, context: Dict = None):\n",
    "        \"\"\"Log errors\"\"\"\n",
    "        if not self.client:\n",
    "            return\n",
    "        \n",
    "        log_entry = {\n",
    "            'type': 'error',\n",
    "            'error_type': error_type,\n",
    "            'error_message': error_message,\n",
    "            'context': context or {},\n",
    "            'timestamp': datetime.utcnow().isoformat()\n",
    "        }\n",
    "        \n",
    "        self.logger.log_struct(log_entry, severity='ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Confluence Crawler with Async Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ConfluencePage:\n",
    "    \"\"\"Represents a Confluence page with metadata\"\"\"\n",
    "    id: str\n",
    "    title: str\n",
    "    url: str\n",
    "    content: str\n",
    "    space_key: str\n",
    "    created_by: str\n",
    "    created_date: str\n",
    "    last_modified: str\n",
    "    parent_id: Optional[str] = None\n",
    "    labels: List[str] = field(default_factory=list)\n",
    "    attachments: List[Dict] = field(default_factory=list)\n",
    "    child_pages: List[str] = field(default_factory=list)\n",
    "    html_content: Optional[str] = None  # Keep HTML for Document AI\n",
    "\n",
    "class EnhancedConfluenceCrawler:\n",
    "    \"\"\"Enhanced Confluence crawler with async support\"\"\"\n",
    "    \n",
    "    def __init__(self, config: GoogleAIConfig, observability: GoogleCloudObservability):\n",
    "        self.config = config\n",
    "        self.observability = observability\n",
    "        self.confluence = Confluence(\n",
    "            url=config.confluence_url,\n",
    "            username=config.confluence_username,\n",
    "            password=config.confluence_api_token,\n",
    "            cloud=True\n",
    "        )\n",
    "        self.visited_pages = set()\n",
    "        self.pages = []\n",
    "    \n",
    "    def extract_page_id_from_url(self, url: str) -> Optional[str]:\n",
    "        \"\"\"Extract page ID from Confluence URL\"\"\"\n",
    "        patterns = [\n",
    "            r'/pages/(\\d+)/',\n",
    "            r'pageId=(\\d+)',\n",
    "            r'/pages/viewpage.action\\?pageId=(\\d+)'\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, url)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "        return None\n",
    "    \n",
    "    def clean_html_content(self, html_content: str) -> str:\n",
    "        \"\"\"Clean HTML content and extract text\"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'lxml')\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for element in soup(['script', 'style', 'meta', 'link']):\n",
    "            element.decompose()\n",
    "        \n",
    "        # Extract text with better formatting\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        \n",
    "        # Clean up whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "    async def fetch_page_async(self, session: aiohttp.ClientSession, page_id: str) -> Optional[ConfluencePage]:\n",
    "        \"\"\"Fetch page asynchronously\"\"\"\n",
    "        try:\n",
    "            # Get page details\n",
    "            page = self.confluence.get_page_by_id(\n",
    "                page_id,\n",
    "                expand='body.storage,metadata.labels,children.page,history,version,ancestors'\n",
    "            )\n",
    "            \n",
    "            # Extract content\n",
    "            content_html = page.get('body', {}).get('storage', {}).get('value', '')\n",
    "            content_text = self.clean_html_content(content_html)\n",
    "            \n",
    "            # Extract metadata\n",
    "            labels = [label['name'] for label in page.get('metadata', {}).get('labels', {}).get('results', [])]\n",
    "            \n",
    "            # Get child pages\n",
    "            child_pages = []\n",
    "            if 'children' in page and 'page' in page['children']:\n",
    "                child_pages = [child['id'] for child in page['children']['page'].get('results', [])]\n",
    "            \n",
    "            # Create page object\n",
    "            confluence_page = ConfluencePage(\n",
    "                id=page['id'],\n",
    "                title=page['title'],\n",
    "                url=f\"{self.config.confluence_url}/wiki/spaces/{page['space']['key']}/pages/{page['id']}\",\n",
    "                content=content_text,\n",
    "                html_content=content_html,  # Keep for Document AI\n",
    "                space_key=page['space']['key'],\n",
    "                created_by=page['history']['createdBy']['displayName'],\n",
    "                created_date=page['history']['createdDate'],\n",
    "                last_modified=page['version']['when'],\n",
    "                labels=labels,\n",
    "                child_pages=child_pages\n",
    "            )\n",
    "            \n",
    "            return confluence_page\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching page {page_id}: {str(e)}\")\n",
    "            self.observability.log_error('page_fetch_error', str(e), {'page_id': page_id})\n",
    "            return None\n",
    "    \n",
    "    async def crawl_from_url_async(self, root_url: str, max_depth: Optional[int] = None) -> List[ConfluencePage]:\n",
    "        \"\"\"Crawl Confluence pages asynchronously\"\"\"\n",
    "        max_depth = max_depth or self.config.max_depth\n",
    "        \n",
    "        # Extract root page ID\n",
    "        root_page_id = self.extract_page_id_from_url(root_url)\n",
    "        if not root_page_id:\n",
    "            raise ValueError(f\"Could not extract page ID from URL: {root_url}\")\n",
    "        \n",
    "        logger.info(f\"Starting async crawl from page ID: {root_page_id}\")\n",
    "        \n",
    "        # BFS crawling with async\n",
    "        queue = [(root_page_id, 0)]\n",
    "        self.visited_pages = set()\n",
    "        self.pages = []\n",
    "        \n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            with tqdm(total=self.config.max_pages, desc=\"Crawling pages\") as pbar:\n",
    "                while queue and len(self.pages) < self.config.max_pages:\n",
    "                    # Process batch of pages\n",
    "                    batch = []\n",
    "                    for _ in range(min(self.config.batch_size, len(queue))):\n",
    "                        if queue:\n",
    "                            page_id, depth = queue.pop(0)\n",
    "                            if page_id not in self.visited_pages and depth <= max_depth:\n",
    "                                batch.append((page_id, depth))\n",
    "                                self.visited_pages.add(page_id)\n",
    "                    \n",
    "                    # Fetch pages in parallel\n",
    "                    tasks = [self.fetch_page_async(session, page_id) for page_id, _ in batch]\n",
    "                    results = await asyncio.gather(*tasks)\n",
    "                    \n",
    "                    # Process results\n",
    "                    for (page_id, depth), page in zip(batch, results):\n",
    "                        if page:\n",
    "                            self.pages.append(page)\n",
    "                            pbar.update(1)\n",
    "                            \n",
    "                            # Add child pages to queue\n",
    "                            for child_id in page.child_pages:\n",
    "                                if child_id not in self.visited_pages:\n",
    "                                    queue.append((child_id, depth + 1))\n",
    "        \n",
    "        logger.info(f\"Crawled {len(self.pages)} pages\")\n",
    "        return self.pages\n",
    "    \n",
    "    def crawl_from_url(self, root_url: str, max_depth: Optional[int] = None) -> List[ConfluencePage]:\n",
    "        \"\"\"Synchronous wrapper for async crawl\"\"\"\n",
    "        return asyncio.run(self.crawl_from_url_async(root_url, max_depth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Document AI Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentAIProcessor:\n",
    "    \"\"\"Process documents using Google Document AI for better extraction\"\"\"\n",
    "    \n",
    "    def __init__(self, config: GoogleAIConfig):\n",
    "        self.config = config\n",
    "        self.client = None\n",
    "        \n",
    "        if config.use_document_ai and config.project_id:\n",
    "            try:\n",
    "                self.client = documentai.DocumentProcessorServiceClient()\n",
    "                self.processor_name = f\"projects/{config.project_id}/locations/{config.location}/processors/{config.document_processor_id}\"\n",
    "                logger.info(\"Document AI initialized\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to initialize Document AI: {e}\")\n",
    "                self.client = None\n",
    "    \n",
    "    def process_html(self, html_content: str, page_title: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process HTML content with Document AI\"\"\"\n",
    "        if not self.client:\n",
    "            return {'text': self.basic_html_extraction(html_content)}\n",
    "        \n",
    "        try:\n",
    "            # Convert HTML to bytes\n",
    "            raw_document = documentai.RawDocument(\n",
    "                content=html_content.encode('utf-8'),\n",
    "                mime_type='text/html'\n",
    "            )\n",
    "            \n",
    "            # Process document\n",
    "            request = documentai.ProcessRequest(\n",
    "                name=self.processor_name,\n",
    "                raw_document=raw_document\n",
    "            )\n",
    "            \n",
    "            result = self.client.process_document(request=request)\n",
    "            document = result.document\n",
    "            \n",
    "            # Extract structured information\n",
    "            extracted_data = {\n",
    "                'text': document.text,\n",
    "                'entities': [],\n",
    "                'tables': [],\n",
    "                'sections': []\n",
    "            }\n",
    "            \n",
    "            # Extract entities\n",
    "            for entity in document.entities:\n",
    "                extracted_data['entities'].append({\n",
    "                    'type': entity.type_,\n",
    "                    'mention': entity.mention_text,\n",
    "                    'confidence': entity.confidence\n",
    "                })\n",
    "            \n",
    "            # Extract tables if present\n",
    "            for page in document.pages:\n",
    "                for table in page.tables:\n",
    "                    table_data = self._extract_table(table)\n",
    "                    if table_data:\n",
    "                        extracted_data['tables'].append(table_data)\n",
    "            \n",
    "            return extracted_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Document AI processing failed: {e}\")\n",
    "            return {'text': self.basic_html_extraction(html_content)}\n",
    "    \n",
    "    def basic_html_extraction(self, html_content: str) -> str:\n",
    "        \"\"\"Basic HTML text extraction as fallback\"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'lxml')\n",
    "        for element in soup(['script', 'style']):\n",
    "            element.decompose()\n",
    "        return soup.get_text(separator=' ', strip=True)\n",
    "    \n",
    "    def _extract_table(self, table) -> Optional[Dict]:\n",
    "        \"\"\"Extract table data from Document AI table object\"\"\"\n",
    "        try:\n",
    "            rows = []\n",
    "            for row in table.body_rows:\n",
    "                row_data = []\n",
    "                for cell in row.cells:\n",
    "                    cell_text = cell.layout.text_anchor.text_segments[0].text if cell.layout.text_anchor.text_segments else ''\n",
    "                    row_data.append(cell_text)\n",
    "                rows.append(row_data)\n",
    "            \n",
    "            return {\n",
    "                'headers': [cell.layout.text_anchor.text_segments[0].text for cell in table.header_rows[0].cells] if table.header_rows else [],\n",
    "                'rows': rows\n",
    "            }\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gemini Embeddings and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeminiDocumentProcessor:\n",
    "    \"\"\"Process documents using Gemini for embeddings and understanding\"\"\"\n",
    "    \n",
    "    def __init__(self, config: GoogleAIConfig, doc_ai_processor: DocumentAIProcessor = None):\n",
    "        self.config = config\n",
    "        self.doc_ai = doc_ai_processor\n",
    "        \n",
    "        # Initialize Gemini embeddings\n",
    "        self.embeddings = GoogleGenerativeAIEmbeddings(\n",
    "            model=f\"models/{config.embedding_model}\",\n",
    "            google_api_key=config.gemini_api_key\n",
    "        )\n",
    "        \n",
    "        # Initialize text splitter\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=config.chunk_size,\n",
    "            chunk_overlap=config.chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "        )\n",
    "        \n",
    "        # Initialize Gemini for content analysis\n",
    "        self.gemini_model = genai.GenerativeModel(config.gemini_model)\n",
    "    \n",
    "    async def process_page_with_gemini(self, page: ConfluencePage) -> Dict[str, Any]:\n",
    "        \"\"\"Process page content with Gemini for enhanced understanding\"\"\"\n",
    "        try:\n",
    "            # First, process with Document AI if available\n",
    "            if self.doc_ai and page.html_content:\n",
    "                doc_ai_result = self.doc_ai.process_html(page.html_content, page.title)\n",
    "                content = doc_ai_result.get('text', page.content)\n",
    "                entities = doc_ai_result.get('entities', [])\n",
    "                tables = doc_ai_result.get('tables', [])\n",
    "            else:\n",
    "                content = page.content\n",
    "                entities = []\n",
    "                tables = []\n",
    "            \n",
    "            # Use Gemini to generate summary and extract key points\n",
    "            prompt = f\"\"\"\n",
    "            Analyze this Confluence page and provide:\n",
    "            1. A concise summary (2-3 sentences)\n",
    "            2. Key topics covered (bullet points)\n",
    "            3. Any technical terms or acronyms with definitions\n",
    "            4. Main actionable items if any\n",
    "            \n",
    "            Page Title: {page.title}\n",
    "            Content: {content[:3000]}  # Limit for token efficiency\n",
    "            \"\"\"\n",
    "            \n",
    "            response = await self.gemini_model.generate_content_async(prompt)\n",
    "            analysis = response.text\n",
    "            \n",
    "            return {\n",
    "                'page': page,\n",
    "                'content': content,\n",
    "                'summary': self._extract_summary(analysis),\n",
    "                'key_topics': self._extract_topics(analysis),\n",
    "                'entities': entities,\n",
    "                'tables': tables,\n",
    "                'gemini_analysis': analysis\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing page with Gemini: {e}\")\n",
    "            return {\n",
    "                'page': page,\n",
    "                'content': page.content,\n",
    "                'summary': '',\n",
    "                'key_topics': [],\n",
    "                'entities': [],\n",
    "                'tables': []\n",
    "            }\n",
    "    \n",
    "    def _extract_summary(self, analysis: str) -> str:\n",
    "        \"\"\"Extract summary from Gemini analysis\"\"\"\n",
    "        # Simple extraction - in production, use regex or structured output\n",
    "        lines = analysis.split('\\n')\n",
    "        for i, line in enumerate(lines):\n",
    "            if 'summary' in line.lower():\n",
    "                return ' '.join(lines[i+1:i+4])\n",
    "        return analysis[:200] if analysis else ''\n",
    "    \n",
    "    def _extract_topics(self, analysis: str) -> List[str]:\n",
    "        \"\"\"Extract topics from Gemini analysis\"\"\"\n",
    "        topics = []\n",
    "        lines = analysis.split('\\n')\n",
    "        in_topics = False\n",
    "        \n",
    "        for line in lines:\n",
    "            if 'topics' in line.lower() or 'key points' in line.lower():\n",
    "                in_topics = True\n",
    "                continue\n",
    "            if in_topics and line.strip().startswith(('-', '*', '•')):\n",
    "                topics.append(line.strip().lstrip('-*• '))\n",
    "            elif in_topics and not line.strip():\n",
    "                break\n",
    "        \n",
    "        return topics\n",
    "    \n",
    "    def create_documents(self, processed_pages: List[Dict]) -> List[Document]:\n",
    "        \"\"\"Create LangChain documents from processed pages\"\"\"\n",
    "        documents = []\n",
    "        \n",
    "        for page_data in tqdm(processed_pages, desc=\"Creating documents\"):\n",
    "            page = page_data['page']\n",
    "            content = page_data['content']\n",
    "            \n",
    "            # Skip minimal content\n",
    "            if len(content) < 50:\n",
    "                continue\n",
    "            \n",
    "            # Split content into chunks\n",
    "            texts = self.text_splitter.split_text(content)\n",
    "            \n",
    "            # Create documents with rich metadata\n",
    "            for i, text in enumerate(texts):\n",
    "                metadata = {\n",
    "                    'page_id': page.id,\n",
    "                    'page_title': page.title,\n",
    "                    'page_url': page.url,\n",
    "                    'space_key': page.space_key,\n",
    "                    'created_by': page.created_by,\n",
    "                    'created_date': page.created_date,\n",
    "                    'last_modified': page.last_modified,\n",
    "                    'labels': ', '.join(page.labels),\n",
    "                    'summary': page_data.get('summary', ''),\n",
    "                    'key_topics': ', '.join(page_data.get('key_topics', [])),\n",
    "                    'chunk_index': i,\n",
    "                    'total_chunks': len(texts)\n",
    "                }\n",
    "                \n",
    "                doc = Document(\n",
    "                    page_content=text,\n",
    "                    metadata=metadata\n",
    "                )\n",
    "                documents.append(doc)\n",
    "        \n",
    "        logger.info(f\"Created {len(documents)} document chunks\")\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Vertex AI Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VertexAIVectorStore:\n",
    "    \"\"\"Manages vector storage using Vertex AI Vector Search\"\"\"\n",
    "    \n",
    "    def __init__(self, config: GoogleAIConfig, embeddings):\n",
    "        self.config = config\n",
    "        self.embeddings = embeddings\n",
    "        self.index = None\n",
    "        self.index_endpoint = None\n",
    "        \n",
    "        if config.use_vertex_ai_search:\n",
    "            self._init_vertex_search()\n",
    "        else:\n",
    "            self._init_local_store()\n",
    "    \n",
    "    def _init_vertex_search(self):\n",
    "        \"\"\"Initialize Vertex AI Vector Search\"\"\"\n",
    "        try:\n",
    "            from google.cloud import aiplatform_v1\n",
    "            \n",
    "            # Initialize the index\n",
    "            index_client = aiplatform_v1.IndexServiceClient(\n",
    "                client_options={\"api_endpoint\": f\"{self.config.location}-aiplatform.googleapis.com\"}\n",
    "            )\n",
    "            \n",
    "            # Create or get existing index\n",
    "            index_name = f\"projects/{self.config.project_id}/locations/{self.config.location}/indexes/{self.config.vector_search_index}\"\n",
    "            \n",
    "            try:\n",
    "                self.index = index_client.get_index(name=index_name)\n",
    "                logger.info(f\"Using existing Vertex AI index: {index_name}\")\n",
    "            except:\n",
    "                # Create new index\n",
    "                logger.info(\"Creating new Vertex AI index...\")\n",
    "                self._create_vertex_index()\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize Vertex AI Vector Search: {e}\")\n",
    "            logger.info(\"Falling back to local vector store\")\n",
    "            self._init_local_store()\n",
    "    \n",
    "    def _create_vertex_index(self):\n",
    "        \"\"\"Create a new Vertex AI Vector Search index\"\"\"\n",
    "        from google.cloud import aiplatform\n",
    "        \n",
    "        # Create index\n",
    "        index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
    "            display_name=\"confluence-search-index\",\n",
    "            dimensions=768,  # Gemini embedding dimension\n",
    "            approximate_neighbors_count=10,\n",
    "            distance_measure_type=\"COSINE_DISTANCE\",\n",
    "            leaf_node_embedding_count=500,\n",
    "            leaf_nodes_to_search_percent=80,\n",
    "            description=\"Confluence semantic search index\",\n",
    "            location=self.config.location,\n",
    "            project=self.config.project_id\n",
    "        )\n",
    "        \n",
    "        self.index = index\n",
    "        \n",
    "        # Create index endpoint\n",
    "        index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "            display_name=\"confluence-search-endpoint\",\n",
    "            description=\"Endpoint for Confluence search\",\n",
    "            location=self.config.location,\n",
    "            project=self.config.project_id\n",
    "        )\n",
    "        \n",
    "        # Deploy index to endpoint\n",
    "        index_endpoint.deploy_index(\n",
    "            index=index,\n",
    "            deployed_index_id=\"confluence_deployed_index\",\n",
    "            machine_type=\"e2-standard-2\",\n",
    "            min_replica_count=1,\n",
    "            max_replica_count=2\n",
    "        )\n",
    "        \n",
    "        self.index_endpoint = index_endpoint\n",
    "    \n",
    "    def _init_local_store(self):\n",
    "        \"\"\"Initialize local ChromaDB as fallback\"\"\"\n",
    "        import chromadb\n",
    "        from langchain.vectorstores import Chroma\n",
    "        \n",
    "        self.vectorstore = Chroma(\n",
    "            collection_name=\"confluence_gemini\",\n",
    "            embedding_function=self.embeddings,\n",
    "            persist_directory=\"./chroma_gemini_db\",\n",
    "            collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    "        )\n",
    "        logger.info(\"Using local ChromaDB for vector storage\")\n",
    "    \n",
    "    async def add_documents_batch(self, documents: List[Document], batch_size: int = 100):\n",
    "        \"\"\"Add documents to vector store in batches\"\"\"\n",
    "        logger.info(f\"Adding {len(documents)} documents to vector store\")\n",
    "        \n",
    "        if hasattr(self, 'vectorstore'):\n",
    "            # Using local store\n",
    "            for i in tqdm(range(0, len(documents), batch_size), desc=\"Adding to vector store\"):\n",
    "                batch = documents[i:i + batch_size]\n",
    "                self.vectorstore.add_documents(batch)\n",
    "            self.vectorstore.persist()\n",
    "        else:\n",
    "            # Using Vertex AI\n",
    "            await self._add_to_vertex_search(documents, batch_size)\n",
    "    \n",
    "    async def _add_to_vertex_search(self, documents: List[Document], batch_size: int):\n",
    "        \"\"\"Add documents to Vertex AI Vector Search\"\"\"\n",
    "        from google.cloud import aiplatform\n",
    "        \n",
    "        # Generate embeddings\n",
    "        texts = [doc.page_content for doc in documents]\n",
    "        embeddings = await self._batch_embed(texts, batch_size)\n",
    "        \n",
    "        # Prepare data for upload\n",
    "        data_points = []\n",
    "        for doc, embedding in zip(documents, embeddings):\n",
    "            data_point = {\n",
    "                \"id\": doc.metadata.get('page_id', str(hash(doc.page_content))),\n",
    "                \"embedding\": embedding,\n",
    "                \"metadata\": doc.metadata\n",
    "            }\n",
    "            data_points.append(data_point)\n",
    "        \n",
    "        # Upload to index\n",
    "        self.index.upsert(data_points)\n",
    "        logger.info(f\"Added {len(data_points)} vectors to Vertex AI index\")\n",
    "    \n",
    "    async def _batch_embed(self, texts: List[str], batch_size: int) -> List[List[float]]:\n",
    "        \"\"\"Generate embeddings in batches\"\"\"\n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            embeddings = await self.embeddings.aembed_documents(batch)\n",
    "            all_embeddings.extend(embeddings)\n",
    "        \n",
    "        return all_embeddings\n",
    "    \n",
    "    def search(self, query: str, k: int = 5, filter_dict: Dict = None) -> List[Document]:\n",
    "        \"\"\"Perform similarity search\"\"\"\n",
    "        if hasattr(self, 'vectorstore'):\n",
    "            # Local search\n",
    "            if filter_dict:\n",
    "                return self.vectorstore.similarity_search(query, k=k, filter=filter_dict)\n",
    "            return self.vectorstore.similarity_search(query, k=k)\n",
    "        else:\n",
    "            # Vertex AI search\n",
    "            return self._vertex_search(query, k, filter_dict)\n",
    "    \n",
    "    def _vertex_search(self, query: str, k: int, filter_dict: Dict = None) -> List[Document]:\n",
    "        \"\"\"Search using Vertex AI Vector Search\"\"\"\n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embeddings.embed_query(query)\n",
    "        \n",
    "        # Perform search\n",
    "        results = self.index_endpoint.find_neighbors(\n",
    "            deployed_index_id=\"confluence_deployed_index\",\n",
    "            queries=[query_embedding],\n",
    "            num_neighbors=k\n",
    "        )\n",
    "        \n",
    "        # Convert results to documents\n",
    "        documents = []\n",
    "        for result in results[0]:\n",
    "            doc = Document(\n",
    "                page_content=result.metadata.get('content', ''),\n",
    "                metadata=result.metadata\n",
    "            )\n",
    "            documents.append(doc)\n",
    "        \n",
    "        return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Gemini File Search Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeminiFileSearch:\n",
    "    \"\"\"Integrate with Gemini's File Search (managed RAG) capability\"\"\"\n",
    "    \n",
    "    def __init__(self, config: GoogleAIConfig):\n",
    "        self.config = config\n",
    "        self.corpus = None\n",
    "        \n",
    "        if config.use_gemini_file_search:\n",
    "            self._init_corpus()\n",
    "    \n",
    "    def _init_corpus(self):\n",
    "        \"\"\"Initialize or get existing corpus for File Search\"\"\"\n",
    "        try:\n",
    "            # Create or get corpus\n",
    "            corpus_name = f\"corpora/{self.config.file_search_corpus}\"\n",
    "            \n",
    "            # Try to get existing corpus\n",
    "            try:\n",
    "                self.corpus = genai.get_corpus(corpus_name)\n",
    "                logger.info(f\"Using existing corpus: {corpus_name}\")\n",
    "            except:\n",
    "                # Create new corpus\n",
    "                self.corpus = genai.create_corpus(\n",
    "                    name=corpus_name,\n",
    "                    display_name=\"Confluence Search Corpus\",\n",
    "                    description=\"Corpus for Confluence page semantic search\"\n",
    "                )\n",
    "                logger.info(f\"Created new corpus: {corpus_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize Gemini File Search: {e}\")\n",
    "    \n",
    "    async def upload_documents(self, pages: List[ConfluencePage]):\n",
    "        \"\"\"Upload Confluence pages to Gemini corpus\"\"\"\n",
    "        if not self.corpus:\n",
    "            logger.error(\"Corpus not initialized\")\n",
    "            return\n",
    "        \n",
    "        documents = []\n",
    "        \n",
    "        for page in tqdm(pages, desc=\"Preparing documents for File Search\"):\n",
    "            # Create document for corpus\n",
    "            doc = genai.Document(\n",
    "                name=f\"documents/{page.id}\",\n",
    "                display_name=page.title,\n",
    "                metadata={\n",
    "                    \"page_id\": page.id,\n",
    "                    \"space_key\": page.space_key,\n",
    "                    \"url\": page.url,\n",
    "                    \"created_by\": page.created_by,\n",
    "                    \"labels\": \",\".join(page.labels)\n",
    "                },\n",
    "                chunks=[\n",
    "                    genai.Chunk(\n",
    "                        data=genai.ChunkData(string_value=page.content),\n",
    "                        custom_metadata={\n",
    "                            \"title\": page.title,\n",
    "                            \"last_modified\": page.last_modified\n",
    "                        }\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "            documents.append(doc)\n",
    "        \n",
    "        # Upload documents to corpus\n",
    "        try:\n",
    "            response = await genai.batch_create_chunk_async(\n",
    "                corpus=self.corpus.name,\n",
    "                documents=documents\n",
    "            )\n",
    "            logger.info(f\"Uploaded {len(documents)} documents to Gemini corpus\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to upload documents: {e}\")\n",
    "    \n",
    "    def search(self, query: str, k: int = 5) -> List[Dict]:\n",
    "        \"\"\"Search using Gemini File Search\"\"\"\n",
    "        if not self.corpus:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # Perform semantic search\n",
    "            response = genai.query_corpus(\n",
    "                corpus=self.corpus.name,\n",
    "                query=query,\n",
    "                results_count=k,\n",
    "                metadata_filters=[]  # Add filters if needed\n",
    "            )\n",
    "            \n",
    "            # Format results\n",
    "            results = []\n",
    "            for chunk in response.relevant_chunks:\n",
    "                results.append({\n",
    "                    'content': chunk.chunk.data.string_value,\n",
    "                    'metadata': chunk.chunk.custom_metadata,\n",
    "                    'score': chunk.chunk_relevance_score\n",
    "                })\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"File Search failed: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Agent Tools Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfluenceSearchTools:\n",
    "    \"\"\"Tools for the Confluence search agent\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VertexAIVectorStore, file_search: GeminiFileSearch, config: GoogleAIConfig):\n",
    "        self.vector_store = vector_store\n",
    "        self.file_search = file_search\n",
    "        self.config = config\n",
    "        self.gemini = genai.GenerativeModel(config.gemini_model)\n",
    "    \n",
    "    def semantic_search(self, query: str, top_k: int = 5) -> str:\n",
    "        \"\"\"Search Confluence pages using semantic similarity\"\"\"\n",
    "        try:\n",
    "            # Try Gemini File Search first\n",
    "            if self.file_search and self.config.use_gemini_file_search:\n",
    "                results = self.file_search.search(query, k=top_k)\n",
    "                if results:\n",
    "                    return self._format_file_search_results(results)\n",
    "            \n",
    "            # Fall back to vector store\n",
    "            docs = self.vector_store.search(query, k=top_k)\n",
    "            return self._format_vector_results(docs)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Semantic search error: {e}\")\n",
    "            return f\"Search failed: {str(e)}\"\n",
    "    \n",
    "    def search_by_space(self, space_key: str, query: str = \"\") -> str:\n",
    "        \"\"\"Search within a specific Confluence space\"\"\"\n",
    "        try:\n",
    "            filter_dict = {'space_key': space_key}\n",
    "            docs = self.vector_store.search(query or \"*\", k=10, filter_dict=filter_dict)\n",
    "            return self._format_vector_results(docs[:5])\n",
    "        except Exception as e:\n",
    "            return f\"Space search failed: {str(e)}\"\n",
    "    \n",
    "    def search_by_labels(self, labels: str, query: str = \"\") -> str:\n",
    "        \"\"\"Search pages by labels\"\"\"\n",
    "        try:\n",
    "            # Parse labels\n",
    "            label_list = [l.strip() for l in labels.split(',')]\n",
    "            filter_dict = {'labels': {'$in': label_list}}\n",
    "            \n",
    "            docs = self.vector_store.search(query or \"*\", k=10, filter_dict=filter_dict)\n",
    "            return self._format_vector_results(docs[:5])\n",
    "        except Exception as e:\n",
    "            return f\"Label search failed: {str(e)}\"\n",
    "    \n",
    "    def analyze_page(self, page_title: str) -> str:\n",
    "        \"\"\"Provide detailed analysis of a specific page using Gemini\"\"\"\n",
    "        try:\n",
    "            # Search for the page\n",
    "            docs = self.vector_store.search(f\"title: {page_title}\", k=3)\n",
    "            \n",
    "            if not docs:\n",
    "                return f\"No page found with title: {page_title}\"\n",
    "            \n",
    "            # Get the most relevant document\n",
    "            doc = docs[0]\n",
    "            \n",
    "            # Use Gemini to analyze\n",
    "            prompt = f\"\"\"\n",
    "            Provide a comprehensive analysis of this Confluence page:\n",
    "            \n",
    "            Title: {doc.metadata.get('page_title', 'Unknown')}\n",
    "            Content: {doc.page_content}\n",
    "            \n",
    "            Include:\n",
    "            1. Main purpose and audience\n",
    "            2. Key information conveyed\n",
    "            3. Action items or decisions documented\n",
    "            4. Related topics that might need attention\n",
    "            5. Quality assessment (completeness, clarity)\n",
    "            \"\"\"\n",
    "            \n",
    "            response = self.gemini.generate_content(prompt)\n",
    "            \n",
    "            return f\"**Page Analysis: {doc.metadata.get('page_title')}**\\n\\n{response.text}\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Analysis failed: {str(e)}\"\n",
    "    \n",
    "    def compare_pages(self, page1_title: str, page2_title: str) -> str:\n",
    "        \"\"\"Compare two Confluence pages\"\"\"\n",
    "        try:\n",
    "            # Get both pages\n",
    "            docs1 = self.vector_store.search(f\"title: {page1_title}\", k=1)\n",
    "            docs2 = self.vector_store.search(f\"title: {page2_title}\", k=1)\n",
    "            \n",
    "            if not docs1 or not docs2:\n",
    "                return \"Could not find both pages for comparison\"\n",
    "            \n",
    "            # Use Gemini to compare\n",
    "            prompt = f\"\"\"\n",
    "            Compare these two Confluence pages:\n",
    "            \n",
    "            Page 1: {docs1[0].metadata.get('page_title')}\n",
    "            Content: {docs1[0].page_content[:1500]}\n",
    "            \n",
    "            Page 2: {docs2[0].metadata.get('page_title')}\n",
    "            Content: {docs2[0].page_content[:1500]}\n",
    "            \n",
    "            Provide:\n",
    "            1. Key similarities\n",
    "            2. Main differences\n",
    "            3. Potential overlaps or redundancies\n",
    "            4. Recommendations for consolidation if applicable\n",
    "            \"\"\"\n",
    "            \n",
    "            response = self.gemini.generate_content(prompt)\n",
    "            return response.text\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Comparison failed: {str(e)}\"\n",
    "    \n",
    "    def _format_file_search_results(self, results: List[Dict]) -> str:\n",
    "        \"\"\"Format Gemini File Search results\"\"\"\n",
    "        formatted = []\n",
    "        for i, result in enumerate(results, 1):\n",
    "            formatted.append(\n",
    "                f\"**Result {i}:**\\n\"\n",
    "                f\"Title: {result['metadata'].get('title', 'Unknown')}\\n\"\n",
    "                f\"Score: {result['score']:.2f}\\n\"\n",
    "                f\"Content: {result['content'][:500]}...\\n\"\n",
    "            )\n",
    "        return \"\\n\\n\".join(formatted) if formatted else \"No results found\"\n",
    "    \n",
    "    def _format_vector_results(self, docs: List[Document]) -> str:\n",
    "        \"\"\"Format vector search results\"\"\"\n",
    "        formatted = []\n",
    "        for i, doc in enumerate(docs, 1):\n",
    "            formatted.append(\n",
    "                f\"**Result {i}:**\\n\"\n",
    "                f\"Title: {doc.metadata.get('page_title', 'Unknown')}\\n\"\n",
    "                f\"URL: {doc.metadata.get('page_url', 'N/A')}\\n\"\n",
    "                f\"Space: {doc.metadata.get('space_key', 'Unknown')}\\n\"\n",
    "                f\"Summary: {doc.metadata.get('summary', 'N/A')}\\n\"\n",
    "                f\"Content: {doc.page_content[:400]}...\\n\"\n",
    "            )\n",
    "        return \"\\n\\n\".join(formatted) if formatted else \"No results found\"\n",
    "    \n",
    "    def get_tools(self) -> List[Tool]:\n",
    "        \"\"\"Get LangChain tools\"\"\"\n",
    "        return [\n",
    "            Tool(\n",
    "                name=\"semantic_search\",\n",
    "                func=self.semantic_search,\n",
    "                description=\"Search Confluence pages using semantic similarity. Input: search query\"\n",
    "            ),\n",
    "            Tool(\n",
    "                name=\"search_by_space\",\n",
    "                func=self.search_by_space,\n",
    "                description=\"Search within a specific Confluence space. Input: 'space_key query'\"\n",
    "            ),\n",
    "            Tool(\n",
    "                name=\"search_by_labels\",\n",
    "                func=self.search_by_labels,\n",
    "                description=\"Search pages by labels. Input: 'label1,label2 optional_query'\"\n",
    "            ),\n",
    "            Tool(\n",
    "                name=\"analyze_page\",\n",
    "                func=self.analyze_page,\n",
    "                description=\"Get detailed AI analysis of a specific page. Input: page title\"\n",
    "            ),\n",
    "            Tool(\n",
    "                name=\"compare_pages\",\n",
    "                func=self.compare_pages,\n",
    "                description=\"Compare two Confluence pages. Input: 'page1_title | page2_title'\"\n",
    "            )\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. LangGraph Agent with Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"State for the LangGraph agent\"\"\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    \n",
    "class GeminiConfluenceAgent:\n",
    "    \"\"\"LangGraph agent powered by Gemini for Confluence search\"\"\"\n",
    "    \n",
    "    def __init__(self, tools: List[Tool], config: GoogleAIConfig):\n",
    "        self.tools = tools\n",
    "        self.config = config\n",
    "        \n",
    "        # Initialize Gemini LLM\n",
    "        self.llm = ChatGoogleGenerativeAI(\n",
    "            model=f\"gemini-1.5-pro\",\n",
    "            temperature=config.agent_temperature,\n",
    "            google_api_key=config.gemini_api_key,\n",
    "            convert_system_message_to_human=True\n",
    "        )\n",
    "        \n",
    "        # Bind tools\n",
    "        self.llm_with_tools = self.llm.bind_tools(tools)\n",
    "        \n",
    "        # Build graph\n",
    "        self.graph = self._build_graph()\n",
    "        \n",
    "        # Memory\n",
    "        self.memory = MemorySaver()\n",
    "        \n",
    "        # Compile\n",
    "        self.app = self.graph.compile(checkpointer=self.memory)\n",
    "    \n",
    "    def _build_graph(self) -> StateGraph:\n",
    "        \"\"\"Build the agent graph\"\"\"\n",
    "        workflow = StateGraph(AgentState)\n",
    "        \n",
    "        def agent(state: AgentState) -> dict:\n",
    "            \"\"\"Agent node\"\"\"\n",
    "            messages = state[\"messages\"]\n",
    "            \n",
    "            # Add system message if first interaction\n",
    "            if len(messages) == 1:\n",
    "                system_prompt = (\n",
    "                    \"You are an expert assistant for searching and analyzing Confluence documentation. \"\n",
    "                    \"You have access to advanced semantic search tools powered by Google's Gemini AI. \"\n",
    "                    \"You can search by content, space, labels, analyze pages, and compare documents. \"\n",
    "                    \"Always provide clear, actionable answers and include relevant page links when available. \"\n",
    "                    \"Use multiple tools when needed to provide comprehensive answers. \"\n",
    "                    \"If unsure, use the search tools to find accurate information.\"\n",
    "                )\n",
    "                messages = [SystemMessage(content=system_prompt)] + messages\n",
    "            \n",
    "            response = self.llm_with_tools.invoke(messages)\n",
    "            return {\"messages\": [response]}\n",
    "        \n",
    "        # Add nodes\n",
    "        workflow.add_node(\"agent\", agent)\n",
    "        workflow.add_node(\"tools\", ToolNode(self.tools))\n",
    "        \n",
    "        # Set entry point\n",
    "        workflow.set_entry_point(\"agent\")\n",
    "        \n",
    "        # Add edges\n",
    "        workflow.add_conditional_edges(\n",
    "            \"agent\",\n",
    "            tools_condition,\n",
    "            {\n",
    "                \"tools\": \"tools\",\n",
    "                \"__end__\": END\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        workflow.add_edge(\"tools\", \"agent\")\n",
    "        \n",
    "        return workflow\n",
    "    \n",
    "    async def search_async(self, query: str, thread_id: str = \"default\") -> str:\n",
    "        \"\"\"Execute search asynchronously\"\"\"\n",
    "        initial_state = {\"messages\": [HumanMessage(content=query)]}\n",
    "        config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "        \n",
    "        try:\n",
    "            result = await self.app.ainvoke(initial_state, config)\n",
    "            \n",
    "            # Extract final response\n",
    "            for message in reversed(result[\"messages\"]):\n",
    "                if isinstance(message, AIMessage) and not message.tool_calls:\n",
    "                    return message.content\n",
    "            \n",
    "            return \"No response generated\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Agent error: {e}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    def search(self, query: str, thread_id: str = \"default\") -> str:\n",
    "        \"\"\"Synchronous search wrapper\"\"\"\n",
    "        return asyncio.run(self.search_async(query, thread_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleAIConfluencePipeline:\n",
    "    \"\"\"Main pipeline for Google AI-powered Confluence search\"\"\"\n",
    "    \n",
    "    def __init__(self, config: GoogleAIConfig = None):\n",
    "        self.config = config or GoogleAIConfig()\n",
    "        self.observability = GoogleCloudObservability(self.config)\n",
    "        self.crawler = None\n",
    "        self.doc_ai = None\n",
    "        self.processor = None\n",
    "        self.vector_store = None\n",
    "        self.file_search = None\n",
    "        self.agent = None\n",
    "    \n",
    "    async def initialize_async(self):\n",
    "        \"\"\"Initialize all components asynchronously\"\"\"\n",
    "        logger.info(\"Initializing Google AI Confluence Pipeline\")\n",
    "        \n",
    "        # Initialize Document AI\n",
    "        self.doc_ai = DocumentAIProcessor(self.config)\n",
    "        \n",
    "        # Initialize crawler\n",
    "        self.crawler = EnhancedConfluenceCrawler(self.config, self.observability)\n",
    "        \n",
    "        # Initialize processor\n",
    "        self.processor = GeminiDocumentProcessor(self.config, self.doc_ai)\n",
    "        \n",
    "        # Initialize vector store\n",
    "        self.vector_store = VertexAIVectorStore(self.config, self.processor.embeddings)\n",
    "        \n",
    "        # Initialize File Search\n",
    "        self.file_search = GeminiFileSearch(self.config)\n",
    "        \n",
    "        # Initialize tools and agent\n",
    "        tools_manager = ConfluenceSearchTools(self.vector_store, self.file_search, self.config)\n",
    "        tools = tools_manager.get_tools()\n",
    "        self.agent = GeminiConfluenceAgent(tools, self.config)\n",
    "        \n",
    "        logger.info(\"Pipeline initialized successfully\")\n",
    "    \n",
    "    def initialize(self):\n",
    "        \"\"\"Synchronous initialization wrapper\"\"\"\n",
    "        asyncio.run(self.initialize_async())\n",
    "    \n",
    "    async def index_confluence_async(self, root_url: str, max_depth: int = None) -> Dict[str, Any]:\n",
    "        \"\"\"Index Confluence pages asynchronously\"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Crawl pages\n",
    "            logger.info(f\"Starting crawl from: {root_url}\")\n",
    "            pages = await self.crawler.crawl_from_url_async(root_url, max_depth)\n",
    "            \n",
    "            if not pages:\n",
    "                return {\"status\": \"error\", \"message\": \"No pages found\"}\n",
    "            \n",
    "            # Process pages with Gemini\n",
    "            logger.info(f\"Processing {len(pages)} pages with Gemini\")\n",
    "            processed_pages = []\n",
    "            for page in tqdm(pages, desc=\"Processing with Gemini\"):\n",
    "                processed = await self.processor.process_page_with_gemini(page)\n",
    "                processed_pages.append(processed)\n",
    "            \n",
    "            # Create documents\n",
    "            documents = self.processor.create_documents(processed_pages)\n",
    "            \n",
    "            # Add to vector store\n",
    "            await self.vector_store.add_documents_batch(documents)\n",
    "            \n",
    "            # Upload to Gemini File Search if enabled\n",
    "            if self.config.use_gemini_file_search:\n",
    "                await self.file_search.upload_documents(pages)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            duration = time.time() - start_time\n",
    "            \n",
    "            # Log metrics\n",
    "            self.observability.log_crawl(len(pages), duration)\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"pages_indexed\": len(pages),\n",
    "                \"documents_created\": len(documents),\n",
    "                \"duration_seconds\": duration,\n",
    "                \"gemini_file_search\": self.config.use_gemini_file_search,\n",
    "                \"vertex_ai_search\": self.config.use_vertex_ai_search\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Indexing error: {e}\")\n",
    "            self.observability.log_error('indexing_error', str(e))\n",
    "            return {\"status\": \"error\", \"message\": str(e)}\n",
    "    \n",
    "    def index_confluence(self, root_url: str, max_depth: int = None) -> Dict[str, Any]:\n",
    "        \"\"\"Synchronous indexing wrapper\"\"\"\n",
    "        return asyncio.run(self.index_confluence_async(root_url, max_depth))\n",
    "    \n",
    "    def search(self, query: str) -> str:\n",
    "        \"\"\"Search using the agent\"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        result = self.agent.search(query)\n",
    "        \n",
    "        # Log search\n",
    "        latency = time.time() - start_time\n",
    "        self.observability.log_search(query, [], latency)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def interactive_search(self):\n",
    "        \"\"\"Interactive search interface\"\"\"\n",
    "        print(\"\\n🚀 Google AI Confluence Search Agent\")\n",
    "        print(\"Powered by Gemini 1.5 Pro\")\n",
    "        print(\"=\"*50)\n",
    "        print(\"Type 'exit' to quit, 'help' for commands\\n\")\n",
    "        \n",
    "        while True:\n",
    "            query = input(\"\\n💬 Your query: \").strip()\n",
    "            \n",
    "            if query.lower() == 'exit':\n",
    "                break\n",
    "            elif query.lower() == 'help':\n",
    "                print(\"\\nAvailable commands:\")\n",
    "                print(\"  - Any search query\")\n",
    "                print(\"  - 'analyze [page_title]' - Analyze a specific page\")\n",
    "                print(\"  - 'compare [page1] | [page2]' - Compare two pages\")\n",
    "                print(\"  - 'space:[key] [query]' - Search within a space\")\n",
    "                print(\"  - 'labels:[label1,label2]' - Search by labels\")\n",
    "                continue\n",
    "            elif not query:\n",
    "                continue\n",
    "            \n",
    "            print(\"\\n🤔 Thinking with Gemini...\")\n",
    "            response = self.search(query)\n",
    "            print(f\"\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Quick Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_start():\n",
    "    \"\"\"\n",
    "    Quick start guide for Google AI Confluence Search.\n",
    "    \n",
    "    Prerequisites:\n",
    "    1. Set up Google Cloud Project with enabled APIs:\n",
    "       - Vertex AI API\n",
    "       - Document AI API\n",
    "       - Cloud Logging API\n",
    "    \n",
    "    2. Set environment variables:\n",
    "       - GOOGLE_CLOUD_PROJECT\n",
    "       - GEMINI_API_KEY\n",
    "       - CONFLUENCE_URL\n",
    "       - CONFLUENCE_USERNAME\n",
    "       - CONFLUENCE_API_TOKEN\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🚀 Google AI Confluence Search - Quick Start\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Check configuration\n",
    "    required_vars = ['GEMINI_API_KEY', 'CONFLUENCE_URL', 'CONFLUENCE_USERNAME', 'CONFLUENCE_API_TOKEN']\n",
    "    missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
    "    \n",
    "    if missing_vars:\n",
    "        print(\"⚠️  Missing environment variables:\")\n",
    "        for var in missing_vars:\n",
    "            print(f\"   - {var}\")\n",
    "        print(\"\\nPlease set these in your .env file.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize pipeline\n",
    "    print(\"\\n📦 Initializing Google AI pipeline...\")\n",
    "    pipeline = GoogleAIConfluencePipeline()\n",
    "    pipeline.initialize()\n",
    "    print(\"✅ Pipeline initialized with Gemini 1.5 Pro\")\n",
    "    \n",
    "    # Index pages\n",
    "    print(\"\\n📚 Let's index some Confluence pages!\")\n",
    "    root_url = input(\"Enter Confluence page URL: \")\n",
    "    \n",
    "    if root_url:\n",
    "        print(\"\\n🔄 Indexing with Gemini (this may take a few minutes)...\")\n",
    "        result = pipeline.index_confluence(root_url, max_depth=2)\n",
    "        \n",
    "        if result['status'] == 'success':\n",
    "            print(f\"\\n✅ Success!\")\n",
    "            print(f\"   Pages indexed: {result['pages_indexed']}\")\n",
    "            print(f\"   Documents created: {result['documents_created']}\")\n",
    "            print(f\"   Time: {result['duration_seconds']:.2f}s\")\n",
    "            print(f\"   Using Gemini File Search: {result.get('gemini_file_search', False)}\")\n",
    "            print(f\"   Using Vertex AI Search: {result.get('vertex_ai_search', False)}\")\n",
    "    \n",
    "    # Demo search\n",
    "    print(\"\\n🔍 Let's try a search!\")\n",
    "    query = input(\"Enter your query: \")\n",
    "    \n",
    "    if query:\n",
    "        print(\"\\n🤖 Gemini is thinking...\\n\")\n",
    "        result = pipeline.search(query)\n",
    "        print(result)\n",
    "    \n",
    "    # Interactive mode\n",
    "    choice = input(\"\\nContinue with interactive search? (y/n): \")\n",
    "    if choice.lower() == 'y':\n",
    "        pipeline.interactive_search()\n",
    "    \n",
    "    print(\"\\n👋 Thank you for using Google AI Confluence Search!\")\n",
    "    return pipeline\n",
    "\n",
    "# Run quick start\n",
    "# pipeline = quick_start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Advanced Usage Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using Gemini's advanced capabilities\n",
    "async def advanced_gemini_example():\n",
    "    \"\"\"Demonstrate advanced Gemini features\"\"\"\n",
    "    \n",
    "    config = GoogleAIConfig()\n",
    "    pipeline = GoogleAIConfluencePipeline(config)\n",
    "    await pipeline.initialize_async()\n",
    "    \n",
    "    # Example queries showcasing Gemini's capabilities\n",
    "    queries = [\n",
    "        \"Find all architecture decision records and summarize the key patterns\",\n",
    "        \"Compare our API documentation with REST best practices\",\n",
    "        \"Identify gaps in our deployment documentation\",\n",
    "        \"What are the most frequently updated pages in the TECH space?\",\n",
    "        \"Generate a knowledge graph of our microservices documentation\"\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        result = await pipeline.agent.search_async(query)\n",
    "        print(result)\n",
    "\n",
    "# Run example\n",
    "# asyncio.run(advanced_gemini_example())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Batch processing with progress tracking\n",
    "async def batch_indexing_example():\n",
    "    \"\"\"Index multiple Confluence spaces in parallel\"\"\"\n",
    "    \n",
    "    spaces = [\n",
    "        \"https://your-domain.atlassian.net/wiki/spaces/TECH/overview\",\n",
    "        \"https://your-domain.atlassian.net/wiki/spaces/DOCS/overview\",\n",
    "        \"https://your-domain.atlassian.net/wiki/spaces/API/overview\"\n",
    "    ]\n",
    "    \n",
    "    pipeline = GoogleAIConfluencePipeline()\n",
    "    await pipeline.initialize_async()\n",
    "    \n",
    "    # Index spaces in parallel\n",
    "    tasks = [pipeline.index_confluence_async(url, max_depth=2) for url in spaces]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    # Summary\n",
    "    total_pages = sum(r.get('pages_indexed', 0) for r in results if r['status'] == 'success')\n",
    "    print(f\"\\nIndexed {total_pages} total pages across {len(spaces)} spaces\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run example\n",
    "# results = asyncio.run(batch_indexing_example())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Best Practices and Tips\n",
    "\n",
    "### Google AI Optimization\n",
    "\n",
    "1. **Gemini Model Selection**\n",
    "   - Use `gemini-1.5-pro` for complex analysis\n",
    "   - Use `gemini-1.5-flash` for faster, simpler queries\n",
    "   - Consider `gemini-1.5-pro-vision` for pages with diagrams\n",
    "\n",
    "2. **File Search vs Vector Search**\n",
    "   - File Search: Managed, no infrastructure, automatic updates\n",
    "   - Vector Search: More control, custom filtering, hybrid search\n",
    "\n",
    "3. **Document AI Integration**\n",
    "   - Use for complex layouts, tables, forms\n",
    "   - Configure processors for specific document types\n",
    "   - Consider OCR processor for image-heavy pages\n",
    "\n",
    "4. **Cost Optimization**\n",
    "   - Batch API calls when possible\n",
    "   - Use caching for frequently accessed pages\n",
    "   - Configure appropriate Vertex AI machine types\n",
    "\n",
    "### Performance Tips\n",
    "\n",
    "1. **Async Processing**: Use async methods for better throughput\n",
    "2. **Batching**: Process documents in batches of 100-500\n",
    "3. **Caching**: Implement Redis caching for embeddings\n",
    "4. **Monitoring**: Use Cloud Monitoring for performance metrics\n",
    "\n",
    "### Security\n",
    "\n",
    "1. **Service Accounts**: Use proper IAM roles\n",
    "2. **VPC-SC**: Configure VPC Service Controls\n",
    "3. **Data Residency**: Choose appropriate regions\n",
    "4. **Encryption**: Enable CMEK for sensitive data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}