{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Workflow with Brave Search API for Grounding\n",
    "\n",
    "This notebook demonstrates a production-ready multi-agent system that uses Brave Search API for grounding. The system includes:\n",
    "\n",
    "- **Research Agent**: Gathers information from multiple searches\n",
    "- **Fact-Checking Agent**: Verifies claims using search results\n",
    "- **Analysis Agent**: Analyzes and synthesizes information\n",
    "- **Quality Assurance Agent**: Ensures response accuracy and completeness\n",
    "- **Orchestrator**: Coordinates agent interactions\n",
    "\n",
    "## Key Features\n",
    "- Real-time web search grounding\n",
    "- Multi-agent collaboration with LangGraph\n",
    "- Fact verification and citation tracking\n",
    "- Iterative refinement based on search results\n",
    "- Comprehensive error handling and retry logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q langgraph langchain langchain-openai langchain-anthropic brave-search python-dotenv pydantic typing-extensions rich pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "from typing import Dict, List, Any, Optional, TypedDict, Annotated, Sequence, Literal\n",
    "from datetime import datetime, timezone\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "import logging\n",
    "from functools import lru_cache\n",
    "\n",
    "# LangChain and LangGraph imports\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# Brave Search\n",
    "from brave_search import BraveSearch\n",
    "\n",
    "# Pydantic for data validation\n",
    "from pydantic import BaseModel, Field, validator\n",
    "\n",
    "# Rich for beautiful output\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from rich.panel import Panel\n",
    "from rich.syntax import Syntax\n",
    "from rich.progress import track\n",
    "\n",
    "console = Console()\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration\n",
    "BRAVE_API_KEY = os.getenv(\"BRAVE_API_KEY\", \"YOUR_BRAVE_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"YOUR_OPENAI_API_KEY\")\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\", \"YOUR_ANTHROPIC_API_KEY\")\n",
    "\n",
    "# Set API keys\n",
    "os.environ[\"BRAVE_API_KEY\"] = BRAVE_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = ANTHROPIC_API_KEY\n",
    "\n",
    "console.print(Panel(\"[green]✓ Environment configured[/green]\", title=\"Setup Status\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Brave Search API Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import Optional, Dict, List, Any\n",
    "import time\n",
    "\n",
    "class BraveSearchClient:\n",
    "    \"\"\"Enhanced Brave Search API client with retry logic and caching\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, max_retries: int = 3, cache_ttl: int = 3600):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://api.search.brave.com/res/v1\"\n",
    "        self.max_retries = max_retries\n",
    "        self.cache_ttl = cache_ttl\n",
    "        self.cache = {}\n",
    "        self.headers = {\n",
    "            \"Accept\": \"application/json\",\n",
    "            \"X-Subscription-Token\": self.api_key\n",
    "        }\n",
    "    \n",
    "    def web_search(self, \n",
    "                   query: str, \n",
    "                   count: int = 10,\n",
    "                   freshness: Optional[str] = None,\n",
    "                   country: str = \"us\",\n",
    "                   search_lang: str = \"en\",\n",
    "                   ui_lang: str = \"en-US\",\n",
    "                   text_decorations: bool = False,\n",
    "                   spellcheck: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Execute web search with Brave Search API\"\"\"\n",
    "        \n",
    "        # Check cache\n",
    "        cache_key = f\"{query}_{count}_{freshness}_{country}\"\n",
    "        if cache_key in self.cache:\n",
    "            cached_result, timestamp = self.cache[cache_key]\n",
    "            if time.time() - timestamp < self.cache_ttl:\n",
    "                logger.info(f\"Cache hit for query: {query}\")\n",
    "                return cached_result\n",
    "        \n",
    "        params = {\n",
    "            \"q\": query,\n",
    "            \"count\": count,\n",
    "            \"country\": country,\n",
    "            \"search_lang\": search_lang,\n",
    "            \"ui_lang\": ui_lang,\n",
    "            \"text_decorations\": text_decorations,\n",
    "            \"spellcheck\": spellcheck\n",
    "        }\n",
    "        \n",
    "        if freshness:\n",
    "            params[\"freshness\"] = freshness\n",
    "        \n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                response = requests.get(\n",
    "                    f\"{self.base_url}/web/search\",\n",
    "                    headers=self.headers,\n",
    "                    params=params,\n",
    "                    timeout=10\n",
    "                )\n",
    "                response.raise_for_status()\n",
    "                result = response.json()\n",
    "                \n",
    "                # Cache the result\n",
    "                self.cache[cache_key] = (result, time.time())\n",
    "                \n",
    "                return result\n",
    "                \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                logger.warning(f\"Search attempt {attempt + 1} failed: {e}\")\n",
    "                if attempt == self.max_retries - 1:\n",
    "                    raise\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "    \n",
    "    def get_snippets(self, search_results: Dict[str, Any]) -> List[Dict[str, str]]:\n",
    "        \"\"\"Extract and format snippets from search results\"\"\"\n",
    "        snippets = []\n",
    "        \n",
    "        if \"web\" in search_results and \"results\" in search_results[\"web\"]:\n",
    "            for result in search_results[\"web\"][\"results\"]:\n",
    "                snippet = {\n",
    "                    \"title\": result.get(\"title\", \"\"),\n",
    "                    \"url\": result.get(\"url\", \"\"),\n",
    "                    \"description\": result.get(\"description\", \"\"),\n",
    "                    \"age\": result.get(\"age\", \"\"),\n",
    "                    \"language\": result.get(\"language\", \"\")\n",
    "                }\n",
    "                \n",
    "                # Add extra snippets if available\n",
    "                if \"extra_snippets\" in result:\n",
    "                    snippet[\"extra_snippets\"] = result[\"extra_snippets\"]\n",
    "                \n",
    "                snippets.append(snippet)\n",
    "        \n",
    "        return snippets\n",
    "    \n",
    "    def get_news(self, search_results: Dict[str, Any]) -> List[Dict[str, str]]:\n",
    "        \"\"\"Extract news results if available\"\"\"\n",
    "        news = []\n",
    "        \n",
    "        if \"news\" in search_results and \"results\" in search_results[\"news\"]:\n",
    "            for result in search_results[\"news\"][\"results\"]:\n",
    "                news.append({\n",
    "                    \"title\": result.get(\"title\", \"\"),\n",
    "                    \"url\": result.get(\"url\", \"\"),\n",
    "                    \"description\": result.get(\"description\", \"\"),\n",
    "                    \"age\": result.get(\"age\", \"\"),\n",
    "                    \"source\": result.get(\"meta_url\", {}).get(\"hostname\", \"\")\n",
    "                })\n",
    "        \n",
    "        return news\n",
    "\n",
    "# Initialize Brave Search client\n",
    "brave_client = BraveSearchClient(BRAVE_API_KEY)\n",
    "console.print(\"[green]✓ Brave Search client initialized[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Models and State Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Models\n",
    "class SearchResult(BaseModel):\n",
    "    \"\"\"Model for search results\"\"\"\n",
    "    query: str\n",
    "    title: str\n",
    "    url: str\n",
    "    description: str\n",
    "    snippet: Optional[str] = None\n",
    "    relevance_score: float = Field(default=0.0, ge=0.0, le=1.0)\n",
    "    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))\n",
    "    source_type: Literal[\"web\", \"news\", \"video\"] = \"web\"\n",
    "    \n",
    "class Fact(BaseModel):\n",
    "    \"\"\"Model for extracted facts\"\"\"\n",
    "    statement: str\n",
    "    source_url: str\n",
    "    confidence: float = Field(ge=0.0, le=1.0)\n",
    "    supporting_evidence: List[str] = Field(default_factory=list)\n",
    "    contradicting_evidence: List[str] = Field(default_factory=list)\n",
    "    verification_status: Literal[\"verified\", \"disputed\", \"unverified\"] = \"unverified\"\n",
    "    \n",
    "class AgentResponse(BaseModel):\n",
    "    \"\"\"Model for agent responses\"\"\"\n",
    "    agent_name: str\n",
    "    content: str\n",
    "    search_queries: List[str] = Field(default_factory=list)\n",
    "    sources: List[str] = Field(default_factory=list)\n",
    "    facts: List[Fact] = Field(default_factory=list)\n",
    "    confidence: float = Field(ge=0.0, le=1.0)\n",
    "    reasoning: Optional[str] = None\n",
    "\n",
    "# State for multi-agent workflow\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"State management for multi-agent workflow\"\"\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    query: str\n",
    "    search_results: List[SearchResult]\n",
    "    facts: List[Fact]\n",
    "    agent_responses: Dict[str, AgentResponse]\n",
    "    current_agent: str\n",
    "    iteration: int\n",
    "    max_iterations: int\n",
    "    final_response: Optional[str]\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "console.print(\"[green]✓ Data models defined[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Agent Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Agent Class\n",
    "class BaseAgent:\n",
    "    \"\"\"Base class for all agents\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, llm, brave_client: BraveSearchClient):\n",
    "        self.name = name\n",
    "        self.llm = llm\n",
    "        self.brave_client = brave_client\n",
    "        self.logger = logging.getLogger(f\"{__name__}.{name}\")\n",
    "    \n",
    "    async def process(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Process state - to be implemented by subclasses\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def search(self, query: str, count: int = 5, freshness: Optional[str] = None) -> List[SearchResult]:\n",
    "        \"\"\"Execute search and return formatted results\"\"\"\n",
    "        try:\n",
    "            results = self.brave_client.web_search(query, count=count, freshness=freshness)\n",
    "            snippets = self.brave_client.get_snippets(results)\n",
    "            \n",
    "            search_results = []\n",
    "            for snippet in snippets:\n",
    "                search_results.append(SearchResult(\n",
    "                    query=query,\n",
    "                    title=snippet[\"title\"],\n",
    "                    url=snippet[\"url\"],\n",
    "                    description=snippet[\"description\"],\n",
    "                    snippet=\" \".join(snippet.get(\"extra_snippets\", [])),\n",
    "                    relevance_score=0.8  # Default score\n",
    "                ))\n",
    "            \n",
    "            return search_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Search failed: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Research Agent\n",
    "class ResearchAgent(BaseAgent):\n",
    "    \"\"\"Agent responsible for gathering information through searches\"\"\"\n",
    "    \n",
    "    async def process(self, state: AgentState) -> AgentState:\n",
    "        self.logger.info(f\"Research Agent processing query: {state['query']}\")\n",
    "        \n",
    "        # Generate search queries\n",
    "        search_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessage(content=\"\"\"You are a research agent that generates comprehensive search queries.\n",
    "            Given a user query, generate 3-5 diverse search queries to gather comprehensive information.\n",
    "            Consider different perspectives, related topics, and fact-checking needs.\n",
    "            Return JSON with 'queries' list.\"\"\"),\n",
    "            HumanMessage(content=f\"Generate search queries for: {state['query']}\")\n",
    "        ])\n",
    "        \n",
    "        response = self.llm.invoke(search_prompt.format_messages())\n",
    "        queries_data = json.loads(response.content)\n",
    "        \n",
    "        # Execute searches\n",
    "        all_results = []\n",
    "        for query in queries_data[\"queries\"]:\n",
    "            results = self.search(query, count=5)\n",
    "            all_results.extend(results)\n",
    "            console.print(f\"[blue]Searched: {query} - Found {len(results)} results[/blue]\")\n",
    "        \n",
    "        # Update state\n",
    "        state[\"search_results\"].extend(all_results)\n",
    "        \n",
    "        # Create agent response\n",
    "        agent_response = AgentResponse(\n",
    "            agent_name=self.name,\n",
    "            content=f\"Gathered {len(all_results)} search results from {len(queries_data['queries'])} queries\",\n",
    "            search_queries=queries_data[\"queries\"],\n",
    "            sources=[r.url for r in all_results],\n",
    "            confidence=0.9\n",
    "        )\n",
    "        \n",
    "        state[\"agent_responses\"][self.name] = agent_response\n",
    "        state[\"messages\"].append(AIMessage(content=agent_response.content, name=self.name))\n",
    "        \n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fact-Checking Agent\n",
    "class FactCheckingAgent(BaseAgent):\n",
    "    \"\"\"Agent responsible for verifying claims and facts\"\"\"\n",
    "    \n",
    "    async def process(self, state: AgentState) -> AgentState:\n",
    "        self.logger.info(\"Fact-Checking Agent processing\")\n",
    "        \n",
    "        # Extract claims to verify\n",
    "        extract_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessage(content=\"\"\"You are a fact-checking agent.\n",
    "            Extract key factual claims from the search results that need verification.\n",
    "            For each claim, assess its verifiability and importance.\n",
    "            Return JSON with 'claims' list, each having 'statement' and 'importance'.\"\"\"),\n",
    "            HumanMessage(content=f\"\"\"Query: {state['query']}\n",
    "            \n",
    "Search results:\n",
    "{json.dumps([r.dict() for r in state['search_results'][:10]], indent=2, default=str)}\"\"\")\n",
    "        ])\n",
    "        \n",
    "        response = self.llm.invoke(extract_prompt.format_messages())\n",
    "        claims_data = json.loads(response.content)\n",
    "        \n",
    "        # Verify each claim\n",
    "        verified_facts = []\n",
    "        for claim in claims_data[\"claims\"][:5]:  # Limit to top 5 claims\n",
    "            # Search for verification\n",
    "            verification_query = f\"fact check {claim['statement']}\"\n",
    "            verification_results = self.search(verification_query, count=3)\n",
    "            \n",
    "            # Analyze verification results\n",
    "            verify_prompt = ChatPromptTemplate.from_messages([\n",
    "                SystemMessage(content=\"\"\"Analyze the search results to verify the claim.\n",
    "                Determine if the claim is verified, disputed, or unverified.\n",
    "                Return JSON with 'status', 'confidence', 'supporting_evidence', 'contradicting_evidence'.\"\"\"),\n",
    "                HumanMessage(content=f\"\"\"Claim: {claim['statement']}\n",
    "                \n",
    "Verification results:\n",
    "{json.dumps([r.dict() for r in verification_results], indent=2, default=str)}\"\"\")\n",
    "            ])\n",
    "            \n",
    "            verify_response = self.llm.invoke(verify_prompt.format_messages())\n",
    "            verification_data = json.loads(verify_response.content)\n",
    "            \n",
    "            fact = Fact(\n",
    "                statement=claim[\"statement\"],\n",
    "                source_url=verification_results[0].url if verification_results else \"\",\n",
    "                confidence=verification_data[\"confidence\"],\n",
    "                supporting_evidence=verification_data[\"supporting_evidence\"],\n",
    "                contradicting_evidence=verification_data[\"contradicting_evidence\"],\n",
    "                verification_status=verification_data[\"status\"]\n",
    "            )\n",
    "            \n",
    "            verified_facts.append(fact)\n",
    "            console.print(f\"[yellow]Verified: {claim['statement'][:50]}... - Status: {fact.verification_status}[/yellow]\")\n",
    "        \n",
    "        # Update state\n",
    "        state[\"facts\"].extend(verified_facts)\n",
    "        \n",
    "        # Create agent response\n",
    "        agent_response = AgentResponse(\n",
    "            agent_name=self.name,\n",
    "            content=f\"Verified {len(verified_facts)} facts\",\n",
    "            facts=verified_facts,\n",
    "            confidence=0.85\n",
    "        )\n",
    "        \n",
    "        state[\"agent_responses\"][self.name] = agent_response\n",
    "        state[\"messages\"].append(AIMessage(content=agent_response.content, name=self.name))\n",
    "        \n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis Agent\n",
    "class AnalysisAgent(BaseAgent):\n",
    "    \"\"\"Agent responsible for analyzing and synthesizing information\"\"\"\n",
    "    \n",
    "    async def process(self, state: AgentState) -> AgentState:\n",
    "        self.logger.info(\"Analysis Agent processing\")\n",
    "        \n",
    "        # Synthesize information\n",
    "        synthesis_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessage(content=\"\"\"You are an analysis agent that synthesizes information.\n",
    "            Analyze the search results and verified facts to provide comprehensive insights.\n",
    "            Identify patterns, contradictions, and key findings.\n",
    "            Provide a structured analysis with main points, supporting evidence, and caveats.\n",
    "            Return JSON with 'analysis', 'key_findings', 'contradictions', 'confidence'.\"\"\"),\n",
    "            HumanMessage(content=f\"\"\"Query: {state['query']}\n",
    "\n",
    "Search Results Summary:\n",
    "{json.dumps([{'title': r.title, 'description': r.description} for r in state['search_results'][:10]], indent=2)}\n",
    "\n",
    "Verified Facts:\n",
    "{json.dumps([f.dict() for f in state['facts']], indent=2, default=str)}\"\"\")\n",
    "        ])\n",
    "        \n",
    "        response = self.llm.invoke(synthesis_prompt.format_messages())\n",
    "        analysis_data = json.loads(response.content)\n",
    "        \n",
    "        # Additional deep-dive if needed\n",
    "        if analysis_data[\"confidence\"] < 0.7:\n",
    "            # Perform additional searches for unclear areas\n",
    "            console.print(\"[orange]Low confidence - performing additional research[/orange]\")\n",
    "            \n",
    "            for finding in analysis_data[\"key_findings\"][:2]:\n",
    "                additional_results = self.search(f\"explain {finding}\", count=3)\n",
    "                state[\"search_results\"].extend(additional_results)\n",
    "        \n",
    "        # Create comprehensive response\n",
    "        agent_response = AgentResponse(\n",
    "            agent_name=self.name,\n",
    "            content=analysis_data[\"analysis\"],\n",
    "            sources=[r.url for r in state[\"search_results\"][:5]],\n",
    "            confidence=analysis_data[\"confidence\"],\n",
    "            reasoning=f\"Key findings: {', '.join(analysis_data['key_findings'])}\"\n",
    "        )\n",
    "        \n",
    "        state[\"agent_responses\"][self.name] = agent_response\n",
    "        state[\"messages\"].append(AIMessage(content=agent_response.content, name=self.name))\n",
    "        \n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality Assurance Agent\n",
    "class QualityAssuranceAgent(BaseAgent):\n",
    "    \"\"\"Agent responsible for ensuring quality and completeness\"\"\"\n",
    "    \n",
    "    async def process(self, state: AgentState) -> AgentState:\n",
    "        self.logger.info(\"QA Agent processing\")\n",
    "        \n",
    "        # Evaluate completeness and quality\n",
    "        qa_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessage(content=\"\"\"You are a quality assurance agent.\n",
    "            Evaluate the completeness and accuracy of the gathered information.\n",
    "            Check for:\n",
    "            1. Answer completeness - does it fully address the query?\n",
    "            2. Source reliability - are sources credible?\n",
    "            3. Fact accuracy - are facts properly verified?\n",
    "            4. Coherence - is the information consistent?\n",
    "            5. Missing information - what gaps exist?\n",
    "            \n",
    "            Return JSON with 'quality_score', 'completeness_score', 'issues', 'missing_info', 'recommendations'.\"\"\"),\n",
    "            HumanMessage(content=f\"\"\"Query: {state['query']}\n",
    "\n",
    "Agent Responses:\n",
    "{json.dumps({name: resp.dict() for name, resp in state['agent_responses'].items()}, indent=2, default=str)}\"\"\")\n",
    "        ])\n",
    "        \n",
    "        response = self.llm.invoke(qa_prompt.format_messages())\n",
    "        qa_data = json.loads(response.content)\n",
    "        \n",
    "        # If quality is low, trigger additional research\n",
    "        if qa_data[\"quality_score\"] < 0.7 or qa_data[\"completeness_score\"] < 0.7:\n",
    "            console.print(\"[red]Quality check failed - requesting additional research[/red]\")\n",
    "            \n",
    "            for missing in qa_data[\"missing_info\"][:2]:\n",
    "                additional_results = self.search(missing, count=3, freshness=\"day\")\n",
    "                state[\"search_results\"].extend(additional_results)\n",
    "            \n",
    "            state[\"iteration\"] += 1\n",
    "        \n",
    "        # Generate final assessment\n",
    "        final_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessage(content=\"\"\"Generate a comprehensive, well-structured response that:\n",
    "            1. Directly answers the user's query\n",
    "            2. Includes verified facts with citations\n",
    "            3. Acknowledges any uncertainties or contradictions\n",
    "            4. Provides actionable insights where relevant\n",
    "            \n",
    "            Format the response in clear sections with proper citations.\"\"\"),\n",
    "            HumanMessage(content=f\"\"\"Query: {state['query']}\n",
    "\n",
    "All gathered information:\n",
    "{json.dumps({name: resp.dict() for name, resp in state['agent_responses'].items()}, indent=2, default=str)}\n",
    "\n",
    "Quality Assessment:\n",
    "{json.dumps(qa_data, indent=2)}\"\"\")\n",
    "        ])\n",
    "        \n",
    "        final_response = self.llm.invoke(final_prompt.format_messages())\n",
    "        \n",
    "        # Update state with final response\n",
    "        state[\"final_response\"] = final_response.content\n",
    "        state[\"metadata\"][\"qa_assessment\"] = qa_data\n",
    "        \n",
    "        agent_response = AgentResponse(\n",
    "            agent_name=self.name,\n",
    "            content=f\"Quality Score: {qa_data['quality_score']}, Completeness: {qa_data['completeness_score']}\",\n",
    "            confidence=(qa_data[\"quality_score\"] + qa_data[\"completeness_score\"]) / 2\n",
    "        )\n",
    "        \n",
    "        state[\"agent_responses\"][self.name] = agent_response\n",
    "        state[\"messages\"].append(AIMessage(content=final_response.content, name=self.name))\n",
    "        \n",
    "        return state\n",
    "\n",
    "console.print(\"[green]✓ All agents defined[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multi-Agent Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAgentOrchestrator:\n",
    "    \"\"\"Orchestrates the multi-agent workflow\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_model: str = \"gpt-4\"):\n",
    "        # Initialize LLMs\n",
    "        self.primary_llm = ChatOpenAI(model=llm_model, temperature=0.3)\n",
    "        self.fast_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "        \n",
    "        # Initialize Brave client\n",
    "        self.brave_client = brave_client\n",
    "        \n",
    "        # Initialize agents\n",
    "        self.agents = {\n",
    "            \"research\": ResearchAgent(\"ResearchAgent\", self.primary_llm, self.brave_client),\n",
    "            \"fact_check\": FactCheckingAgent(\"FactCheckAgent\", self.fast_llm, self.brave_client),\n",
    "            \"analysis\": AnalysisAgent(\"AnalysisAgent\", self.primary_llm, self.brave_client),\n",
    "            \"qa\": QualityAssuranceAgent(\"QAAgent\", self.primary_llm, self.brave_client)\n",
    "        }\n",
    "        \n",
    "        # Build the graph\n",
    "        self.graph = self._build_graph()\n",
    "    \n",
    "    def _build_graph(self) -> StateGraph:\n",
    "        \"\"\"Build the LangGraph workflow\"\"\"\n",
    "        \n",
    "        # Create the graph\n",
    "        workflow = StateGraph(AgentState)\n",
    "        \n",
    "        # Add nodes\n",
    "        workflow.add_node(\"research\", self.research_node)\n",
    "        workflow.add_node(\"fact_check\", self.fact_check_node)\n",
    "        workflow.add_node(\"analysis\", self.analysis_node)\n",
    "        workflow.add_node(\"qa\", self.qa_node)\n",
    "        workflow.add_node(\"router\", self.router_node)\n",
    "        \n",
    "        # Set entry point\n",
    "        workflow.set_entry_point(\"research\")\n",
    "        \n",
    "        # Add edges\n",
    "        workflow.add_edge(\"research\", \"fact_check\")\n",
    "        workflow.add_edge(\"fact_check\", \"analysis\")\n",
    "        workflow.add_edge(\"analysis\", \"qa\")\n",
    "        workflow.add_edge(\"qa\", \"router\")\n",
    "        \n",
    "        # Add conditional edges from router\n",
    "        workflow.add_conditional_edges(\n",
    "            \"router\",\n",
    "            self.should_continue,\n",
    "            {\n",
    "                \"research\": \"research\",\n",
    "                \"end\": END\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return workflow.compile()\n",
    "    \n",
    "    async def research_node(self, state: AgentState) -> AgentState:\n",
    "        return await self.agents[\"research\"].process(state)\n",
    "    \n",
    "    async def fact_check_node(self, state: AgentState) -> AgentState:\n",
    "        return await self.agents[\"fact_check\"].process(state)\n",
    "    \n",
    "    async def analysis_node(self, state: AgentState) -> AgentState:\n",
    "        return await self.agents[\"analysis\"].process(state)\n",
    "    \n",
    "    async def qa_node(self, state: AgentState) -> AgentState:\n",
    "        return await self.agents[\"qa\"].process(state)\n",
    "    \n",
    "    def router_node(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Router node to decide next steps\"\"\"\n",
    "        state[\"current_agent\"] = \"router\"\n",
    "        return state\n",
    "    \n",
    "    def should_continue(self, state: AgentState) -> str:\n",
    "        \"\"\"Decide whether to continue or end the workflow\"\"\"\n",
    "        \n",
    "        # Check if we have a final response\n",
    "        if state.get(\"final_response\") and state[\"iteration\"] >= 1:\n",
    "            return \"end\"\n",
    "        \n",
    "        # Check if we've reached max iterations\n",
    "        if state[\"iteration\"] >= state[\"max_iterations\"]:\n",
    "            return \"end\"\n",
    "        \n",
    "        # Check quality scores from QA\n",
    "        qa_assessment = state.get(\"metadata\", {}).get(\"qa_assessment\", {})\n",
    "        if qa_assessment:\n",
    "            quality = qa_assessment.get(\"quality_score\", 0)\n",
    "            completeness = qa_assessment.get(\"completeness_score\", 0)\n",
    "            \n",
    "            if quality >= 0.8 and completeness >= 0.8:\n",
    "                return \"end\"\n",
    "        \n",
    "        # Continue with more research\n",
    "        return \"research\"\n",
    "    \n",
    "    async def run(self, query: str, max_iterations: int = 2) -> Dict[str, Any]:\n",
    "        \"\"\"Run the multi-agent workflow\"\"\"\n",
    "        \n",
    "        # Initialize state\n",
    "        initial_state = {\n",
    "            \"messages\": [HumanMessage(content=query)],\n",
    "            \"query\": query,\n",
    "            \"search_results\": [],\n",
    "            \"facts\": [],\n",
    "            \"agent_responses\": {},\n",
    "            \"current_agent\": \"orchestrator\",\n",
    "            \"iteration\": 0,\n",
    "            \"max_iterations\": max_iterations,\n",
    "            \"final_response\": None,\n",
    "            \"metadata\": {}\n",
    "        }\n",
    "        \n",
    "        console.print(Panel(f\"[bold green]Starting multi-agent workflow for:[/bold green]\\n{query}\", title=\"Query\"))\n",
    "        \n",
    "        # Run the workflow\n",
    "        final_state = await self.graph.ainvoke(initial_state)\n",
    "        \n",
    "        return final_state\n",
    "\n",
    "# Initialize orchestrator\n",
    "orchestrator = MultiAgentOrchestrator(llm_model=\"gpt-4\")\n",
    "console.print(\"[green]✓ Orchestrator initialized[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization and Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(state: Dict[str, Any]):\n",
    "    \"\"\"Visualize the workflow results\"\"\"\n",
    "    \n",
    "    # Display agent contributions\n",
    "    console.print(\"\\n\" + \"=\"*80)\n",
    "    console.print(Panel(\"[bold]Multi-Agent Workflow Results[/bold]\", style=\"cyan\"))\n",
    "    \n",
    "    # Agent responses table\n",
    "    table = Table(title=\"Agent Contributions\")\n",
    "    table.add_column(\"Agent\", style=\"cyan\")\n",
    "    table.add_column(\"Confidence\", style=\"yellow\")\n",
    "    table.add_column(\"Sources Used\", style=\"green\")\n",
    "    table.add_column(\"Facts Verified\", style=\"magenta\")\n",
    "    \n",
    "    for agent_name, response in state[\"agent_responses\"].items():\n",
    "        table.add_row(\n",
    "            agent_name,\n",
    "            f\"{response.confidence:.2f}\",\n",
    "            str(len(response.sources)),\n",
    "            str(len(response.facts))\n",
    "        )\n",
    "    \n",
    "    console.print(table)\n",
    "    \n",
    "    # Display verified facts\n",
    "    if state[\"facts\"]:\n",
    "        console.print(\"\\n[bold]Verified Facts:[/bold]\")\n",
    "        for i, fact in enumerate(state[\"facts\"], 1):\n",
    "            status_color = {\n",
    "                \"verified\": \"green\",\n",
    "                \"disputed\": \"red\",\n",
    "                \"unverified\": \"yellow\"\n",
    "            }.get(fact.verification_status, \"white\")\n",
    "            \n",
    "            console.print(f\"  {i}. [{status_color}]{fact.statement}[/{status_color}]\")\n",
    "            console.print(f\"     Confidence: {fact.confidence:.2f} | Source: {fact.source_url[:50]}...\")\n",
    "    \n",
    "    # Display final response\n",
    "    if state.get(\"final_response\"):\n",
    "        console.print(\"\\n\" + \"=\"*80)\n",
    "        console.print(Panel(state[\"final_response\"], title=\"[bold green]Final Response[/bold green]\", border_style=\"green\"))\n",
    "    \n",
    "    # Display QA assessment\n",
    "    if \"qa_assessment\" in state.get(\"metadata\", {}):\n",
    "        qa = state[\"metadata\"][\"qa_assessment\"]\n",
    "        console.print(\"\\n[bold]Quality Assessment:[/bold]\")\n",
    "        console.print(f\"  Quality Score: {qa.get('quality_score', 0):.2f}\")\n",
    "        console.print(f\"  Completeness: {qa.get('completeness_score', 0):.2f}\")\n",
    "        \n",
    "        if qa.get(\"issues\"):\n",
    "            console.print(\"  Issues found:\")\n",
    "            for issue in qa[\"issues\"]:\n",
    "                console.print(f\"    - {issue}\")\n",
    "\n",
    "def export_results(state: Dict[str, Any], filename: str = \"results.json\"):\n",
    "    \"\"\"Export results to JSON file\"\"\"\n",
    "    \n",
    "    export_data = {\n",
    "        \"query\": state[\"query\"],\n",
    "        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "        \"iterations\": state[\"iteration\"],\n",
    "        \"agent_responses\": {name: resp.dict() for name, resp in state[\"agent_responses\"].items()},\n",
    "        \"facts\": [fact.dict() for fact in state[\"facts\"]],\n",
    "        \"search_results_count\": len(state[\"search_results\"]),\n",
    "        \"final_response\": state.get(\"final_response\"),\n",
    "        \"metadata\": state[\"metadata\"]\n",
    "    }\n",
    "    \n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(export_data, f, indent=2, default=str)\n",
    "    \n",
    "    console.print(f\"\\n[green]Results exported to {filename}[/green]\")\n",
    "\n",
    "console.print(\"[green]✓ Visualization functions ready[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Technology Research Query\n",
    "async def example_tech_research():\n",
    "    query = \"What are the latest developments in quantum computing and how do they compare to classical computing for machine learning tasks?\"\n",
    "    \n",
    "    result = await orchestrator.run(query, max_iterations=2)\n",
    "    visualize_results(result)\n",
    "    export_results(result, \"quantum_computing_research.json\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run the example\n",
    "result = await example_tech_research()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Current Events Analysis\n",
    "async def example_current_events():\n",
    "    query = \"What are the major AI safety initiatives launched in 2024 and their potential impact on AI development?\"\n",
    "    \n",
    "    result = await orchestrator.run(query, max_iterations=2)\n",
    "    visualize_results(result)\n",
    "    export_results(result, \"ai_safety_2024.json\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run the example\n",
    "result = await example_current_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Comparative Analysis\n",
    "async def example_comparative_analysis():\n",
    "    query = \"Compare the approaches of OpenAI, Anthropic, and Google DeepMind to AI alignment and safety research\"\n",
    "    \n",
    "    result = await orchestrator.run(query, max_iterations=3)\n",
    "    visualize_results(result)\n",
    "    export_results(result, \"ai_companies_comparison.json\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run the example\n",
    "result = await example_comparative_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream Processing for Real-time Updates\n",
    "class StreamingOrchestrator(MultiAgentOrchestrator):\n",
    "    \"\"\"Extended orchestrator with streaming capabilities\"\"\"\n",
    "    \n",
    "    async def run_with_streaming(self, query: str, max_iterations: int = 2):\n",
    "        \"\"\"Run workflow with real-time streaming updates\"\"\"\n",
    "        \n",
    "        initial_state = {\n",
    "            \"messages\": [HumanMessage(content=query)],\n",
    "            \"query\": query,\n",
    "            \"search_results\": [],\n",
    "            \"facts\": [],\n",
    "            \"agent_responses\": {},\n",
    "            \"current_agent\": \"orchestrator\",\n",
    "            \"iteration\": 0,\n",
    "            \"max_iterations\": max_iterations,\n",
    "            \"final_response\": None,\n",
    "            \"metadata\": {}\n",
    "        }\n",
    "        \n",
    "        console.print(Panel(f\"[bold green]Streaming workflow for:[/bold green]\\n{query}\", title=\"Query\"))\n",
    "        \n",
    "        # Stream events\n",
    "        async for event in self.graph.astream_events(initial_state, version=\"v1\"):\n",
    "            if event[\"event\"] == \"on_chain_start\":\n",
    "                console.print(f\"[dim]Starting: {event['name']}[/dim]\")\n",
    "            elif event[\"event\"] == \"on_chain_end\":\n",
    "                console.print(f\"[dim]Completed: {event['name']}[/dim]\")\n",
    "            elif event[\"event\"] == \"on_chat_model_stream\":\n",
    "                # Stream token-by-token for supported models\n",
    "                if event.get(\"data\", {}).get(\"chunk\"):\n",
    "                    print(event[\"data\"][\"chunk\"].content, end=\"\", flush=True)\n",
    "        \n",
    "        # Get final state\n",
    "        final_state = await self.graph.ainvoke(initial_state)\n",
    "        return final_state\n",
    "\n",
    "# Caching layer for improved performance\n",
    "class CachedBraveSearch(BraveSearchClient):\n",
    "    \"\"\"Brave Search with persistent caching\"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.persistent_cache = {}\n",
    "        self.load_cache()\n",
    "    \n",
    "    def load_cache(self, filename=\"search_cache.json\"):\n",
    "        \"\"\"Load cache from file\"\"\"\n",
    "        try:\n",
    "            with open(filename, \"r\") as f:\n",
    "                self.persistent_cache = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            self.persistent_cache = {}\n",
    "    \n",
    "    def save_cache(self, filename=\"search_cache.json\"):\n",
    "        \"\"\"Save cache to file\"\"\"\n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(self.persistent_cache, f, indent=2)\n",
    "\n",
    "console.print(\"[green]✓ Advanced features loaded[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Performance Metrics and Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "class PerformanceMonitor:\n",
    "    \"\"\"Monitor and track agent performance\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = defaultdict(list)\n",
    "        self.timings = defaultdict(list)\n",
    "    \n",
    "    def track_agent_performance(self, agent_name: str, start_time: float, end_time: float, \n",
    "                               success: bool, confidence: float):\n",
    "        \"\"\"Track individual agent performance\"\"\"\n",
    "        duration = end_time - start_time\n",
    "        \n",
    "        self.timings[agent_name].append(duration)\n",
    "        self.metrics[agent_name].append({\n",
    "            \"duration\": duration,\n",
    "            \"success\": success,\n",
    "            \"confidence\": confidence,\n",
    "            \"timestamp\": datetime.now(timezone.utc).isoformat()\n",
    "        })\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        \"\"\"Calculate performance statistics\"\"\"\n",
    "        stats = {}\n",
    "        \n",
    "        for agent_name, metrics_list in self.metrics.items():\n",
    "            durations = [m[\"duration\"] for m in metrics_list]\n",
    "            confidences = [m[\"confidence\"] for m in metrics_list]\n",
    "            success_rate = sum(m[\"success\"] for m in metrics_list) / len(metrics_list)\n",
    "            \n",
    "            stats[agent_name] = {\n",
    "                \"avg_duration\": sum(durations) / len(durations),\n",
    "                \"min_duration\": min(durations),\n",
    "                \"max_duration\": max(durations),\n",
    "                \"avg_confidence\": sum(confidences) / len(confidences),\n",
    "                \"success_rate\": success_rate,\n",
    "                \"total_runs\": len(metrics_list)\n",
    "            }\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def display_dashboard(self):\n",
    "        \"\"\"Display performance dashboard\"\"\"\n",
    "        stats = self.get_statistics()\n",
    "        \n",
    "        table = Table(title=\"Agent Performance Dashboard\")\n",
    "        table.add_column(\"Agent\", style=\"cyan\")\n",
    "        table.add_column(\"Avg Duration (s)\", style=\"yellow\")\n",
    "        table.add_column(\"Avg Confidence\", style=\"green\")\n",
    "        table.add_column(\"Success Rate\", style=\"magenta\")\n",
    "        table.add_column(\"Total Runs\", style=\"blue\")\n",
    "        \n",
    "        for agent_name, agent_stats in stats.items():\n",
    "            table.add_row(\n",
    "                agent_name,\n",
    "                f\"{agent_stats['avg_duration']:.2f}\",\n",
    "                f\"{agent_stats['avg_confidence']:.2f}\",\n",
    "                f\"{agent_stats['success_rate']:.0%}\",\n",
    "                str(agent_stats['total_runs'])\n",
    "            )\n",
    "        \n",
    "        console.print(table)\n",
    "\n",
    "# Initialize performance monitor\n",
    "monitor = PerformanceMonitor()\n",
    "console.print(\"[green]✓ Performance monitoring initialized[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Error Handling and Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustOrchestrator(MultiAgentOrchestrator):\n",
    "    \"\"\"Orchestrator with enhanced error handling and recovery\"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.fallback_strategies = {\n",
    "            \"search_failure\": self.handle_search_failure,\n",
    "            \"llm_failure\": self.handle_llm_failure,\n",
    "            \"parsing_failure\": self.handle_parsing_failure\n",
    "        }\n",
    "    \n",
    "    async def handle_search_failure(self, state: AgentState, error: Exception) -> AgentState:\n",
    "        \"\"\"Handle search API failures\"\"\"\n",
    "        console.print(f\"[red]Search failure: {error}[/red]\")\n",
    "        \n",
    "        # Try alternative search strategies\n",
    "        state[\"metadata\"][\"search_errors\"] = state[\"metadata\"].get(\"search_errors\", []) + [str(error)]\n",
    "        \n",
    "        # Use cached results if available\n",
    "        if hasattr(self.brave_client, 'cache') and self.brave_client.cache:\n",
    "            console.print(\"[yellow]Using cached search results[/yellow]\")\n",
    "            # Return state with cached results\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    async def handle_llm_failure(self, state: AgentState, error: Exception) -> AgentState:\n",
    "        \"\"\"Handle LLM API failures\"\"\"\n",
    "        console.print(f\"[red]LLM failure: {error}[/red]\")\n",
    "        \n",
    "        # Try with fallback model\n",
    "        state[\"metadata\"][\"llm_errors\"] = state[\"metadata\"].get(\"llm_errors\", []) + [str(error)]\n",
    "        \n",
    "        # Use simpler prompts or cached responses\n",
    "        return state\n",
    "    \n",
    "    async def handle_parsing_failure(self, state: AgentState, error: Exception) -> AgentState:\n",
    "        \"\"\"Handle JSON parsing failures\"\"\"\n",
    "        console.print(f\"[red]Parsing failure: {error}[/red]\")\n",
    "        \n",
    "        # Try alternative parsing strategies\n",
    "        state[\"metadata\"][\"parsing_errors\"] = state[\"metadata\"].get(\"parsing_errors\", []) + [str(error)]\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    async def run_with_recovery(self, query: str, max_iterations: int = 2) -> Dict[str, Any]:\n",
    "        \"\"\"Run workflow with automatic error recovery\"\"\"\n",
    "        \n",
    "        try:\n",
    "            return await self.run(query, max_iterations)\n",
    "        except Exception as e:\n",
    "            console.print(f\"[bold red]Workflow failed: {e}[/bold red]\")\n",
    "            \n",
    "            # Attempt recovery\n",
    "            for strategy_name, strategy_func in self.fallback_strategies.items():\n",
    "                try:\n",
    "                    console.print(f\"[yellow]Attempting recovery: {strategy_name}[/yellow]\")\n",
    "                    # Recovery logic here\n",
    "                    return await self.run(query, max_iterations=1)  # Simplified retry\n",
    "                except Exception as recovery_error:\n",
    "                    console.print(f\"[red]Recovery failed: {recovery_error}[/red]\")\n",
    "            \n",
    "            raise\n",
    "\n",
    "console.print(\"[green]✓ Error handling configured[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Interactive Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def interactive_demo():\n",
    "    \"\"\"Interactive demonstration of the multi-agent system\"\"\"\n",
    "    \n",
    "    console.print(Panel(\n",
    "        \"[bold cyan]Multi-Agent Brave Search Workflow Demo[/bold cyan]\\n\\n\"\n",
    "        \"This system uses multiple specialized agents to:\\n\"\n",
    "        \"• Research information using Brave Search API\\n\"\n",
    "        \"• Verify facts and claims\\n\"\n",
    "        \"• Analyze and synthesize findings\\n\"\n",
    "        \"• Ensure quality and completeness\\n\",\n",
    "        title=\"Welcome\",\n",
    "        border_style=\"cyan\"\n",
    "    ))\n",
    "    \n",
    "    # Predefined queries for demonstration\n",
    "    demo_queries = [\n",
    "        \"What are the latest breakthroughs in large language models and their implications for AGI?\",\n",
    "        \"Compare the environmental impact of electric vehicles vs hydrogen fuel cells\",\n",
    "        \"What are the emerging cybersecurity threats in 2024 and recommended defenses?\",\n",
    "        \"Analyze the current state of quantum computing startups and their funding\"\n",
    "    ]\n",
    "    \n",
    "    console.print(\"\\n[bold]Select a demo query:[/bold]\")\n",
    "    for i, query in enumerate(demo_queries, 1):\n",
    "        console.print(f\"  {i}. {query}\")\n",
    "    \n",
    "    # For notebook, we'll use the first query\n",
    "    selected_query = demo_queries[0]\n",
    "    console.print(f\"\\n[green]Selected: {selected_query}[/green]\\n\")\n",
    "    \n",
    "    # Run the workflow\n",
    "    start_time = time.time()\n",
    "    result = await orchestrator.run(selected_query, max_iterations=2)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Display results\n",
    "    visualize_results(result)\n",
    "    \n",
    "    # Show performance metrics\n",
    "    console.print(f\"\\n[bold]Performance Summary:[/bold]\")\n",
    "    console.print(f\"  Total time: {end_time - start_time:.2f} seconds\")\n",
    "    console.print(f\"  Search queries executed: {len(result['search_results'])}\")\n",
    "    console.print(f\"  Facts verified: {len(result['facts'])}\")\n",
    "    console.print(f\"  Agents involved: {len(result['agent_responses'])}\")\n",
    "    \n",
    "    # Export results\n",
    "    export_results(result, \"demo_results.json\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run the interactive demo\n",
    "demo_result = await interactive_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Best Practices and Optimization Tips\n",
    "\n",
    "### Key Recommendations:\n",
    "\n",
    "1. **API Rate Limiting**: Implement proper rate limiting for Brave Search API\n",
    "2. **Caching Strategy**: Use intelligent caching to reduce API calls and costs\n",
    "3. **Prompt Engineering**: Optimize prompts for each agent's specific role\n",
    "4. **Error Recovery**: Implement comprehensive error handling and fallback strategies\n",
    "5. **Monitoring**: Track performance metrics to identify bottlenecks\n",
    "6. **Grounding Quality**: Always verify information from multiple sources\n",
    "7. **Context Management**: Efficiently manage context size for LLM calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration best practices\n",
    "BEST_PRACTICE_CONFIG = {\n",
    "    \"brave_search\": {\n",
    "        \"max_results_per_query\": 10,\n",
    "        \"cache_ttl\": 3600,  # 1 hour\n",
    "        \"rate_limit\": 100,  # requests per minute\n",
    "        \"retry_attempts\": 3,\n",
    "        \"timeout\": 10\n",
    "    },\n",
    "    \"agents\": {\n",
    "        \"research\": {\n",
    "            \"max_queries\": 5,\n",
    "            \"query_diversity\": True,\n",
    "            \"include_news\": True\n",
    "        },\n",
    "        \"fact_check\": {\n",
    "            \"min_sources\": 2,\n",
    "            \"confidence_threshold\": 0.7\n",
    "        },\n",
    "        \"analysis\": {\n",
    "            \"synthesis_depth\": \"comprehensive\",\n",
    "            \"include_contradictions\": True\n",
    "        },\n",
    "        \"qa\": {\n",
    "            \"quality_threshold\": 0.8,\n",
    "            \"completeness_threshold\": 0.8\n",
    "        }\n",
    "    },\n",
    "    \"workflow\": {\n",
    "        \"max_iterations\": 3,\n",
    "        \"parallel_processing\": False,  # Can be enabled for faster processing\n",
    "        \"streaming\": True,\n",
    "        \"save_intermediate_results\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "console.print(Panel(\n",
    "    \"[green]✓ Multi-Agent Brave Search Workflow Setup Complete![/green]\\n\\n\"\n",
    "    \"The system is now ready to:\\n\"\n",
    "    \"• Execute complex research queries\\n\"\n",
    "    \"• Verify information through multiple sources\\n\"\n",
    "    \"• Provide grounded, fact-checked responses\\n\"\n",
    "    \"• Track performance and optimize over time\",\n",
    "    title=\"Setup Complete\",\n",
    "    border_style=\"green\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates a production-ready multi-agent system using Brave Search API for grounding. The system features:\n",
    "\n",
    "- **Modular Architecture**: Easily extensible with new agents\n",
    "- **Robust Search Integration**: Comprehensive Brave Search API implementation\n",
    "- **Fact Verification**: Multi-source verification for accuracy\n",
    "- **Quality Assurance**: Built-in quality checks and iterative refinement\n",
    "- **Performance Monitoring**: Track and optimize agent performance\n",
    "- **Error Recovery**: Graceful handling of failures\n",
    "\n",
    "The workflow can be adapted for various use cases including:\n",
    "- Research automation\n",
    "- Real-time fact-checking\n",
    "- Content generation with citations\n",
    "- Competitive intelligence gathering\n",
    "- News aggregation and analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}