{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hallucination Detection with MLflow Faithfulness Metrics\n",
        "\n",
        "This notebook demonstrates how to use MLflow faithfulness metrics to detect hallucinations in LLM outputs. We'll walk through several test examples showing how faithful vs. hallucinated answers are scored differently.\n",
        "\n",
        "## What is Hallucination Detection?\n",
        "\n",
        "Hallucination detection identifies when an LLM generates content that is:\n",
        "- **Not supported by the provided context**\n",
        "- **Factually incorrect** relative to the source\n",
        "- **Fabricated or invented** information\n",
        "\n",
        "### Faithfulness Score Interpretation:\n",
        "| Score Range | Interpretation | Risk Level |\n",
        "|-------------|----------------|------------|\n",
        "| >= 0.80 | Highly Faithful | Low - Safe |\n",
        "| 0.60 - 0.80 | Mostly Faithful | Low - Monitor |\n",
        "| 0.40 - 0.60 | Partially Faithful | Medium - Review |\n",
        "| 0.20 - 0.40 | Likely Hallucination | High - Flag |\n",
        "| < 0.20 | Severe Hallucination | Critical - Block |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q mlflow sentence-transformers pandas numpy scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%restart_python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from typing import List, Dict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "print(\"‚úÖ Libraries loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Faithfulness Metrics for Hallucination Detection\n",
        "\n",
        "We'll use three approaches to detect hallucinations:\n",
        "1. **Semantic Similarity**: Embedding-based comparison between answer and context\n",
        "2. **NLI (Natural Language Inference)**: Checks if context entails the answer\n",
        "3. **Token Overlap**: Simple baseline checking word overlap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HallucinationDetector:\n",
        "    \"\"\"\n",
        "    Detects hallucinations by measuring faithfulness of generated answers to source context.\n",
        "    Uses semantic similarity, NLI entailment, and token overlap approaches.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, \n",
        "                 embedding_model: str = \"all-MiniLM-L6-v2\",\n",
        "                 nli_model: str = \"cross-encoder/nli-deberta-v3-small\"):\n",
        "        \"\"\"Initialize with embedding and NLI models.\"\"\"\n",
        "        print(f\"Loading embedding model: {embedding_model}...\")\n",
        "        self.embedding_model = SentenceTransformer(embedding_model)\n",
        "        \n",
        "        print(f\"Loading NLI model: {nli_model}...\")\n",
        "        self.nli_model = CrossEncoder(nli_model)\n",
        "        \n",
        "        print(\"‚úÖ Hallucination detector ready!\")\n",
        "\n",
        "    def compute_semantic_faithfulness(self, answer: str, context: str) -> float:\n",
        "        \"\"\"Compute faithfulness using semantic similarity.\"\"\"\n",
        "        answer_embedding = self.embedding_model.encode([answer])[0]\n",
        "        context_embedding = self.embedding_model.encode([context])[0]\n",
        "        similarity = cosine_similarity([answer_embedding], [context_embedding])[0][0]\n",
        "        return max(0, min(1, (similarity + 1) / 2))\n",
        "\n",
        "    def compute_nli_faithfulness(self, answer: str, context: str) -> float:\n",
        "        \"\"\"Compute faithfulness using NLI entailment.\"\"\"\n",
        "        scores = self.nli_model.predict([(context, answer)])[0]\n",
        "        if isinstance(scores, (int, float)):\n",
        "            return 1 / (1 + np.exp(-scores))  # sigmoid\n",
        "        return float(scores[2])  # entailment score\n",
        "\n",
        "    def compute_token_overlap(self, answer: str, context: str) -> float:\n",
        "        \"\"\"Compute simple token overlap faithfulness.\"\"\"\n",
        "        stop_words = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', \n",
        "                      'to', 'of', 'in', 'for', 'on', 'with', 'at', 'by', 'from',\n",
        "                      'and', 'but', 'or', 'it', 'its', 'this', 'that'}\n",
        "        \n",
        "        answer_tokens = set(answer.lower().split()) - stop_words\n",
        "        context_tokens = set(context.lower().split()) - stop_words\n",
        "        \n",
        "        if not answer_tokens:\n",
        "            return 1.0\n",
        "        \n",
        "        overlap = answer_tokens.intersection(context_tokens)\n",
        "        return len(overlap) / len(answer_tokens)\n",
        "\n",
        "    def detect_hallucination(self, answer: str, context: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Detect if an answer is a hallucination.\n",
        "        \n",
        "        Returns:\n",
        "            Dictionary with scores, combined score, and hallucination verdict\n",
        "        \"\"\"\n",
        "        semantic = self.compute_semantic_faithfulness(answer, context)\n",
        "        nli = self.compute_nli_faithfulness(answer, context)\n",
        "        overlap = self.compute_token_overlap(answer, context)\n",
        "        \n",
        "        # Combined score: NLI weighted most (50%), then semantic (35%), then overlap (15%)\n",
        "        combined = 0.5 * nli + 0.35 * semantic + 0.15 * overlap\n",
        "        \n",
        "        # Determine hallucination category\n",
        "        if combined >= 0.8:\n",
        "            category = \"‚úÖ Highly Faithful\"\n",
        "            is_hallucination = False\n",
        "            risk = \"Low\"\n",
        "        elif combined >= 0.6:\n",
        "            category = \"‚úì Mostly Faithful\"\n",
        "            is_hallucination = False\n",
        "            risk = \"Low\"\n",
        "        elif combined >= 0.4:\n",
        "            category = \"‚ö†Ô∏è Partially Faithful\"\n",
        "            is_hallucination = True\n",
        "            risk = \"Medium\"\n",
        "        elif combined >= 0.2:\n",
        "            category = \"‚ùå Likely Hallucination\"\n",
        "            is_hallucination = True\n",
        "            risk = \"High\"\n",
        "        else:\n",
        "            category = \"üö´ Severe Hallucination\"\n",
        "            is_hallucination = True\n",
        "            risk = \"Critical\"\n",
        "        \n",
        "        return {\n",
        "            \"semantic_score\": semantic,\n",
        "            \"nli_score\": nli,\n",
        "            \"overlap_score\": overlap,\n",
        "            \"combined_score\": combined,\n",
        "            \"category\": category,\n",
        "            \"is_hallucination\": is_hallucination,\n",
        "            \"risk_level\": risk\n",
        "        }\n",
        "\n",
        "# Initialize the detector\n",
        "detector = HallucinationDetector()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Test Examples: Detecting Hallucinations\n",
        "\n",
        "Let's run through several examples showing how the faithfulness metrics detect hallucinations vs. faithful answers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define test examples with context, faithful answer, and hallucinated answer\n",
        "test_examples = [\n",
        "    {\n",
        "        \"name\": \"Example 1: Machine Learning Definition\",\n",
        "        \"context\": \"Machine learning is a subset of artificial intelligence that enables computers to learn from data without being explicitly programmed. It uses algorithms to identify patterns in data and make predictions.\",\n",
        "        \"faithful_answer\": \"Machine learning is a subset of AI that allows computers to learn from data using algorithms to find patterns and make predictions.\",\n",
        "        \"hallucinated_answer\": \"Machine learning was invented by Alan Turing in 1950 and requires quantum computers to function properly.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Example 2: Climate Change Causes\",\n",
        "        \"context\": \"Climate change is primarily driven by human activities, especially the burning of fossil fuels like coal, oil, and natural gas. These activities release greenhouse gases, particularly carbon dioxide, into the atmosphere.\",\n",
        "        \"faithful_answer\": \"Climate change is mainly caused by burning fossil fuels which release greenhouse gases like CO2 into the atmosphere.\",\n",
        "        \"hallucinated_answer\": \"Climate change is primarily caused by changes in Earth's orbit and volcanic activity, with human impact being minimal.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Example 3: Capital of France\",\n",
        "        \"context\": \"France is a country in Western Europe. Its capital city is Paris, which is also its largest city with a population of about 2.2 million. Paris is known for the Eiffel Tower.\",\n",
        "        \"faithful_answer\": \"The capital of France is Paris, with about 2.2 million people, known for the Eiffel Tower.\",\n",
        "        \"hallucinated_answer\": \"The capital of France is Lyon, which became the capital in 2010 after a national referendum.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Example 4: DNA Structure\",\n",
        "        \"context\": \"DNA (deoxyribonucleic acid) is a molecule that carries genetic instructions. It has a double helix structure discovered by Watson and Crick. DNA contains four bases: adenine, thymine, guanine, and cytosine.\",\n",
        "        \"faithful_answer\": \"DNA is a molecule with a double helix structure containing four bases: adenine, thymine, guanine, and cytosine.\",\n",
        "        \"hallucinated_answer\": \"DNA is a type of protein discovered in 1990. It has a single helix structure and contains only two bases.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Example 5: Photosynthesis\",\n",
        "        \"context\": \"Photosynthesis is the process by which plants convert light energy into chemical energy stored in glucose. Plants absorb carbon dioxide and water, using chlorophyll to capture light and produce glucose and oxygen.\",\n",
        "        \"faithful_answer\": \"Plants use photosynthesis to convert light into glucose by absorbing CO2 and water with chlorophyll, producing oxygen.\",\n",
        "        \"hallucinated_answer\": \"Photosynthesis is how plants breathe oxygen and release carbon dioxide at night through their roots.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"üìã Loaded {len(test_examples)} test examples for hallucination detection\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Running Hallucination Detection on All Examples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_detection_result(name: str, context: str, answer: str, result: Dict, answer_type: str):\n",
        "    \"\"\"Pretty print the hallucination detection result.\"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"üìù {name} - {answer_type}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"\\nüìÑ Context: {context[:100]}...\")\n",
        "    print(f\"\\nüí¨ Answer: {answer}\")\n",
        "    print(f\"\\nüìä SCORES:\")\n",
        "    print(f\"   ‚Ä¢ Semantic Similarity: {result['semantic_score']:.3f}\")\n",
        "    print(f\"   ‚Ä¢ NLI Entailment:      {result['nli_score']:.3f}\")\n",
        "    print(f\"   ‚Ä¢ Token Overlap:       {result['overlap_score']:.3f}\")\n",
        "    print(f\"   ‚Ä¢ Combined Score:      {result['combined_score']:.3f}\")\n",
        "    print(f\"\\nüéØ VERDICT: {result['category']}\")\n",
        "    print(f\"   ‚Ä¢ Is Hallucination: {'YES ‚ùå' if result['is_hallucination'] else 'NO ‚úì'}\")\n",
        "    print(f\"   ‚Ä¢ Risk Level: {result['risk_level']}\")\n",
        "\n",
        "# Run detection on all examples\n",
        "print(\"üîç HALLUCINATION DETECTION RESULTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for example in test_examples:\n",
        "    # Test faithful answer\n",
        "    faithful_result = detector.detect_hallucination(\n",
        "        example[\"faithful_answer\"], \n",
        "        example[\"context\"]\n",
        "    )\n",
        "    print_detection_result(\n",
        "        example[\"name\"], \n",
        "        example[\"context\"], \n",
        "        example[\"faithful_answer\"], \n",
        "        faithful_result, \n",
        "        \"FAITHFUL ANSWER\"\n",
        "    )\n",
        "    all_results.append({\n",
        "        \"example\": example[\"name\"],\n",
        "        \"type\": \"Faithful\",\n",
        "        \"combined_score\": faithful_result[\"combined_score\"],\n",
        "        \"is_hallucination\": faithful_result[\"is_hallucination\"],\n",
        "        \"category\": faithful_result[\"category\"]\n",
        "    })\n",
        "    \n",
        "    # Test hallucinated answer\n",
        "    hallucinated_result = detector.detect_hallucination(\n",
        "        example[\"hallucinated_answer\"], \n",
        "        example[\"context\"]\n",
        "    )\n",
        "    print_detection_result(\n",
        "        example[\"name\"], \n",
        "        example[\"context\"], \n",
        "        example[\"hallucinated_answer\"], \n",
        "        hallucinated_result, \n",
        "        \"HALLUCINATED ANSWER\"\n",
        "    )\n",
        "    all_results.append({\n",
        "        \"example\": example[\"name\"],\n",
        "        \"type\": \"Hallucinated\",\n",
        "        \"combined_score\": hallucinated_result[\"combined_score\"],\n",
        "        \"is_hallucination\": hallucinated_result[\"is_hallucination\"],\n",
        "        \"category\": hallucinated_result[\"category\"]\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Summary Results Table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary DataFrame\n",
        "results_df = pd.DataFrame(all_results)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä SUMMARY: HALLUCINATION DETECTION RESULTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Format the dataframe for display\n",
        "display_df = results_df.copy()\n",
        "display_df[\"combined_score\"] = display_df[\"combined_score\"].apply(lambda x: f\"{x:.3f}\")\n",
        "display_df[\"is_hallucination\"] = display_df[\"is_hallucination\"].apply(lambda x: \"‚ùå YES\" if x else \"‚úì NO\")\n",
        "display_df.columns = [\"Example\", \"Answer Type\", \"Score\", \"Hallucination?\", \"Category\"]\n",
        "\n",
        "print(\"\\n\")\n",
        "print(display_df.to_string(index=False))\n",
        "\n",
        "# Calculate accuracy\n",
        "faithful_correct = sum(1 for r in all_results if r[\"type\"] == \"Faithful\" and not r[\"is_hallucination\"])\n",
        "hallucinated_correct = sum(1 for r in all_results if r[\"type\"] == \"Hallucinated\" and r[\"is_hallucination\"])\n",
        "total = len(test_examples)\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"üìà DETECTION ACCURACY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"   ‚Ä¢ Faithful answers correctly identified:     {faithful_correct}/{total} ({faithful_correct/total*100:.0f}%)\")\n",
        "print(f\"   ‚Ä¢ Hallucinations correctly detected:         {hallucinated_correct}/{total} ({hallucinated_correct/total*100:.0f}%)\")\n",
        "print(f\"   ‚Ä¢ Overall accuracy:                          {(faithful_correct+hallucinated_correct)/(total*2)*100:.0f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Interactive Testing: Try Your Own Examples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_hallucination(context: str, answer: str):\n",
        "    \"\"\"\n",
        "    Test if an answer is a hallucination given a context.\n",
        "    \n",
        "    Usage:\n",
        "        test_hallucination(\n",
        "            context=\"The Eiffel Tower is located in Paris, France.\",\n",
        "            answer=\"The Eiffel Tower is in Paris.\"\n",
        "        )\n",
        "    \"\"\"\n",
        "    result = detector.detect_hallucination(answer, context)\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(\"üîç HALLUCINATION TEST\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nüìÑ Context:\\n{context}\")\n",
        "    print(f\"\\nüí¨ Answer:\\n{answer}\")\n",
        "    print(f\"\\nüìä SCORES:\")\n",
        "    print(f\"   ‚Ä¢ Semantic: {result['semantic_score']:.3f}\")\n",
        "    print(f\"   ‚Ä¢ NLI:      {result['nli_score']:.3f}\")\n",
        "    print(f\"   ‚Ä¢ Overlap:  {result['overlap_score']:.3f}\")\n",
        "    print(f\"   ‚Ä¢ Combined: {result['combined_score']:.3f}\")\n",
        "    print(f\"\\nüéØ RESULT: {result['category']}\")\n",
        "    print(f\"   Risk Level: {result['risk_level']}\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    return result\n",
        "\n",
        "# Example usage - test a faithful answer\n",
        "print(\"TEST 1: Faithful Answer\")\n",
        "test_hallucination(\n",
        "    context=\"The Great Wall of China is over 13,000 miles long and was built over many centuries to protect against invasions.\",\n",
        "    answer=\"The Great Wall of China is over 13,000 miles long and was built for protection against invasions.\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage - test a hallucinated answer\n",
        "print(\"TEST 2: Hallucinated Answer\")\n",
        "test_hallucination(\n",
        "    context=\"The Great Wall of China is over 13,000 miles long and was built over many centuries to protect against invasions.\",\n",
        "    answer=\"The Great Wall of China is 500 miles long and was built in 1920 by the British Empire.\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Score Comparison Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate average scores by answer type\n",
        "faithful_scores = [r[\"combined_score\"] for r in all_results if r[\"type\"] == \"Faithful\"]\n",
        "hallucinated_scores = [r[\"combined_score\"] for r in all_results if r[\"type\"] == \"Hallucinated\"]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üìä SCORE COMPARISON BY ANSWER TYPE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nüìó FAITHFUL ANSWERS:\")\n",
        "print(f\"   Average Score: {np.mean(faithful_scores):.3f}\")\n",
        "print(f\"   Min Score:     {np.min(faithful_scores):.3f}\")\n",
        "print(f\"   Max Score:     {np.max(faithful_scores):.3f}\")\n",
        "\n",
        "print(\"\\nüìï HALLUCINATED ANSWERS:\")\n",
        "print(f\"   Average Score: {np.mean(hallucinated_scores):.3f}\")\n",
        "print(f\"   Min Score:     {np.min(hallucinated_scores):.3f}\")\n",
        "print(f\"   Max Score:     {np.max(hallucinated_scores):.3f}\")\n",
        "\n",
        "print(\"\\nüìâ SCORE DIFFERENCE:\")\n",
        "score_diff = np.mean(faithful_scores) - np.mean(hallucinated_scores)\n",
        "print(f\"   Faithful vs Hallucinated Gap: {score_diff:.3f}\")\n",
        "print(f\"   This {score_diff:.1%} gap shows clear separation between faithful and hallucinated content!\")\n",
        "\n",
        "# Visual bar representation\n",
        "print(\"\\nüìä VISUAL COMPARISON:\")\n",
        "print(f\"   Faithful:     {'‚ñà' * int(np.mean(faithful_scores) * 20):<20} {np.mean(faithful_scores):.2f}\")\n",
        "print(f\"   Hallucinated: {'‚ñà' * int(np.mean(hallucinated_scores) * 20):<20} {np.mean(hallucinated_scores):.2f}\")\n",
        "print(f\"   Threshold:    {'‚îÄ' * 8}‚îÇ{'‚îÄ' * 11}  0.40 (hallucination cutoff)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Key Takeaways\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\"\"\n",
        "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "‚ïë                    KEY TAKEAWAYS: HALLUCINATION DETECTION                    ‚ïë\n",
        "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "\n",
        "‚úÖ WHAT WE DEMONSTRATED:\n",
        "\n",
        "1. MULTI-SIGNAL APPROACH\n",
        "   ‚Ä¢ Semantic similarity alone is not enough\n",
        "   ‚Ä¢ NLI (entailment) provides the strongest signal\n",
        "   ‚Ä¢ Token overlap serves as a useful baseline\n",
        "   ‚Ä¢ Combined scoring improves robustness\n",
        "\n",
        "2. CLEAR SEPARATION\n",
        "   ‚Ä¢ Faithful answers typically score > 0.60\n",
        "   ‚Ä¢ Hallucinations typically score < 0.40\n",
        "   ‚Ä¢ The gap provides reliable detection\n",
        "\n",
        "3. TYPES OF HALLUCINATIONS DETECTED:\n",
        "   ‚Ä¢ Factual errors (wrong dates, numbers, names)\n",
        "   ‚Ä¢ Contradictions (opposite of what context says)\n",
        "   ‚Ä¢ Fabrications (invented information not in context)\n",
        "   ‚Ä¢ Misattributions (assigning actions to wrong entities)\n",
        "\n",
        "4. RECOMMENDED THRESHOLDS:\n",
        "   ‚Ä¢ >= 0.60: Safe to use (faithful)\n",
        "   ‚Ä¢ 0.40-0.60: Needs human review\n",
        "   ‚Ä¢ < 0.40: Flag as potential hallucination\n",
        "   ‚Ä¢ < 0.20: Block from production\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "üîó NEXT STEPS:\n",
        "   ‚Ä¢ Integrate with MLflow for production monitoring\n",
        "   ‚Ä¢ Set up alerts for low-scoring responses\n",
        "   ‚Ä¢ Use in RAG evaluation pipelines\n",
        "   ‚Ä¢ Combine with relevance and answer quality metrics\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\"\"\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
