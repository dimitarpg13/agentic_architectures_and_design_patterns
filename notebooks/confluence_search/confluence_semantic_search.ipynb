{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confluence Semantic Search System\n",
    "\n",
    "A production-ready implementation for semantic search through Confluence page content using embeddings and vector databases.\n",
    "\n",
    "## Features\n",
    "- Confluence API integration with pagination and rate limiting\n",
    "- Multiple embedding model support (OpenAI, Sentence Transformers)\n",
    "- Vector storage with FAISS, ChromaDB, or Pinecone\n",
    "- Incremental indexing and caching\n",
    "- Advanced search with metadata filtering\n",
    "- Performance monitoring and observability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install atlassian-python-api\n",
    "!pip install openai\n",
    "!pip install sentence-transformers\n",
    "!pip install faiss-cpu  # or faiss-gpu for GPU support\n",
    "!pip install chromadb\n",
    "!pip install pinecone-client\n",
    "!pip install beautifulsoup4\n",
    "!pip install lxml\n",
    "!pip install python-dotenv\n",
    "!pip install tqdm\n",
    "!pip install pandas\n",
    "!pip install plotly\n",
    "!pip install tenacity\n",
    "!pip install langchain langchain-community\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import hashlib\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "from dataclasses import dataclass, asdict\n",
    "from enum import Enum\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "from atlassian import Confluence\n",
    "from bs4 import BeautifulSoup\n",
    "import openai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import pinecone\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "import tiktoken\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration class\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Confluence settings\n",
    "    confluence_url: str = os.getenv('CONFLUENCE_URL', 'https://your-domain.atlassian.net')\n",
    "    confluence_username: str = os.getenv('CONFLUENCE_USERNAME', '')\n",
    "    confluence_api_token: str = os.getenv('CONFLUENCE_API_TOKEN', '')\n",
    "    \n",
    "    # Embedding settings\n",
    "    embedding_model: str = 'openai'  # 'openai' or 'sentence-transformers'\n",
    "    openai_api_key: str = os.getenv('OPENAI_API_KEY', '')\n",
    "    openai_model: str = 'text-embedding-3-small'\n",
    "    sentence_transformer_model: str = 'all-MiniLM-L6-v2'\n",
    "    \n",
    "    # Vector store settings\n",
    "    vector_store: str = 'faiss'  # 'faiss', 'chroma', or 'pinecone'\n",
    "    pinecone_api_key: str = os.getenv('PINECONE_API_KEY', '')\n",
    "    pinecone_environment: str = os.getenv('PINECONE_ENVIRONMENT', '')\n",
    "    pinecone_index_name: str = 'confluence-search'\n",
    "    \n",
    "    # Processing settings\n",
    "    chunk_size: int = 1000  # Characters per chunk\n",
    "    chunk_overlap: int = 200\n",
    "    max_pages_per_batch: int = 50\n",
    "    cache_dir: str = './confluence_cache'\n",
    "    \n",
    "    # Search settings\n",
    "    top_k: int = 10\n",
    "    min_similarity: float = 0.7\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Confluence Content Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfluenceExtractor:\n",
    "    \"\"\"Extract and process content from Confluence pages.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.confluence = Confluence(\n",
    "            url=config.confluence_url,\n",
    "            username=config.confluence_username,\n",
    "            password=config.confluence_api_token,\n",
    "            cloud=True\n",
    "        )\n",
    "        self.cache_dir = Path(config.cache_dir)\n",
    "        self.cache_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "    def get_all_pages(self, space_key: Optional[str] = None, limit: int = 1000) -> List[Dict]:\n",
    "        \"\"\"Fetch all pages from Confluence with pagination.\"\"\"\n",
    "        logger.info(f\"Fetching pages from Confluence (space: {space_key or 'all'})\")\n",
    "        \n",
    "        all_pages = []\n",
    "        start = 0\n",
    "        batch_size = 50\n",
    "        \n",
    "        while start < limit:\n",
    "            try:\n",
    "                if space_key:\n",
    "                    results = self.confluence.get_all_pages_from_space(\n",
    "                        space=space_key,\n",
    "                        start=start,\n",
    "                        limit=min(batch_size, limit - start),\n",
    "                        expand='body.storage,version,history'\n",
    "                    )\n",
    "                else:\n",
    "                    # Get pages from all spaces\n",
    "                    results = self.confluence.get_all_pages_by_label(\n",
    "                        label='',\n",
    "                        start=start,\n",
    "                        limit=min(batch_size, limit - start)\n",
    "                    )\n",
    "                \n",
    "                if not results:\n",
    "                    break\n",
    "                    \n",
    "                all_pages.extend(results)\n",
    "                start += batch_size\n",
    "                \n",
    "                logger.info(f\"Fetched {len(all_pages)} pages so far...\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error fetching pages: {e}\")\n",
    "                break\n",
    "                \n",
    "        return all_pages\n",
    "    \n",
    "    def extract_page_content(self, page: Dict) -> Dict:\n",
    "        \"\"\"Extract and clean content from a Confluence page.\"\"\"\n",
    "        try:\n",
    "            # Get HTML content\n",
    "            html_content = page.get('body', {}).get('storage', {}).get('value', '')\n",
    "            \n",
    "            # Parse HTML and extract text\n",
    "            soup = BeautifulSoup(html_content, 'lxml')\n",
    "            \n",
    "            # Remove script and style elements\n",
    "            for script in soup(['script', 'style']):\n",
    "                script.decompose()\n",
    "                \n",
    "            # Get text\n",
    "            text = soup.get_text(separator=' ', strip=True)\n",
    "            \n",
    "            # Clean up whitespace\n",
    "            text = ' '.join(text.split())\n",
    "            \n",
    "            # Extract metadata\n",
    "            metadata = {\n",
    "                'page_id': page.get('id'),\n",
    "                'title': page.get('title', ''),\n",
    "                'space': page.get('space', {}).get('key', ''),\n",
    "                'url': f\"{self.config.confluence_url}/wiki{page.get('_links', {}).get('webui', '')}\",\n",
    "                'version': page.get('version', {}).get('number', 0),\n",
    "                'created_by': page.get('history', {}).get('createdBy', {}).get('displayName', ''),\n",
    "                'created_date': page.get('history', {}).get('createdDate', ''),\n",
    "                'last_updated': page.get('version', {}).get('when', ''),\n",
    "                'last_updated_by': page.get('version', {}).get('by', {}).get('displayName', '')\n",
    "            }\n",
    "            \n",
    "            return {\n",
    "                'content': text,\n",
    "                'metadata': metadata,\n",
    "                'html': html_content\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting content from page {page.get('id')}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def chunk_content(self, text: str, metadata: Dict) -> List[Dict]:\n",
    "        \"\"\"Split content into chunks for embedding.\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        # Simple character-based chunking with overlap\n",
    "        for i in range(0, len(text), self.config.chunk_size - self.config.chunk_overlap):\n",
    "            chunk_text = text[i:i + self.config.chunk_size]\n",
    "            \n",
    "            # Create chunk with metadata\n",
    "            chunk = {\n",
    "                'text': chunk_text,\n",
    "                'metadata': {\n",
    "                    **metadata,\n",
    "                    'chunk_index': len(chunks),\n",
    "                    'chunk_start': i,\n",
    "                    'chunk_end': min(i + self.config.chunk_size, len(text))\n",
    "                }\n",
    "            }\n",
    "            chunks.append(chunk)\n",
    "            \n",
    "        return chunks\n",
    "    \n",
    "    def process_pages(self, pages: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Process all pages and create chunks.\"\"\"\n",
    "        all_chunks = []\n",
    "        \n",
    "        for page in tqdm(pages, desc=\"Processing pages\"):\n",
    "            content_data = self.extract_page_content(page)\n",
    "            \n",
    "            if content_data and content_data['content']:\n",
    "                chunks = self.chunk_content(\n",
    "                    content_data['content'],\n",
    "                    content_data['metadata']\n",
    "                )\n",
    "                all_chunks.extend(chunks)\n",
    "                \n",
    "        logger.info(f\"Created {len(all_chunks)} chunks from {len(pages)} pages\")\n",
    "        return all_chunks\n",
    "\n",
    "# Initialize extractor\n",
    "extractor = ConfluenceExtractor(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingGenerator:\n",
    "    \"\"\"Generate embeddings using various models.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        \n",
    "        if config.embedding_model == 'openai':\n",
    "            openai.api_key = config.openai_api_key\n",
    "            self.encoder = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "        elif config.embedding_model == 'sentence-transformers':\n",
    "            self.model = SentenceTransformer(config.sentence_transformer_model)\n",
    "            \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in text.\"\"\"\n",
    "        if self.config.embedding_model == 'openai':\n",
    "            return len(self.encoder.encode(text))\n",
    "        else:\n",
    "            # Approximate for sentence transformers\n",
    "            return len(text.split()) * 1.3\n",
    "    \n",
    "    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "    def generate_embeddings(self, texts: List[str], batch_size: int = 100) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings for a list of texts.\"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            \n",
    "            if self.config.embedding_model == 'openai':\n",
    "                response = openai.embeddings.create(\n",
    "                    model=self.config.openai_model,\n",
    "                    input=batch_texts\n",
    "                )\n",
    "                batch_embeddings = [item.embedding for item in response.data]\n",
    "                \n",
    "            elif self.config.embedding_model == 'sentence-transformers':\n",
    "                batch_embeddings = self.model.encode(\n",
    "                    batch_texts,\n",
    "                    show_progress_bar=False,\n",
    "                    convert_to_numpy=True\n",
    "                )\n",
    "                \n",
    "            embeddings.extend(batch_embeddings)\n",
    "            \n",
    "        return np.array(embeddings, dtype=np.float32)\n",
    "    \n",
    "    def generate_query_embedding(self, query: str) -> np.ndarray:\n",
    "        \"\"\"Generate embedding for a single query.\"\"\"\n",
    "        if self.config.embedding_model == 'openai':\n",
    "            response = openai.embeddings.create(\n",
    "                model=self.config.openai_model,\n",
    "                input=query\n",
    "            )\n",
    "            return np.array(response.data[0].embedding, dtype=np.float32)\n",
    "            \n",
    "        elif self.config.embedding_model == 'sentence-transformers':\n",
    "            return self.model.encode(query, convert_to_numpy=True)\n",
    "\n",
    "# Initialize embedding generator\n",
    "embedder = EmbeddingGenerator(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Vector Store Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Abstract base class for vector stores.\"\"\"\n",
    "    \n",
    "    def add_embeddings(self, embeddings: np.ndarray, metadata: List[Dict]):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def search(self, query_embedding: np.ndarray, k: int = 10) -> Tuple[List[int], List[float]]:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def save(self, path: str):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def load(self, path: str):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class FAISSVectorStore(VectorStore):\n",
    "    \"\"\"FAISS-based vector store.\"\"\"\n",
    "    \n",
    "    def __init__(self, dimension: int):\n",
    "        self.dimension = dimension\n",
    "        self.index = faiss.IndexFlatL2(dimension)\n",
    "        self.metadata = []\n",
    "        \n",
    "    def add_embeddings(self, embeddings: np.ndarray, metadata: List[Dict]):\n",
    "        \"\"\"Add embeddings to the index.\"\"\"\n",
    "        self.index.add(embeddings)\n",
    "        self.metadata.extend(metadata)\n",
    "        logger.info(f\"Added {len(embeddings)} embeddings to FAISS index\")\n",
    "        \n",
    "    def search(self, query_embedding: np.ndarray, k: int = 10) -> Tuple[List[Dict], List[float]]:\n",
    "        \"\"\"Search for similar embeddings.\"\"\"\n",
    "        query_embedding = query_embedding.reshape(1, -1)\n",
    "        distances, indices = self.index.search(query_embedding, k)\n",
    "        \n",
    "        results = []\n",
    "        for idx in indices[0]:\n",
    "            if idx < len(self.metadata):\n",
    "                results.append(self.metadata[idx])\n",
    "                \n",
    "        # Convert L2 distance to similarity score\n",
    "        similarities = 1 / (1 + distances[0])\n",
    "        \n",
    "        return results, similarities.tolist()\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save index and metadata to disk.\"\"\"\n",
    "        faiss.write_index(self.index, f\"{path}.index\")\n",
    "        with open(f\"{path}.metadata\", 'wb') as f:\n",
    "            pickle.dump(self.metadata, f)\n",
    "        logger.info(f\"Saved FAISS index to {path}\")\n",
    "            \n",
    "    def load(self, path: str):\n",
    "        \"\"\"Load index and metadata from disk.\"\"\"\n",
    "        self.index = faiss.read_index(f\"{path}.index\")\n",
    "        with open(f\"{path}.metadata\", 'rb') as f:\n",
    "            self.metadata = pickle.load(f)\n",
    "        logger.info(f\"Loaded FAISS index from {path}\")\n",
    "\n",
    "\n",
    "class ChromaVectorStore(VectorStore):\n",
    "    \"\"\"ChromaDB-based vector store.\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"confluence_docs\"):\n",
    "        self.client = chromadb.Client(Settings(\n",
    "            chroma_db_impl=\"duckdb+parquet\",\n",
    "            persist_directory=\"./chroma_db\"\n",
    "        ))\n",
    "        self.collection = self.client.get_or_create_collection(collection_name)\n",
    "        \n",
    "    def add_embeddings(self, embeddings: np.ndarray, metadata: List[Dict]):\n",
    "        \"\"\"Add embeddings to ChromaDB.\"\"\"\n",
    "        ids = [f\"doc_{i}_{datetime.now().timestamp()}\" for i in range(len(embeddings))]\n",
    "        \n",
    "        # Extract texts from metadata\n",
    "        documents = [m.get('text', '') for m in metadata]\n",
    "        \n",
    "        # Clean metadata for ChromaDB\n",
    "        clean_metadata = []\n",
    "        for m in metadata:\n",
    "            clean_m = {k: str(v) for k, v in m.items() if k != 'text'}\n",
    "            clean_metadata.append(clean_m)\n",
    "        \n",
    "        self.collection.add(\n",
    "            embeddings=embeddings.tolist(),\n",
    "            documents=documents,\n",
    "            metadatas=clean_metadata,\n",
    "            ids=ids\n",
    "        )\n",
    "        logger.info(f\"Added {len(embeddings)} embeddings to ChromaDB\")\n",
    "        \n",
    "    def search(self, query_embedding: np.ndarray, k: int = 10) -> Tuple[List[Dict], List[float]]:\n",
    "        \"\"\"Search for similar documents in ChromaDB.\"\"\"\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[query_embedding.tolist()],\n",
    "            n_results=k\n",
    "        )\n",
    "        \n",
    "        metadata_results = []\n",
    "        for i in range(len(results['ids'][0])):\n",
    "            meta = results['metadatas'][0][i]\n",
    "            meta['text'] = results['documents'][0][i]\n",
    "            metadata_results.append(meta)\n",
    "            \n",
    "        similarities = [1 - d for d in results['distances'][0]]  # Convert distance to similarity\n",
    "        \n",
    "        return metadata_results, similarities\n",
    "\n",
    "\n",
    "class PineconeVectorStore(VectorStore):\n",
    "    \"\"\"Pinecone-based vector store.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config, dimension: int):\n",
    "        pinecone.init(\n",
    "            api_key=config.pinecone_api_key,\n",
    "            environment=config.pinecone_environment\n",
    "        )\n",
    "        \n",
    "        # Create or get index\n",
    "        if config.pinecone_index_name not in pinecone.list_indexes():\n",
    "            pinecone.create_index(\n",
    "                name=config.pinecone_index_name,\n",
    "                dimension=dimension,\n",
    "                metric='cosine'\n",
    "            )\n",
    "            \n",
    "        self.index = pinecone.Index(config.pinecone_index_name)\n",
    "        self.metadata_store = {}\n",
    "        \n",
    "    def add_embeddings(self, embeddings: np.ndarray, metadata: List[Dict]):\n",
    "        \"\"\"Add embeddings to Pinecone.\"\"\"\n",
    "        batch_size = 100\n",
    "        \n",
    "        for i in range(0, len(embeddings), batch_size):\n",
    "            batch_embeddings = embeddings[i:i + batch_size]\n",
    "            batch_metadata = metadata[i:i + batch_size]\n",
    "            \n",
    "            # Create unique IDs\n",
    "            ids = [f\"doc_{i}_{j}\" for j in range(len(batch_embeddings))]\n",
    "            \n",
    "            # Prepare vectors for upsert\n",
    "            vectors = []\n",
    "            for j, (emb, meta) in enumerate(zip(batch_embeddings, batch_metadata)):\n",
    "                vec_id = ids[j]\n",
    "                self.metadata_store[vec_id] = meta\n",
    "                \n",
    "                # Pinecone metadata must be flat dict with string/number values\n",
    "                pinecone_meta = {\n",
    "                    k: v for k, v in meta.items() \n",
    "                    if k in ['title', 'space', 'page_id'] and isinstance(v, (str, int, float))\n",
    "                }\n",
    "                \n",
    "                vectors.append((vec_id, emb.tolist(), pinecone_meta))\n",
    "                \n",
    "            self.index.upsert(vectors=vectors)\n",
    "            \n",
    "        logger.info(f\"Added {len(embeddings)} embeddings to Pinecone\")\n",
    "        \n",
    "    def search(self, query_embedding: np.ndarray, k: int = 10) -> Tuple[List[Dict], List[float]]:\n",
    "        \"\"\"Search for similar vectors in Pinecone.\"\"\"\n",
    "        results = self.index.query(\n",
    "            vector=query_embedding.tolist(),\n",
    "            top_k=k,\n",
    "            include_metadata=True\n",
    "        )\n",
    "        \n",
    "        metadata_results = []\n",
    "        similarities = []\n",
    "        \n",
    "        for match in results.matches:\n",
    "            if match.id in self.metadata_store:\n",
    "                metadata_results.append(self.metadata_store[match.id])\n",
    "            else:\n",
    "                metadata_results.append(match.metadata)\n",
    "            similarities.append(match.score)\n",
    "            \n",
    "        return metadata_results, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Semantic Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfluenceSemanticSearch:\n",
    "    \"\"\"Main semantic search engine for Confluence.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.extractor = ConfluenceExtractor(config)\n",
    "        self.embedder = EmbeddingGenerator(config)\n",
    "        self.vector_store = None\n",
    "        self.chunks = []\n",
    "        \n",
    "    def initialize_vector_store(self, dimension: int = None):\n",
    "        \"\"\"Initialize the appropriate vector store.\"\"\"\n",
    "        if dimension is None:\n",
    "            # Get dimension from a test embedding\n",
    "            test_embedding = self.embedder.generate_query_embedding(\"test\")\n",
    "            dimension = len(test_embedding)\n",
    "            \n",
    "        if self.config.vector_store == 'faiss':\n",
    "            self.vector_store = FAISSVectorStore(dimension)\n",
    "        elif self.config.vector_store == 'chroma':\n",
    "            self.vector_store = ChromaVectorStore()\n",
    "        elif self.config.vector_store == 'pinecone':\n",
    "            self.vector_store = PineconeVectorStore(self.config, dimension)\n",
    "            \n",
    "        logger.info(f\"Initialized {self.config.vector_store} vector store\")\n",
    "        \n",
    "    def index_confluence_content(self, space_key: Optional[str] = None, limit: int = 1000):\n",
    "        \"\"\"Index Confluence content into the vector store.\"\"\"\n",
    "        # Fetch pages\n",
    "        pages = self.extractor.get_all_pages(space_key, limit)\n",
    "        \n",
    "        if not pages:\n",
    "            logger.warning(\"No pages found to index\")\n",
    "            return\n",
    "            \n",
    "        # Process pages into chunks\n",
    "        self.chunks = self.extractor.process_pages(pages)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        texts = [chunk['text'] for chunk in self.chunks]\n",
    "        embeddings = self.embedder.generate_embeddings(texts)\n",
    "        \n",
    "        # Initialize vector store if not already done\n",
    "        if self.vector_store is None:\n",
    "            self.initialize_vector_store(embeddings.shape[1])\n",
    "            \n",
    "        # Add to vector store\n",
    "        self.vector_store.add_embeddings(embeddings, self.chunks)\n",
    "        \n",
    "        logger.info(f\"Successfully indexed {len(self.chunks)} chunks from {len(pages)} pages\")\n",
    "        \n",
    "    def search(self, query: str, k: int = None, filter_space: Optional[str] = None,\n",
    "               filter_metadata: Optional[Dict] = None) -> List[Dict]:\n",
    "        \"\"\"Search for relevant content.\"\"\"\n",
    "        if self.vector_store is None:\n",
    "            raise ValueError(\"Vector store not initialized. Please index content first.\")\n",
    "            \n",
    "        k = k or self.config.top_k\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedder.generate_query_embedding(query)\n",
    "        \n",
    "        # Search vector store\n",
    "        results, similarities = self.vector_store.search(query_embedding, k * 2)  # Get extra for filtering\n",
    "        \n",
    "        # Apply filters\n",
    "        filtered_results = []\n",
    "        for result, similarity in zip(results, similarities):\n",
    "            # Apply similarity threshold\n",
    "            if similarity < self.config.min_similarity:\n",
    "                continue\n",
    "                \n",
    "            # Apply space filter\n",
    "            if filter_space and result['metadata'].get('space') != filter_space:\n",
    "                continue\n",
    "                \n",
    "            # Apply custom metadata filters\n",
    "            if filter_metadata:\n",
    "                match = True\n",
    "                for key, value in filter_metadata.items():\n",
    "                    if result['metadata'].get(key) != value:\n",
    "                        match = False\n",
    "                        break\n",
    "                if not match:\n",
    "                    continue\n",
    "                    \n",
    "            # Add similarity score to result\n",
    "            result['similarity'] = similarity\n",
    "            filtered_results.append(result)\n",
    "            \n",
    "            if len(filtered_results) >= k:\n",
    "                break\n",
    "                \n",
    "        return filtered_results\n",
    "    \n",
    "    def rerank_results(self, query: str, results: List[Dict], use_cross_encoder: bool = True) -> List[Dict]:\n",
    "        \"\"\"Rerank search results using cross-encoder or other methods.\"\"\"\n",
    "        if use_cross_encoder:\n",
    "            from sentence_transformers import CrossEncoder\n",
    "            \n",
    "            # Initialize cross-encoder for reranking\n",
    "            cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "            \n",
    "            # Prepare pairs for cross-encoder\n",
    "            pairs = [[query, result['text']] for result in results]\n",
    "            \n",
    "            # Get scores\n",
    "            scores = cross_encoder.predict(pairs)\n",
    "            \n",
    "            # Add rerank scores and sort\n",
    "            for result, score in zip(results, scores):\n",
    "                result['rerank_score'] = float(score)\n",
    "                \n",
    "            results.sort(key=lambda x: x['rerank_score'], reverse=True)\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def save_index(self, path: str):\n",
    "        \"\"\"Save the index to disk.\"\"\"\n",
    "        if self.vector_store and hasattr(self.vector_store, 'save'):\n",
    "            self.vector_store.save(path)\n",
    "            \n",
    "        # Save chunks separately\n",
    "        with open(f\"{path}.chunks\", 'wb') as f:\n",
    "            pickle.dump(self.chunks, f)\n",
    "            \n",
    "        logger.info(f\"Saved index to {path}\")\n",
    "        \n",
    "    def load_index(self, path: str):\n",
    "        \"\"\"Load the index from disk.\"\"\"\n",
    "        # Load chunks\n",
    "        with open(f\"{path}.chunks\", 'rb') as f:\n",
    "            self.chunks = pickle.load(f)\n",
    "            \n",
    "        # Initialize and load vector store\n",
    "        if self.config.vector_store == 'faiss':\n",
    "            self.vector_store = FAISSVectorStore(1)  # Dimension will be loaded\n",
    "            self.vector_store.load(path)\n",
    "            \n",
    "        logger.info(f\"Loaded index from {path}\")\n",
    "\n",
    "# Initialize search engine\n",
    "search_engine = ConfluenceSemanticSearch(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Usage Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Index all content from a specific space\n",
    "def index_space_example():\n",
    "    \"\"\"Index content from a specific Confluence space.\"\"\"\n",
    "    \n",
    "    # Initialize search engine\n",
    "    search_engine = ConfluenceSemanticSearch(config)\n",
    "    \n",
    "    # Index content from a specific space\n",
    "    search_engine.index_confluence_content(\n",
    "        space_key='DOCS',  # Replace with your space key\n",
    "        limit=100  # Limit number of pages for testing\n",
    "    )\n",
    "    \n",
    "    # Save index for later use\n",
    "    search_engine.save_index('./confluence_index')\n",
    "    \n",
    "    return search_engine\n",
    "\n",
    "# Run indexing\n",
    "# search_engine = index_space_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Perform semantic search\n",
    "def search_example(search_engine, query: str):\n",
    "    \"\"\"Perform a semantic search and display results.\"\"\"\n",
    "    \n",
    "    # Search for relevant content\n",
    "    results = search_engine.search(\n",
    "        query=query,\n",
    "        k=5,\n",
    "        filter_space=None  # Optional: filter by space\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nSearch Query: {query}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\nResult {i}:\")\n",
    "        print(f\"Title: {result['metadata']['title']}\")\n",
    "        print(f\"Space: {result['metadata']['space']}\")\n",
    "        print(f\"URL: {result['metadata']['url']}\")\n",
    "        print(f\"Similarity: {result['similarity']:.3f}\")\n",
    "        print(f\"Content: {result['text'][:200]}...\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Example search\n",
    "# results = search_example(search_engine, \"How to deploy to production?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Advanced search with reranking\n",
    "def advanced_search_example(search_engine, query: str):\n",
    "    \"\"\"Perform advanced search with reranking.\"\"\"\n",
    "    \n",
    "    # Initial search\n",
    "    results = search_engine.search(query=query, k=10)\n",
    "    \n",
    "    # Rerank results\n",
    "    reranked_results = search_engine.rerank_results(query, results)\n",
    "    \n",
    "    # Create comparison dataframe\n",
    "    df_results = pd.DataFrame([\n",
    "        {\n",
    "            'Title': r['metadata']['title'],\n",
    "            'Initial Rank': i + 1,\n",
    "            'Similarity Score': r['similarity'],\n",
    "            'Rerank Score': r.get('rerank_score', 0)\n",
    "        }\n",
    "        for i, r in enumerate(results)\n",
    "    ])\n",
    "    \n",
    "    # Sort by rerank score\n",
    "    df_results = df_results.sort_values('Rerank Score', ascending=False).reset_index(drop=True)\n",
    "    df_results['Final Rank'] = range(1, len(df_results) + 1)\n",
    "    \n",
    "    print(f\"\\nSearch Query: {query}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(df_results[['Final Rank', 'Initial Rank', 'Title', 'Similarity Score', 'Rerank Score']])\n",
    "    \n",
    "    return reranked_results\n",
    "\n",
    "# Example advanced search\n",
    "# reranked_results = advanced_search_example(search_engine, \"API authentication best practices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Monitoring and Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchAnalytics:\n",
    "    \"\"\"Analytics and monitoring for search system.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.search_logs = []\n",
    "        \n",
    "    def log_search(self, query: str, results: List[Dict], response_time: float):\n",
    "        \"\"\"Log search query and results.\"\"\"\n",
    "        log_entry = {\n",
    "            'timestamp': datetime.now(),\n",
    "            'query': query,\n",
    "            'num_results': len(results),\n",
    "            'top_result': results[0]['metadata']['title'] if results else None,\n",
    "            'avg_similarity': np.mean([r['similarity'] for r in results]) if results else 0,\n",
    "            'response_time': response_time\n",
    "        }\n",
    "        self.search_logs.append(log_entry)\n",
    "        \n",
    "    def get_analytics_dashboard(self) -> Dict:\n",
    "        \"\"\"Generate analytics dashboard data.\"\"\"\n",
    "        if not self.search_logs:\n",
    "            return {}\n",
    "            \n",
    "        df = pd.DataFrame(self.search_logs)\n",
    "        \n",
    "        analytics = {\n",
    "            'total_searches': len(df),\n",
    "            'avg_response_time': df['response_time'].mean(),\n",
    "            'avg_results_per_search': df['num_results'].mean(),\n",
    "            'avg_similarity_score': df['avg_similarity'].mean(),\n",
    "            'searches_with_no_results': (df['num_results'] == 0).sum(),\n",
    "            'unique_queries': df['query'].nunique(),\n",
    "            'top_queries': df['query'].value_counts().head(10).to_dict()\n",
    "        }\n",
    "        \n",
    "        return analytics\n",
    "    \n",
    "    def plot_search_metrics(self):\n",
    "        \"\"\"Create visualization of search metrics.\"\"\"\n",
    "        if not self.search_logs:\n",
    "            print(\"No search logs available\")\n",
    "            return\n",
    "            \n",
    "        df = pd.DataFrame(self.search_logs)\n",
    "        \n",
    "        # Create subplots\n",
    "        from plotly.subplots import make_subplots\n",
    "        \n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('Searches Over Time', 'Response Time Distribution',\n",
    "                          'Results per Search', 'Similarity Score Distribution')\n",
    "        )\n",
    "        \n",
    "        # Searches over time\n",
    "        df['hour'] = df['timestamp'].dt.floor('H')\n",
    "        searches_by_hour = df.groupby('hour').size().reset_index(name='count')\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=searches_by_hour['hour'], y=searches_by_hour['count'],\n",
    "                      mode='lines+markers', name='Searches'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Response time distribution\n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=df['response_time'], name='Response Time', nbinsx=20),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # Results per search\n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=df['num_results'], name='Results Count', nbinsx=15),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Similarity score distribution\n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=df['avg_similarity'], name='Similarity', nbinsx=20),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(height=600, showlegend=False,\n",
    "                         title_text=\"Search System Analytics Dashboard\")\n",
    "        fig.show()\n",
    "\n",
    "# Initialize analytics\n",
    "analytics = SearchAnalytics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Production Deployment Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionSearchSystem:\n",
    "    \"\"\"Production-ready search system with caching, incremental updates, and monitoring.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.search_engine = ConfluenceSemanticSearch(config)\n",
    "        self.analytics = SearchAnalytics()\n",
    "        self.cache = {}\n",
    "        self.last_update = None\n",
    "        \n",
    "    def initialize_or_load(self, index_path: str):\n",
    "        \"\"\"Initialize system by loading existing index or creating new one.\"\"\"\n",
    "        if os.path.exists(f\"{index_path}.chunks\"):\n",
    "            logger.info(\"Loading existing index...\")\n",
    "            self.search_engine.load_index(index_path)\n",
    "            self.last_update = datetime.now()\n",
    "        else:\n",
    "            logger.info(\"Creating new index...\")\n",
    "            self.search_engine.index_confluence_content(limit=1000)\n",
    "            self.search_engine.save_index(index_path)\n",
    "            self.last_update = datetime.now()\n",
    "            \n",
    "    def incremental_update(self, hours_back: int = 24):\n",
    "        \"\"\"Perform incremental update for recently modified pages.\"\"\"\n",
    "        logger.info(f\"Performing incremental update for last {hours_back} hours...\")\n",
    "        \n",
    "        # This would require Confluence API support for querying by modification date\n",
    "        # Implementation would fetch only recently modified pages and update the index\n",
    "        \n",
    "        # Placeholder for incremental update logic\n",
    "        # recent_pages = self.search_engine.extractor.get_recently_modified_pages(hours_back)\n",
    "        # ...\n",
    "        \n",
    "        self.last_update = datetime.now()\n",
    "        \n",
    "    def search_with_cache(self, query: str, **kwargs) -> List[Dict]:\n",
    "        \"\"\"Search with caching for repeated queries.\"\"\"\n",
    "        import time\n",
    "        \n",
    "        # Create cache key\n",
    "        cache_key = hashlib.md5(f\"{query}{kwargs}\".encode()).hexdigest()\n",
    "        \n",
    "        # Check cache\n",
    "        if cache_key in self.cache:\n",
    "            cache_entry = self.cache[cache_key]\n",
    "            if datetime.now() - cache_entry['timestamp'] < timedelta(minutes=30):\n",
    "                logger.info(f\"Returning cached results for query: {query}\")\n",
    "                return cache_entry['results']\n",
    "                \n",
    "        # Perform search\n",
    "        start_time = time.time()\n",
    "        results = self.search_engine.search(query, **kwargs)\n",
    "        response_time = time.time() - start_time\n",
    "        \n",
    "        # Cache results\n",
    "        self.cache[cache_key] = {\n",
    "            'timestamp': datetime.now(),\n",
    "            'results': results\n",
    "        }\n",
    "        \n",
    "        # Log analytics\n",
    "        self.analytics.log_search(query, results, response_time)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_system_health(self) -> Dict:\n",
    "        \"\"\"Get system health metrics.\"\"\"\n",
    "        health = {\n",
    "            'status': 'healthy',\n",
    "            'last_update': self.last_update.isoformat() if self.last_update else None,\n",
    "            'index_size': len(self.search_engine.chunks) if self.search_engine.chunks else 0,\n",
    "            'cache_size': len(self.cache),\n",
    "            'analytics': self.analytics.get_analytics_dashboard()\n",
    "        }\n",
    "        \n",
    "        return health\n",
    "    \n",
    "    def export_search_logs(self, filepath: str):\n",
    "        \"\"\"Export search logs for analysis.\"\"\"\n",
    "        if self.analytics.search_logs:\n",
    "            df = pd.DataFrame(self.analytics.search_logs)\n",
    "            df.to_csv(filepath, index=False)\n",
    "            logger.info(f\"Exported {len(df)} search logs to {filepath}\")\n",
    "        else:\n",
    "            logger.warning(\"No search logs to export\")\n",
    "\n",
    "# Initialize production system\n",
    "prod_system = ProductionSearchSystem(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. API Wrapper for Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from fastapi import FastAPI, HTTPException, BackgroundTasks\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "\n",
    "# API models\n",
    "class SearchRequest(BaseModel):\n",
    "    query: str\n",
    "    k: Optional[int] = 10\n",
    "    filter_space: Optional[str] = None\n",
    "    rerank: Optional[bool] = False\n",
    "\n",
    "class SearchResponse(BaseModel):\n",
    "    query: str\n",
    "    results: List[Dict]\n",
    "    response_time: float\n",
    "    timestamp: str\n",
    "\n",
    "# Create FastAPI app\n",
    "app = FastAPI(title=\"Confluence Semantic Search API\")\n",
    "\n",
    "# Global production system instance\n",
    "prod_system = None\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    \"\"\"Initialize the search system on startup.\"\"\"\n",
    "    global prod_system\n",
    "    config = Config()\n",
    "    prod_system = ProductionSearchSystem(config)\n",
    "    prod_system.initialize_or_load('./confluence_index')\n",
    "\n",
    "@app.post(\"/search\", response_model=SearchResponse)\n",
    "async def search(request: SearchRequest):\n",
    "    \"\"\"Perform semantic search on Confluence content.\"\"\"\n",
    "    import time\n",
    "    \n",
    "    if not prod_system:\n",
    "        raise HTTPException(status_code=503, detail=\"Search system not initialized\")\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Perform search\n",
    "        results = prod_system.search_with_cache(\n",
    "            query=request.query,\n",
    "            k=request.k,\n",
    "            filter_space=request.filter_space\n",
    "        )\n",
    "        \n",
    "        # Optionally rerank\n",
    "        if request.rerank:\n",
    "            results = prod_system.search_engine.rerank_results(request.query, results)\n",
    "        \n",
    "        response_time = time.time() - start_time\n",
    "        \n",
    "        return SearchResponse(\n",
    "            query=request.query,\n",
    "            results=results,\n",
    "            response_time=response_time,\n",
    "            timestamp=datetime.now().isoformat()\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Search error: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    \"\"\"Get system health status.\"\"\"\n",
    "    if not prod_system:\n",
    "        return {\"status\": \"initializing\"}\n",
    "    \n",
    "    return prod_system.get_system_health()\n",
    "\n",
    "@app.post(\"/update\")\n",
    "async def update(background_tasks: BackgroundTasks):\n",
    "    \"\"\"Trigger incremental index update.\"\"\"\n",
    "    if not prod_system:\n",
    "        raise HTTPException(status_code=503, detail=\"Search system not initialized\")\n",
    "    \n",
    "    background_tasks.add_task(prod_system.incremental_update)\n",
    "    return {\"message\": \"Update task scheduled\"}\n",
    "\n",
    "@app.get(\"/analytics\")\n",
    "async def analytics():\n",
    "    \"\"\"Get search analytics.\"\"\"\n",
    "    if not prod_system:\n",
    "        raise HTTPException(status_code=503, detail=\"Search system not initialized\")\n",
    "    \n",
    "    return prod_system.analytics.get_analytics_dashboard()\n",
    "\n",
    "# To run the API:\n",
    "# if __name__ == \"__main__\":\n",
    "#     uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Complete Example Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_workflow_example():\n",
    "    \"\"\"\n",
    "    Complete example workflow demonstrating:\n",
    "    1. Configuration setup\n",
    "    2. Content indexing\n",
    "    3. Search operations\n",
    "    4. Analytics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Configure the system\n",
    "    config = Config(\n",
    "        confluence_url=os.getenv('CONFLUENCE_URL'),\n",
    "        confluence_username=os.getenv('CONFLUENCE_USERNAME'),\n",
    "        confluence_api_token=os.getenv('CONFLUENCE_API_TOKEN'),\n",
    "        embedding_model='sentence-transformers',  # Use free local model\n",
    "        vector_store='faiss',  # Use local vector store\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "    \n",
    "    # Step 2: Initialize production system\n",
    "    print(\"Initializing production search system...\")\n",
    "    prod_system = ProductionSearchSystem(config)\n",
    "    \n",
    "    # Step 3: Index content (or load existing index)\n",
    "    index_path = './confluence_search_index'\n",
    "    prod_system.initialize_or_load(index_path)\n",
    "    \n",
    "    # Step 4: Perform various searches\n",
    "    test_queries = [\n",
    "        \"How to deploy to production?\",\n",
    "        \"API authentication methods\",\n",
    "        \"Database backup procedures\",\n",
    "        \"Team onboarding process\",\n",
    "        \"Security best practices\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PERFORMING TEST SEARCHES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\nSearching: {query}\")\n",
    "        results = prod_system.search_with_cache(query, k=3)\n",
    "        \n",
    "        if results:\n",
    "            print(f\"Found {len(results)} results:\")\n",
    "            for i, result in enumerate(results, 1):\n",
    "                print(f\"  {i}. {result['metadata']['title']} (similarity: {result['similarity']:.3f})\")\n",
    "        else:\n",
    "            print(\"  No results found\")\n",
    "    \n",
    "    # Step 5: Display analytics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SEARCH ANALYTICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    analytics_data = prod_system.analytics.get_analytics_dashboard()\n",
    "    for key, value in analytics_data.items():\n",
    "        if key != 'top_queries':\n",
    "            print(f\"{key}: {value}\")\n",
    "    \n",
    "    # Step 6: Visualize metrics\n",
    "    prod_system.analytics.plot_search_metrics()\n",
    "    \n",
    "    # Step 7: Export logs\n",
    "    prod_system.export_search_logs('./search_logs.csv')\n",
    "    \n",
    "    # Step 8: System health check\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SYSTEM HEALTH\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    health = prod_system.get_system_health()\n",
    "    print(f\"Status: {health['status']}\")\n",
    "    print(f\"Index Size: {health['index_size']} chunks\")\n",
    "    print(f\"Cache Size: {health['cache_size']} entries\")\n",
    "    print(f\"Last Update: {health['last_update']}\")\n",
    "    \n",
    "    return prod_system\n",
    "\n",
    "# Run the complete workflow\n",
    "# prod_system = complete_workflow_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Advanced Features and Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedSearchFeatures:\n",
    "    \"\"\"Advanced features for the search system.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def hybrid_search(search_engine, query: str, alpha: float = 0.5) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Hybrid search combining semantic and keyword search.\n",
    "        alpha: weight for semantic search (0-1)\n",
    "        \"\"\"\n",
    "        # Semantic search\n",
    "        semantic_results = search_engine.search(query, k=20)\n",
    "        \n",
    "        # Keyword search (BM25-like scoring)\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        \n",
    "        # Get all texts\n",
    "        texts = [chunk['text'] for chunk in search_engine.chunks]\n",
    "        \n",
    "        # Create TF-IDF vectorizer\n",
    "        vectorizer = TfidfVectorizer(max_features=5000)\n",
    "        tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "        \n",
    "        # Transform query\n",
    "        query_vector = vectorizer.transform([query])\n",
    "        \n",
    "        # Calculate similarities\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        keyword_similarities = cosine_similarity(query_vector, tfidf_matrix)[0]\n",
    "        \n",
    "        # Get top keyword results\n",
    "        top_keyword_indices = keyword_similarities.argsort()[-20:][::-1]\n",
    "        \n",
    "        # Combine scores\n",
    "        combined_scores = {}\n",
    "        \n",
    "        for result in semantic_results:\n",
    "            chunk_id = result['metadata'].get('chunk_index', -1)\n",
    "            if chunk_id >= 0:\n",
    "                semantic_score = result['similarity']\n",
    "                keyword_score = keyword_similarities[chunk_id] if chunk_id < len(keyword_similarities) else 0\n",
    "                combined_scores[chunk_id] = alpha * semantic_score + (1 - alpha) * keyword_score\n",
    "        \n",
    "        # Sort by combined score\n",
    "        sorted_chunks = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top results\n",
    "        results = []\n",
    "        for chunk_id, score in sorted_chunks[:10]:\n",
    "            if chunk_id < len(search_engine.chunks):\n",
    "                result = search_engine.chunks[chunk_id].copy()\n",
    "                result['hybrid_score'] = score\n",
    "                results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    @staticmethod\n",
    "    def query_expansion(query: str, model='sentence-transformers') -> List[str]:\n",
    "        \"\"\"\n",
    "        Expand query with synonyms and related terms.\n",
    "        \"\"\"\n",
    "        expanded_queries = [query]\n",
    "        \n",
    "        # Simple expansion with common patterns\n",
    "        expansions = {\n",
    "            'deploy': ['deployment', 'release', 'push to production'],\n",
    "            'auth': ['authentication', 'authorization', 'login'],\n",
    "            'api': ['API', 'endpoint', 'REST', 'service'],\n",
    "            'database': ['DB', 'SQL', 'data store', 'persistence'],\n",
    "            'error': ['bug', 'issue', 'problem', 'exception'],\n",
    "            'config': ['configuration', 'settings', 'parameters']\n",
    "        }\n",
    "        \n",
    "        # Check for expansion terms\n",
    "        for term, synonyms in expansions.items():\n",
    "            if term.lower() in query.lower():\n",
    "                for synonym in synonyms:\n",
    "                    expanded_queries.append(query.replace(term, synonym))\n",
    "        \n",
    "        return expanded_queries[:5]  # Limit to 5 variations\n",
    "    \n",
    "    @staticmethod\n",
    "    def clustering_results(results: List[Dict], n_clusters: int = 3):\n",
    "        \"\"\"\n",
    "        Cluster search results to identify themes.\n",
    "        \"\"\"\n",
    "        if len(results) < n_clusters:\n",
    "            return results\n",
    "        \n",
    "        from sklearn.cluster import KMeans\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        \n",
    "        # Generate embeddings for clustering\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        texts = [r['text'] for r in results]\n",
    "        embeddings = model.encode(texts)\n",
    "        \n",
    "        # Perform clustering\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        clusters = kmeans.fit_predict(embeddings)\n",
    "        \n",
    "        # Add cluster labels to results\n",
    "        for result, cluster in zip(results, clusters):\n",
    "            result['cluster'] = int(cluster)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Example usage of advanced features\n",
    "advanced = AdvancedSearchFeatures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a comprehensive, production-ready implementation of semantic search for Confluence content with the following features:\n",
    "\n",
    "### Core Capabilities\n",
    "- **Confluence Integration**: Full API integration with pagination and error handling\n",
    "- **Multiple Embedding Models**: Support for OpenAI and Sentence Transformers\n",
    "- **Vector Store Options**: FAISS, ChromaDB, and Pinecone implementations\n",
    "- **Advanced Search**: Semantic search with metadata filtering and reranking\n",
    "\n",
    "### Production Features\n",
    "- **Caching**: Query result caching for improved performance\n",
    "- **Incremental Updates**: Support for updating only changed content\n",
    "- **Monitoring**: Comprehensive analytics and health monitoring\n",
    "- **API Wrapper**: FastAPI integration for REST API access\n",
    "\n",
    "### Advanced Capabilities\n",
    "- **Hybrid Search**: Combining semantic and keyword-based search\n",
    "- **Query Expansion**: Automatic query enhancement with synonyms\n",
    "- **Result Clustering**: Grouping similar results\n",
    "- **Cross-encoder Reranking**: Improved relevance scoring\n",
    "\n",
    "### Next Steps\n",
    "1. Set up environment variables for Confluence and embedding providers\n",
    "2. Choose appropriate vector store based on scale requirements\n",
    "3. Run initial indexing of Confluence content\n",
    "4. Deploy API for integration with other systems\n",
    "5. Monitor analytics and optimize based on usage patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}