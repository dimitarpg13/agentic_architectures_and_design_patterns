{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/dimitarpg13/agentic_architectures_and_design_patterns/blob/main/notebooks/model_evaluation/faithfulness_metric_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Faithfulness Metric Implementation from Scratch\n",
        "\n",
        "This notebook demonstrates how to implement faithfulness metrics for evaluating RAG (Retrieval Augmented Generation) systems **without using MLflow's built-in faithfulness metric**.\n",
        "\n",
        "## What is Faithfulness?\n",
        "\n",
        "Faithfulness measures whether a generated response is grounded in the provided context, without introducing hallucinations or fabricated information.\n",
        "\n",
        "### Implementation Approaches in this Notebook:\n",
        "\n",
        "1. **Keyword-Based Faithfulness**: Measures overlap between context and response keywords\n",
        "2. **NLI-Based Faithfulness**: Uses Natural Language Inference to check entailment\n",
        "3. **Semantic Similarity Faithfulness**: Uses sentence embeddings to measure semantic alignment\n",
        "4. **Claim Decomposition Faithfulness**: Breaks response into claims and verifies each\n",
        "\n",
        "### Faithfulness Score Range:\n",
        "- 0.0 to 1.0 (or scaled to 1-5)\n",
        "- Higher scores indicate better grounding in context\n",
        "- A score of 1.0 means fully faithful with no hallucinations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Installation and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q transformers sentence-transformers torch pandas numpy matplotlib seaborn plotly nltk scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# NLP libraries\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "# Transformers for NLI and embeddings\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "\n",
        "print(\"âœ… All imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Sample Evaluation Data\n",
        "\n",
        "Let's create diverse examples with varying levels of faithfulness to test our implementations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create diverse RAG examples with varying faithfulness levels\n",
        "evaluation_data = [\n",
        "    {\n",
        "        \"id\": 1,\n",
        "        \"question\": \"What is the main function of mitochondria?\",\n",
        "        \"context\": \"Mitochondria are membrane-bound organelles found in the cytoplasm of eukaryotic cells. They are often referred to as the 'powerhouse of the cell' because they generate most of the cell's supply of adenosine triphosphate (ATP), used as a source of chemical energy.\",\n",
        "        \"faithful_response\": \"Mitochondria are the powerhouse of the cell, responsible for generating most of the cell's ATP, which serves as chemical energy.\",\n",
        "        \"partially_faithful\": \"Mitochondria generate ATP and are found in most animal and plant cells. They have a double membrane structure with their own DNA.\",\n",
        "        \"unfaithful_response\": \"Mitochondria are responsible for protein synthesis and DNA replication in cells. They were discovered in 1950.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": 2,\n",
        "        \"question\": \"When was the Eiffel Tower built?\",\n",
        "        \"context\": \"The Eiffel Tower is a wrought-iron lattice tower located in Paris, France. It was constructed from 1887 to 1889 as the centerpiece of the 1889 World's Fair. Gustave Eiffel's company designed and built the tower.\",\n",
        "        \"faithful_response\": \"The Eiffel Tower was constructed between 1887 and 1889. It was built as the centerpiece of the 1889 World's Fair and was designed by Gustave Eiffel's company.\",\n",
        "        \"partially_faithful\": \"The Eiffel Tower was built in Paris around 1889. It stands 324 meters tall and attracts millions of visitors annually.\",\n",
        "        \"unfaithful_response\": \"The Eiffel Tower was built in 1920 by the French government to celebrate the end of World War I. It is 500 meters tall.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": 3,\n",
        "        \"question\": \"How does photosynthesis work?\",\n",
        "        \"context\": \"Photosynthesis is a process used by plants and other organisms to convert light energy into chemical energy. This chemical energy is stored in carbohydrate molecules synthesized from carbon dioxide and water. Chlorophyll, the green pigment in plants, absorbs light for this process.\",\n",
        "        \"faithful_response\": \"Photosynthesis converts light energy into chemical energy stored in carbohydrates. Plants use carbon dioxide and water, with chlorophyll absorbing the light needed for the process.\",\n",
        "        \"partially_faithful\": \"Photosynthesis is how plants make food using sunlight. It occurs primarily in the leaves and produces glucose and oxygen as byproducts.\",\n",
        "        \"unfaithful_response\": \"Photosynthesis is the process where plants absorb nutrients from the soil and convert them into oxygen. It happens mainly at night when temperatures are cooler.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": 4,\n",
        "        \"question\": \"What causes earthquakes?\",\n",
        "        \"context\": \"Earthquakes are caused by the sudden release of energy in the Earth's lithosphere that creates seismic waves. Most earthquakes are caused by geological faults, but they can also be caused by volcanic activity, landslides, or human activities like mining.\",\n",
        "        \"faithful_response\": \"Earthquakes occur due to the sudden release of energy in Earth's lithosphere, creating seismic waves. They are primarily caused by geological faults, though volcanic activity, landslides, and human activities can also trigger them.\",\n",
        "        \"partially_faithful\": \"Earthquakes happen when tectonic plates shift and release energy. The Richter scale measures their intensity, and they can cause tsunamis when occurring underwater.\",\n",
        "        \"unfaithful_response\": \"Earthquakes are caused by changes in atmospheric pressure and solar activity. They typically occur every full moon and can be predicted weeks in advance.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": 5,\n",
        "        \"question\": \"What is machine learning?\",\n",
        "        \"context\": \"Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on developing algorithms that can access data and use it to learn for themselves.\",\n",
        "        \"faithful_response\": \"Machine learning is a branch of AI that allows systems to learn from experience without explicit programming. It develops algorithms that can access and learn from data autonomously.\",\n",
        "        \"partially_faithful\": \"Machine learning is an AI technology that learns patterns from data. Popular frameworks include TensorFlow and PyTorch, and it powers applications like image recognition.\",\n",
        "        \"unfaithful_response\": \"Machine learning is a type of database management system that stores and retrieves information using complex queries. It was invented in 2010.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(evaluation_data)\n",
        "\n",
        "print(f\"ðŸ“Š Created {len(df)} evaluation examples\")\n",
        "print(\"\\nExample structure:\")\n",
        "print(df[['id', 'question']].to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Approach 1: Keyword-Based Faithfulness\n",
        "\n",
        "This approach measures faithfulness based on keyword overlap between the context and response. While simple, it provides a fast baseline metric.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Approach 2: NLI-Based Faithfulness\n",
        "\n",
        "Natural Language Inference (NLI) models can determine if a hypothesis is entailed by a premise. We use this to check if response sentences are entailed by the context.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NLIFaithfulness:\n",
        "    \"\"\"\n",
        "    NLI-based faithfulness scorer using Natural Language Inference.\n",
        "    \n",
        "    Checks if each sentence in the response is entailed by the context.\n",
        "    Uses a pre-trained NLI model (DeBERTa or similar).\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model_name: str = \"microsoft/deberta-base-mnli\"):\n",
        "        \"\"\"\n",
        "        Initialize with an NLI model.\n",
        "        \n",
        "        Args:\n",
        "            model_name: HuggingFace model name for NLI\n",
        "        \"\"\"\n",
        "        print(f\"Loading NLI model: {model_name}...\")\n",
        "        self.nli_pipeline = pipeline(\n",
        "            \"text-classification\", \n",
        "            model=model_name,\n",
        "            top_k=None  # Return all scores\n",
        "        )\n",
        "        print(\"âœ… NLI model loaded!\")\n",
        "        \n",
        "        # Label mappings (model-specific)\n",
        "        self.entailment_label = \"ENTAILMENT\"\n",
        "        self.contradiction_label = \"CONTRADICTION\"\n",
        "        self.neutral_label = \"NEUTRAL\"\n",
        "    \n",
        "    def split_into_sentences(self, text: str) -> List[str]:\n",
        "        \"\"\"Split text into sentences.\"\"\"\n",
        "        sentences = sent_tokenize(text)\n",
        "        # Filter out very short sentences\n",
        "        return [s.strip() for s in sentences if len(s.strip()) > 10]\n",
        "    \n",
        "    def check_entailment(self, premise: str, hypothesis: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Check if premise entails hypothesis.\n",
        "        \n",
        "        Returns:\n",
        "            Dict with entailment probabilities\n",
        "        \"\"\"\n",
        "        # NLI input format: premise [SEP] hypothesis\n",
        "        input_text = f\"{premise} [SEP] {hypothesis}\"\n",
        "        \n",
        "        try:\n",
        "            result = self.nli_pipeline(input_text)\n",
        "            \n",
        "            # Parse results into a dict\n",
        "            scores = {item['label']: item['score'] for item in result[0]}\n",
        "            \n",
        "            return {\n",
        "                'entailment': scores.get(self.entailment_label, scores.get('entailment', 0)),\n",
        "                'contradiction': scores.get(self.contradiction_label, scores.get('contradiction', 0)),\n",
        "                'neutral': scores.get(self.neutral_label, scores.get('neutral', 0))\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: NLI error - {e}\")\n",
        "            return {'entailment': 0.33, 'contradiction': 0.33, 'neutral': 0.33}\n",
        "    \n",
        "    def score(self, context: str, response: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Calculate NLI-based faithfulness score.\n",
        "        \n",
        "        Each sentence in the response is checked for entailment against the context.\n",
        "        \"\"\"\n",
        "        # Split response into sentences\n",
        "        response_sentences = self.split_into_sentences(response)\n",
        "        \n",
        "        if not response_sentences:\n",
        "            return {\n",
        "                'score': 0.5,\n",
        "                'score_1_5': 3.0,\n",
        "                'sentence_scores': [],\n",
        "                'entailment_ratio': 0.5,\n",
        "                'contradiction_ratio': 0.0\n",
        "            }\n",
        "        \n",
        "        sentence_results = []\n",
        "        entailment_count = 0\n",
        "        contradiction_count = 0\n",
        "        \n",
        "        for sentence in response_sentences:\n",
        "            nli_result = self.check_entailment(context, sentence)\n",
        "            \n",
        "            # Determine the classification\n",
        "            max_label = max(nli_result, key=nli_result.get)\n",
        "            \n",
        "            sentence_results.append({\n",
        "                'sentence': sentence[:50] + '...' if len(sentence) > 50 else sentence,\n",
        "                'entailment_prob': nli_result['entailment'],\n",
        "                'contradiction_prob': nli_result['contradiction'],\n",
        "                'classification': max_label\n",
        "            })\n",
        "            \n",
        "            if nli_result['entailment'] > 0.5:\n",
        "                entailment_count += 1\n",
        "            if nli_result['contradiction'] > 0.5:\n",
        "                contradiction_count += 1\n",
        "        \n",
        "        # Calculate overall score\n",
        "        entailment_ratio = entailment_count / len(response_sentences)\n",
        "        contradiction_ratio = contradiction_count / len(response_sentences)\n",
        "        \n",
        "        # Average entailment probability\n",
        "        avg_entailment = np.mean([r['entailment_prob'] for r in sentence_results])\n",
        "        avg_contradiction = np.mean([r['contradiction_prob'] for r in sentence_results])\n",
        "        \n",
        "        # Final score: high entailment is good, contradiction is very bad\n",
        "        final_score = avg_entailment * (1 - avg_contradiction * 1.5)\n",
        "        final_score = max(0, min(1, final_score))\n",
        "        \n",
        "        return {\n",
        "            'score': round(final_score, 4),\n",
        "            'score_1_5': round(1 + final_score * 4, 2),\n",
        "            'avg_entailment': round(avg_entailment, 4),\n",
        "            'avg_contradiction': round(avg_contradiction, 4),\n",
        "            'entailment_ratio': round(entailment_ratio, 4),\n",
        "            'contradiction_ratio': round(contradiction_ratio, 4),\n",
        "            'sentence_scores': sentence_results\n",
        "        }\n",
        "\n",
        "# Initialize NLI scorer\n",
        "print(\"Initializing NLI-based faithfulness scorer...\")\n",
        "nli_scorer = NLIFaithfulness()\n",
        "\n",
        "# Test on first example\n",
        "example = df.iloc[0]\n",
        "print(\"\\nðŸ“ Testing NLI-Based Faithfulness\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nContext: {example['context'][:100]}...\")\n",
        "\n",
        "for resp_type in ['faithful_response', 'partially_faithful', 'unfaithful_response']:\n",
        "    result = nli_scorer.score(example['context'], example[resp_type])\n",
        "    label = resp_type.replace('_', ' ').title()\n",
        "    print(f\"\\n{label}:\")\n",
        "    print(f\"  Response: {example[resp_type][:60]}...\")\n",
        "    print(f\"  Score: {result['score']:.3f} (1-5 scale: {result['score_1_5']})\")\n",
        "    print(f\"  Avg Entailment: {result['avg_entailment']:.3f}\")\n",
        "    print(f\"  Avg Contradiction: {result['avg_contradiction']:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Approach 3: Semantic Similarity Faithfulness\n",
        "\n",
        "Using sentence embeddings to measure semantic similarity between context and response. Each sentence in the response is compared to the most similar sentence in the context.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SemanticSimilarityFaithfulness:\n",
        "    \"\"\"\n",
        "    Semantic similarity-based faithfulness scorer.\n",
        "    \n",
        "    Uses sentence embeddings to measure how semantically similar\n",
        "    the response is to the context.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
        "        \"\"\"\n",
        "        Initialize with a sentence transformer model.\n",
        "        \n",
        "        Args:\n",
        "            model_name: SentenceTransformer model name\n",
        "        \"\"\"\n",
        "        print(f\"Loading sentence embedding model: {model_name}...\")\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        print(\"âœ… Embedding model loaded!\")\n",
        "    \n",
        "    def split_into_sentences(self, text: str) -> List[str]:\n",
        "        \"\"\"Split text into sentences.\"\"\"\n",
        "        sentences = sent_tokenize(text)\n",
        "        return [s.strip() for s in sentences if len(s.strip()) > 10]\n",
        "    \n",
        "    def get_embeddings(self, texts: List[str]) -> np.ndarray:\n",
        "        \"\"\"Get embeddings for a list of texts.\"\"\"\n",
        "        return self.model.encode(texts, convert_to_numpy=True)\n",
        "    \n",
        "    def score(self, context: str, response: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Calculate semantic similarity-based faithfulness score.\n",
        "        \n",
        "        For each sentence in the response, find the maximum similarity\n",
        "        to any sentence in the context.\n",
        "        \"\"\"\n",
        "        # Split into sentences\n",
        "        context_sentences = self.split_into_sentences(context)\n",
        "        response_sentences = self.split_into_sentences(response)\n",
        "        \n",
        "        if not context_sentences or not response_sentences:\n",
        "            return {\n",
        "                'score': 0.5,\n",
        "                'score_1_5': 3.0,\n",
        "                'sentence_similarities': [],\n",
        "                'avg_similarity': 0.5,\n",
        "                'min_similarity': 0.5\n",
        "            }\n",
        "        \n",
        "        # Get embeddings\n",
        "        context_embeddings = self.get_embeddings(context_sentences)\n",
        "        response_embeddings = self.get_embeddings(response_sentences)\n",
        "        \n",
        "        # Also get overall document embeddings\n",
        "        context_full_emb = self.get_embeddings([context])[0]\n",
        "        response_full_emb = self.get_embeddings([response])[0]\n",
        "        \n",
        "        # Calculate document-level similarity\n",
        "        doc_similarity = cosine_similarity(\n",
        "            context_full_emb.reshape(1, -1),\n",
        "            response_full_emb.reshape(1, -1)\n",
        "        )[0][0]\n",
        "        \n",
        "        # Calculate sentence-level similarities\n",
        "        sentence_results = []\n",
        "        max_similarities = []\n",
        "        \n",
        "        for i, resp_emb in enumerate(response_embeddings):\n",
        "            # Find max similarity to any context sentence\n",
        "            similarities = cosine_similarity(\n",
        "                resp_emb.reshape(1, -1),\n",
        "                context_embeddings\n",
        "            )[0]\n",
        "            \n",
        "            max_sim = float(np.max(similarities))\n",
        "            max_idx = int(np.argmax(similarities))\n",
        "            \n",
        "            max_similarities.append(max_sim)\n",
        "            sentence_results.append({\n",
        "                'response_sentence': response_sentences[i][:50] + '...',\n",
        "                'best_match': context_sentences[max_idx][:50] + '...',\n",
        "                'similarity': round(max_sim, 4)\n",
        "            })\n",
        "        \n",
        "        # Calculate aggregate scores\n",
        "        avg_similarity = np.mean(max_similarities)\n",
        "        min_similarity = np.min(max_similarities)\n",
        "        \n",
        "        # Final score combines document and sentence-level similarity\n",
        "        # Penalize low minimum similarity (indicates hallucinated content)\n",
        "        final_score = (doc_similarity * 0.3 + avg_similarity * 0.4 + min_similarity * 0.3)\n",
        "        \n",
        "        # Apply threshold: very low similarities should be penalized more\n",
        "        if min_similarity < 0.3:\n",
        "            final_score *= (0.5 + min_similarity)\n",
        "        \n",
        "        final_score = max(0, min(1, final_score))\n",
        "        \n",
        "        return {\n",
        "            'score': round(final_score, 4),\n",
        "            'score_1_5': round(1 + final_score * 4, 2),\n",
        "            'doc_similarity': round(doc_similarity, 4),\n",
        "            'avg_similarity': round(avg_similarity, 4),\n",
        "            'min_similarity': round(min_similarity, 4),\n",
        "            'sentence_similarities': sentence_results\n",
        "        }\n",
        "\n",
        "# Initialize semantic similarity scorer\n",
        "print(\"Initializing Semantic Similarity faithfulness scorer...\")\n",
        "semantic_scorer = SemanticSimilarityFaithfulness()\n",
        "\n",
        "# Test on first example\n",
        "example = df.iloc[0]\n",
        "print(\"\\nðŸ“ Testing Semantic Similarity Faithfulness\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nContext: {example['context'][:100]}...\")\n",
        "\n",
        "for resp_type in ['faithful_response', 'partially_faithful', 'unfaithful_response']:\n",
        "    result = semantic_scorer.score(example['context'], example[resp_type])\n",
        "    label = resp_type.replace('_', ' ').title()\n",
        "    print(f\"\\n{label}:\")\n",
        "    print(f\"  Response: {example[resp_type][:60]}...\")\n",
        "    print(f\"  Score: {result['score']:.3f} (1-5 scale: {result['score_1_5']})\")\n",
        "    print(f\"  Doc Similarity: {result['doc_similarity']:.3f}\")\n",
        "    print(f\"  Avg Sentence Sim: {result['avg_similarity']:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Ensemble Faithfulness Scorer\n",
        "\n",
        "Combining multiple approaches for a more robust faithfulness score.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EnsembleFaithfulness:\n",
        "    \"\"\"\n",
        "    Ensemble faithfulness scorer combining multiple approaches.\n",
        "    \n",
        "    Combines keyword-based, NLI-based, and semantic similarity approaches\n",
        "    for a more robust faithfulness evaluation.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, \n",
        "                 keyword_scorer: KeywordFaithfulness,\n",
        "                 nli_scorer: NLIFaithfulness,\n",
        "                 semantic_scorer: SemanticSimilarityFaithfulness,\n",
        "                 weights: Dict[str, float] = None):\n",
        "        \"\"\"\n",
        "        Initialize ensemble with individual scorers.\n",
        "        \n",
        "        Args:\n",
        "            keyword_scorer: Keyword-based scorer\n",
        "            nli_scorer: NLI-based scorer  \n",
        "            semantic_scorer: Semantic similarity scorer\n",
        "            weights: Dict of weights for each scorer (must sum to 1)\n",
        "        \"\"\"\n",
        "        self.keyword_scorer = keyword_scorer\n",
        "        self.nli_scorer = nli_scorer\n",
        "        self.semantic_scorer = semantic_scorer\n",
        "        \n",
        "        # Default weights\n",
        "        self.weights = weights or {\n",
        "            'keyword': 0.2,\n",
        "            'nli': 0.4,\n",
        "            'semantic': 0.4\n",
        "        }\n",
        "    \n",
        "    def score(self, context: str, response: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Calculate ensemble faithfulness score.\n",
        "        \"\"\"\n",
        "        # Get individual scores\n",
        "        keyword_result = self.keyword_scorer.score(context, response)\n",
        "        nli_result = self.nli_scorer.score(context, response)\n",
        "        semantic_result = self.semantic_scorer.score(context, response)\n",
        "        \n",
        "        # Calculate weighted ensemble score\n",
        "        ensemble_score = (\n",
        "            keyword_result['score'] * self.weights['keyword'] +\n",
        "            nli_result['score'] * self.weights['nli'] +\n",
        "            semantic_result['score'] * self.weights['semantic']\n",
        "        )\n",
        "        \n",
        "        # Also calculate agreement between methods\n",
        "        scores = [keyword_result['score'], nli_result['score'], semantic_result['score']]\n",
        "        agreement = 1 - np.std(scores)  # Higher agreement = lower std\n",
        "        \n",
        "        return {\n",
        "            'score': round(ensemble_score, 4),\n",
        "            'score_1_5': round(1 + ensemble_score * 4, 2),\n",
        "            'keyword_score': keyword_result['score'],\n",
        "            'nli_score': nli_result['score'],\n",
        "            'semantic_score': semantic_result['score'],\n",
        "            'method_agreement': round(agreement, 4),\n",
        "            'individual_results': {\n",
        "                'keyword': keyword_result,\n",
        "                'nli': nli_result,\n",
        "                'semantic': semantic_result\n",
        "            }\n",
        "        }\n",
        "    \n",
        "    def score_batch(self, contexts: List[str], responses: List[str]) -> List[Dict]:\n",
        "        \"\"\"Score multiple context-response pairs.\"\"\"\n",
        "        return [\n",
        "            self.score(ctx, resp) \n",
        "            for ctx, resp in zip(contexts, responses)\n",
        "        ]\n",
        "\n",
        "# Initialize ensemble scorer\n",
        "ensemble_scorer = EnsembleFaithfulness(\n",
        "    keyword_scorer=keyword_scorer,\n",
        "    nli_scorer=nli_scorer,\n",
        "    semantic_scorer=semantic_scorer\n",
        ")\n",
        "\n",
        "# Test on first example\n",
        "example = df.iloc[0]\n",
        "print(\"ðŸ“ Testing Ensemble Faithfulness Scorer\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nContext: {example['context'][:100]}...\")\n",
        "\n",
        "for resp_type in ['faithful_response', 'partially_faithful', 'unfaithful_response']:\n",
        "    result = ensemble_scorer.score(example['context'], example[resp_type])\n",
        "    label = resp_type.replace('_', ' ').title()\n",
        "    print(f\"\\n{label}:\")\n",
        "    print(f\"  Response: {example[resp_type][:60]}...\")\n",
        "    print(f\"  Ensemble Score: {result['score']:.3f} (1-5 scale: {result['score_1_5']})\")\n",
        "    print(f\"  Keyword: {result['keyword_score']:.3f} | NLI: {result['nli_score']:.3f} | Semantic: {result['semantic_score']:.3f}\")\n",
        "    print(f\"  Method Agreement: {result['method_agreement']:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Comprehensive Evaluation on All Examples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate all examples with all methods\n",
        "print(\"ðŸ”„ Running comprehensive evaluation on all examples...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "results = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    context = row['context']\n",
        "    \n",
        "    for resp_type in ['faithful_response', 'partially_faithful', 'unfaithful_response']:\n",
        "        response = row[resp_type]\n",
        "        \n",
        "        # Get ensemble score (includes all individual scores)\n",
        "        ensemble_result = ensemble_scorer.score(context, response)\n",
        "        \n",
        "        results.append({\n",
        "            'question_id': row['id'],\n",
        "            'question': row['question'],\n",
        "            'response_type': resp_type.replace('_', ' ').title().replace(' Response', ''),\n",
        "            'keyword_score': ensemble_result['keyword_score'],\n",
        "            'nli_score': ensemble_result['nli_score'],\n",
        "            'semantic_score': ensemble_result['semantic_score'],\n",
        "            'ensemble_score': ensemble_result['score'],\n",
        "            'ensemble_1_5': ensemble_result['score_1_5'],\n",
        "            'method_agreement': ensemble_result['method_agreement']\n",
        "        })\n",
        "    \n",
        "    print(f\"  âœ“ Evaluated example {row['id']}\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "print(f\"\\nâœ… Evaluation complete! {len(results_df)} evaluations performed.\")\n",
        "\n",
        "# Display summary\n",
        "print(\"\\nðŸ“Š Summary Statistics by Response Type:\")\n",
        "print(\"=\"*70)\n",
        "summary = results_df.groupby('response_type').agg({\n",
        "    'keyword_score': ['mean', 'std'],\n",
        "    'nli_score': ['mean', 'std'],\n",
        "    'semantic_score': ['mean', 'std'],\n",
        "    'ensemble_score': ['mean', 'std']\n",
        "}).round(3)\n",
        "print(summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive visualizations\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=2,\n",
        "    subplot_titles=(\n",
        "        \"Ensemble Score by Response Type\",\n",
        "        \"Method Comparison\",\n",
        "        \"Score Distribution by Method\",\n",
        "        \"Per-Question Heatmap\"\n",
        "    ),\n",
        "    specs=[\n",
        "        [{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
        "        [{\"type\": \"box\"}, {\"type\": \"heatmap\"}]\n",
        "    ]\n",
        ")\n",
        "\n",
        "colors = {\n",
        "    \"Faithful\": \"#2ecc71\",\n",
        "    \"Partially Faithful\": \"#f39c12\",\n",
        "    \"Unfaithful\": \"#e74c3c\"\n",
        "}\n",
        "\n",
        "# Plot 1: Ensemble Score by Response Type\n",
        "avg_ensemble = results_df.groupby('response_type')['ensemble_score'].mean().reset_index()\n",
        "avg_ensemble = avg_ensemble.sort_values('ensemble_score', ascending=False)\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Bar(\n",
        "        x=avg_ensemble['response_type'],\n",
        "        y=avg_ensemble['ensemble_score'],\n",
        "        marker_color=[colors.get(t, '#3498db') for t in avg_ensemble['response_type']],\n",
        "        text=avg_ensemble['ensemble_score'].round(3),\n",
        "        textposition='outside',\n",
        "        name='Ensemble'\n",
        "    ),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# Plot 2: Method Comparison (grouped bar)\n",
        "methods = ['keyword_score', 'nli_score', 'semantic_score']\n",
        "method_names = ['Keyword', 'NLI', 'Semantic']\n",
        "method_colors = ['#3498db', '#9b59b6', '#1abc9c']\n",
        "\n",
        "for resp_type in ['Faithful', 'Partially Faithful', 'Unfaithful']:\n",
        "    type_data = results_df[results_df['response_type'] == resp_type]\n",
        "    avg_scores = [type_data[m].mean() for m in methods]\n",
        "    \n",
        "    fig.add_trace(\n",
        "        go.Bar(\n",
        "            name=resp_type,\n",
        "            x=method_names,\n",
        "            y=avg_scores,\n",
        "            marker_color=colors[resp_type]\n",
        "        ),\n",
        "        row=1, col=2\n",
        "    )\n",
        "\n",
        "# Plot 3: Score Distribution (Box Plot)\n",
        "for method, name, color in zip(methods, method_names, method_colors):\n",
        "    fig.add_trace(\n",
        "        go.Box(\n",
        "            y=results_df[method],\n",
        "            name=name,\n",
        "            marker_color=color\n",
        "        ),\n",
        "        row=2, col=1\n",
        "    )\n",
        "\n",
        "# Plot 4: Heatmap\n",
        "pivot_data = results_df.pivot(\n",
        "    index='response_type', \n",
        "    columns='question_id', \n",
        "    values='ensemble_score'\n",
        ")\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Heatmap(\n",
        "        z=pivot_data.values,\n",
        "        x=[f\"Q{i}\" for i in pivot_data.columns],\n",
        "        y=pivot_data.index,\n",
        "        colorscale='RdYlGn',\n",
        "        text=np.round(pivot_data.values, 2),\n",
        "        texttemplate=\"%{text}\",\n",
        "        textfont={\"size\": 10}\n",
        "    ),\n",
        "    row=2, col=2\n",
        ")\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(\n",
        "    title_text=\"Faithfulness Metric Analysis (Custom Implementation)\",\n",
        "    showlegend=True,\n",
        "    height=800,\n",
        "    width=1200,\n",
        "    barmode='group'\n",
        ")\n",
        "\n",
        "fig.update_yaxes(title_text=\"Score (0-1)\", row=1, col=1)\n",
        "fig.update_yaxes(title_text=\"Score (0-1)\", row=1, col=2)\n",
        "fig.update_yaxes(title_text=\"Score (0-1)\", row=2, col=1)\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Radar chart comparing methods\n",
        "fig_radar = go.Figure()\n",
        "\n",
        "categories = ['Keyword\\nScore', 'NLI\\nScore', 'Semantic\\nScore', 'Ensemble\\nScore', 'Method\\nAgreement']\n",
        "\n",
        "for resp_type in ['Faithful', 'Partially Faithful', 'Unfaithful']:\n",
        "    type_data = results_df[results_df['response_type'] == resp_type]\n",
        "    \n",
        "    values = [\n",
        "        type_data['keyword_score'].mean(),\n",
        "        type_data['nli_score'].mean(),\n",
        "        type_data['semantic_score'].mean(),\n",
        "        type_data['ensemble_score'].mean(),\n",
        "        type_data['method_agreement'].mean()\n",
        "    ]\n",
        "    \n",
        "    fig_radar.add_trace(go.Scatterpolar(\n",
        "        r=values,\n",
        "        theta=categories,\n",
        "        fill='toself',\n",
        "        name=resp_type,\n",
        "        marker_color=colors[resp_type]\n",
        "    ))\n",
        "\n",
        "fig_radar.update_layout(\n",
        "    polar=dict(\n",
        "        radialaxis=dict(\n",
        "            visible=True,\n",
        "            range=[0, 1]\n",
        "        )\n",
        "    ),\n",
        "    showlegend=True,\n",
        "    title=\"Faithfulness Dimensions by Response Type\",\n",
        "    height=500,\n",
        "    width=700\n",
        ")\n",
        "\n",
        "fig_radar.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Results Comparison Table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a comprehensive comparison table\n",
        "print(\"ðŸ“Š Detailed Results Comparison\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "# Average scores by response type\n",
        "comparison_table = results_df.groupby('response_type').agg({\n",
        "    'keyword_score': 'mean',\n",
        "    'nli_score': 'mean',\n",
        "    'semantic_score': 'mean',\n",
        "    'ensemble_score': 'mean',\n",
        "    'ensemble_1_5': 'mean'\n",
        "}).round(3)\n",
        "\n",
        "comparison_table.columns = ['Keyword', 'NLI', 'Semantic', 'Ensemble (0-1)', 'Ensemble (1-5)']\n",
        "comparison_table = comparison_table.sort_values('Ensemble (0-1)', ascending=False)\n",
        "\n",
        "print(\"\\nAverage Scores by Response Type:\")\n",
        "print(comparison_table.to_string())\n",
        "\n",
        "# Visual bar representation\n",
        "print(\"\\n\\nðŸ“ˆ Visual Score Comparison:\")\n",
        "print(\"=\"*90)\n",
        "for resp_type in comparison_table.index:\n",
        "    score = comparison_table.loc[resp_type, 'Ensemble (0-1)']\n",
        "    bar = 'â–ˆ' * int(score * 40)\n",
        "    print(f\"{resp_type:25s}: {bar:40s} {score:.3f}\")\n",
        "\n",
        "# Method correlation analysis\n",
        "print(\"\\n\\nðŸ”— Method Correlation Analysis:\")\n",
        "print(\"=\"*90)\n",
        "correlation = results_df[['keyword_score', 'nli_score', 'semantic_score', 'ensemble_score']].corr().round(3)\n",
        "correlation.columns = ['Keyword', 'NLI', 'Semantic', 'Ensemble']\n",
        "correlation.index = ['Keyword', 'NLI', 'Semantic', 'Ensemble']\n",
        "print(correlation.to_string())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Hallucination Detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HallucinationDetector:\n",
        "    \"\"\"\n",
        "    Detect potential hallucinations using our faithfulness scorers.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, ensemble_scorer: EnsembleFaithfulness, threshold: float = 0.5):\n",
        "        self.ensemble_scorer = ensemble_scorer\n",
        "        self.threshold = threshold\n",
        "    \n",
        "    def detect(self, context: str, response: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Detect hallucinations in a response.\n",
        "        \"\"\"\n",
        "        result = self.ensemble_scorer.score(context, response)\n",
        "        \n",
        "        # Extract detailed info\n",
        "        keyword_result = result['individual_results']['keyword']\n",
        "        nli_result = result['individual_results']['nli']\n",
        "        \n",
        "        # Identify specific issues\n",
        "        issues = []\n",
        "        \n",
        "        # Check for novel keywords (potential hallucinations)\n",
        "        if keyword_result.get('novel_keywords'):\n",
        "            issues.append(f\"Novel keywords not in context: {keyword_result['novel_keywords'][:5]}\")\n",
        "        \n",
        "        if keyword_result.get('novel_entities'):\n",
        "            issues.append(f\"Novel entities/numbers: {keyword_result['novel_entities']}\")\n",
        "        \n",
        "        # Check NLI contradictions\n",
        "        if nli_result.get('contradiction_ratio', 0) > 0.3:\n",
        "            issues.append(\"High contradiction rate detected in NLI analysis\")\n",
        "        \n",
        "        # Determine risk level\n",
        "        score = result['score']\n",
        "        if score >= 0.7:\n",
        "            risk_level = \"LOW\"\n",
        "            recommendation = \"Response appears faithful. Safe to use.\"\n",
        "        elif score >= 0.5:\n",
        "            risk_level = \"MEDIUM\"\n",
        "            recommendation = \"Some concerns. Review before using.\"\n",
        "        elif score >= 0.3:\n",
        "            risk_level = \"HIGH\"\n",
        "            recommendation = \"Significant hallucination risk. Manual verification required.\"\n",
        "        else:\n",
        "            risk_level = \"CRITICAL\"\n",
        "            recommendation = \"Likely contains hallucinations. Do not use without revision.\"\n",
        "        \n",
        "        return {\n",
        "            'faithfulness_score': result['score'],\n",
        "            'score_1_5': result['score_1_5'],\n",
        "            'risk_level': risk_level,\n",
        "            'recommendation': recommendation,\n",
        "            'issues': issues,\n",
        "            'method_scores': {\n",
        "                'keyword': result['keyword_score'],\n",
        "                'nli': result['nli_score'],\n",
        "                'semantic': result['semantic_score']\n",
        "            }\n",
        "        }\n",
        "\n",
        "# Initialize detector\n",
        "detector = HallucinationDetector(ensemble_scorer)\n",
        "\n",
        "# Test on examples\n",
        "print(\"ðŸ” Hallucination Detection Analysis\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for idx, row in df.head(3).iterrows():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Question {row['id']}: {row['question']}\")\n",
        "    \n",
        "    for resp_type, label in [('faithful_response', 'Faithful'), \n",
        "                              ('unfaithful_response', 'Unfaithful')]:\n",
        "        result = detector.detect(row['context'], row[resp_type])\n",
        "        \n",
        "        print(f\"\\n  {label} Response:\")\n",
        "        print(f\"    Response: {row[resp_type][:50]}...\")\n",
        "        print(f\"    Risk Level: {result['risk_level']}\")\n",
        "        print(f\"    Score: {result['faithfulness_score']:.3f} (1-5: {result['score_1_5']})\")\n",
        "        if result['issues']:\n",
        "            print(f\"    Issues: {result['issues'][0][:60]}...\")\n",
        "        print(f\"    ðŸ’¡ {result['recommendation']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Best Practices and Recommendations\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "1. **Multiple Methods Are Better**:\n",
        "   - Keyword-based: Fast, interpretable, but surface-level\n",
        "   - NLI-based: Captures semantic entailment, detects contradictions\n",
        "   - Semantic similarity: Good for overall alignment, may miss subtle issues\n",
        "   - **Ensemble**: Combines strengths, most robust\n",
        "\n",
        "2. **When to Use Each Method**:\n",
        "   - **Keyword**: Quick sanity check, identifying obvious hallucinations\n",
        "   - **NLI**: Detecting contradictions, factual inconsistencies\n",
        "   - **Semantic**: Overall coherence, topic alignment\n",
        "   - **Ensemble**: Production systems, comprehensive evaluation\n",
        "\n",
        "3. **Limitations**:\n",
        "   - Keyword-based misses paraphrases\n",
        "   - NLI may struggle with long contexts (need chunking)\n",
        "   - Semantic similarity can be fooled by topically similar but factually incorrect content\n",
        "\n",
        "4. **Production Considerations**:\n",
        "   - Cache embeddings for repeated evaluations\n",
        "   - Consider batching for throughput\n",
        "   - Set thresholds based on your risk tolerance\n",
        "   - Log scores for monitoring and improvement\n",
        "\n",
        "### Score Interpretation:\n",
        "- **0.7-1.0**: High faithfulness, safe to use\n",
        "- **0.5-0.7**: Moderate faithfulness, review recommended\n",
        "- **0.3-0.5**: Low faithfulness, verification required\n",
        "- **0.0-0.3**: Likely hallucinated, do not use\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary\n",
        "print(\"âœ… Faithfulness Metric Demo Complete!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nðŸ“‹ Summary of Implementations:\")\n",
        "print(\"   1. KeywordFaithfulness    - Keyword overlap analysis\")\n",
        "print(\"   2. NLIFaithfulness        - Natural Language Inference\")\n",
        "print(\"   3. SemanticSimilarity     - Sentence embedding similarity\")\n",
        "print(\"   4. EnsembleFaithfulness   - Weighted combination\")\n",
        "print(\"   5. HallucinationDetector  - Risk assessment\")\n",
        "\n",
        "print(\"\\nðŸ“Š Average Results by Response Type:\")\n",
        "summary_final = results_df.groupby('response_type')['ensemble_score'].mean().sort_values(ascending=False)\n",
        "for resp_type, score in summary_final.items():\n",
        "    print(f\"   {resp_type:25s}: {score:.3f}\")\n",
        "\n",
        "print(\"\\nðŸ”— Resources:\")\n",
        "print(\"   - NLI Models: https://huggingface.co/models?pipeline_tag=text-classification&sort=trending&search=nli\")\n",
        "print(\"   - Sentence Transformers: https://www.sbert.net/\")\n",
        "print(\"   - Faithfulness in RAG: https://arxiv.org/abs/2307.15992\")\n",
        "print(\"   - RAGAS Framework: https://docs.ragas.io/\")\n",
        "\n",
        "print(\"\\nðŸ’¡ Next Steps:\")\n",
        "print(\"   - Fine-tune NLI model on your domain\")\n",
        "print(\"   - Add custom entity extraction for your use case\")\n",
        "print(\"   - Implement claim-level verification\")\n",
        "print(\"   - Integrate with your RAG pipeline\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KeywordFaithfulness:\n",
        "    \"\"\"\n",
        "    Keyword-based faithfulness scorer.\n",
        "    \n",
        "    Measures faithfulness based on how many keywords in the response\n",
        "    can be found in the context (i.e., are grounded).\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        # Add common words that don't indicate factual content\n",
        "        self.stop_words.update(['also', 'however', 'therefore', 'thus', 'hence'])\n",
        "    \n",
        "    def extract_keywords(self, text: str) -> set:\n",
        "        \"\"\"Extract meaningful keywords from text.\"\"\"\n",
        "        # Tokenize and lowercase\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        \n",
        "        # Remove stopwords and non-alphabetic tokens\n",
        "        keywords = set()\n",
        "        for token in tokens:\n",
        "            if (token.isalpha() and \n",
        "                token not in self.stop_words and \n",
        "                len(token) > 2):\n",
        "                keywords.add(token)\n",
        "        \n",
        "        return keywords\n",
        "    \n",
        "    def extract_entities_and_numbers(self, text: str) -> set:\n",
        "        \"\"\"Extract named entities and numbers (important for factual accuracy).\"\"\"\n",
        "        entities = set()\n",
        "        \n",
        "        # Extract numbers\n",
        "        numbers = re.findall(r'\\b\\d+(?:\\.\\d+)?\\b', text)\n",
        "        entities.update(numbers)\n",
        "        \n",
        "        # Extract potential proper nouns (capitalized words)\n",
        "        words = text.split()\n",
        "        for word in words:\n",
        "            if word[0].isupper() and len(word) > 1:\n",
        "                entities.add(word.lower().strip('.,!?'))\n",
        "        \n",
        "        return entities\n",
        "    \n",
        "    def score(self, context: str, response: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Calculate keyword-based faithfulness score.\n",
        "        \n",
        "        Returns:\n",
        "            Dict with score and detailed breakdown\n",
        "        \"\"\"\n",
        "        # Extract keywords\n",
        "        context_keywords = self.extract_keywords(context)\n",
        "        response_keywords = self.extract_keywords(response)\n",
        "        \n",
        "        # Extract entities/numbers (weighted more heavily)\n",
        "        context_entities = self.extract_entities_and_numbers(context)\n",
        "        response_entities = self.extract_entities_and_numbers(response)\n",
        "        \n",
        "        # Calculate overlap\n",
        "        keyword_overlap = response_keywords.intersection(context_keywords)\n",
        "        entity_overlap = response_entities.intersection(context_entities)\n",
        "        \n",
        "        # Calculate scores\n",
        "        if len(response_keywords) > 0:\n",
        "            keyword_precision = len(keyword_overlap) / len(response_keywords)\n",
        "        else:\n",
        "            keyword_precision = 0.0\n",
        "        \n",
        "        if len(response_entities) > 0:\n",
        "            entity_precision = len(entity_overlap) / len(response_entities)\n",
        "        else:\n",
        "            entity_precision = 1.0  # No entities to verify\n",
        "        \n",
        "        # Novel keywords (potential hallucinations)\n",
        "        novel_keywords = response_keywords - context_keywords\n",
        "        novel_entities = response_entities - context_entities\n",
        "        \n",
        "        # Penalize novel entities more heavily\n",
        "        novelty_penalty = 0.0\n",
        "        if len(response_keywords) > 0:\n",
        "            novelty_penalty = len(novel_keywords) / len(response_keywords) * 0.3\n",
        "        if len(response_entities) > 0:\n",
        "            novelty_penalty += len(novel_entities) / len(response_entities) * 0.7\n",
        "        \n",
        "        # Combined score\n",
        "        combined_score = (keyword_precision * 0.4 + entity_precision * 0.6) * (1 - novelty_penalty * 0.5)\n",
        "        combined_score = max(0, min(1, combined_score))\n",
        "        \n",
        "        return {\n",
        "            'score': round(combined_score, 4),\n",
        "            'score_1_5': round(1 + combined_score * 4, 2),  # Scale to 1-5\n",
        "            'keyword_precision': round(keyword_precision, 4),\n",
        "            'entity_precision': round(entity_precision, 4),\n",
        "            'novelty_penalty': round(novelty_penalty, 4),\n",
        "            'grounded_keywords': list(keyword_overlap)[:10],\n",
        "            'novel_keywords': list(novel_keywords)[:10],\n",
        "            'novel_entities': list(novel_entities)[:5]\n",
        "        }\n",
        "\n",
        "# Initialize the scorer\n",
        "keyword_scorer = KeywordFaithfulness()\n",
        "\n",
        "# Test on first example\n",
        "example = df.iloc[0]\n",
        "print(\"ðŸ“ Testing Keyword-Based Faithfulness\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nContext: {example['context'][:100]}...\")\n",
        "\n",
        "for resp_type in ['faithful_response', 'partially_faithful', 'unfaithful_response']:\n",
        "    result = keyword_scorer.score(example['context'], example[resp_type])\n",
        "    label = resp_type.replace('_', ' ').title()\n",
        "    print(f\"\\n{label}:\")\n",
        "    print(f\"  Response: {example[resp_type][:60]}...\")\n",
        "    print(f\"  Score: {result['score']:.3f} (1-5 scale: {result['score_1_5']})\")\n",
        "    print(f\"  Keyword Precision: {result['keyword_precision']:.3f}\")\n",
        "    print(f\"  Novel Keywords: {result['novel_keywords'][:5]}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
