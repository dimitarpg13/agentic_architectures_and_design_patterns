{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/dimitarpg13/agentic_architectures_and_design_patterns/blob/main/notebooks/model_evaluation/mlflow_faithfulness_metric_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MLflow Faithfulness Metric Demonstration\n",
        "\n",
        "This notebook provides a comprehensive guide to using MLflow's Faithfulness metric for evaluating RAG (Retrieval Augmented Generation) and text generation systems.\n",
        "\n",
        "## What is Faithfulness?\n",
        "\n",
        "Faithfulness is a critical metric for evaluating whether generated text accurately represents the source information without introducing hallucinations. Key characteristics:\n",
        "\n",
        "- **Factual Consistency**: Measures if claims in the output can be verified from the context\n",
        "- **Hallucination Detection**: Identifies fabricated information not present in the source\n",
        "- **LLM-as-Judge**: Uses a language model to assess faithfulness\n",
        "- **RAG-Focused**: Essential for retrieval-augmented generation systems\n",
        "\n",
        "### Faithfulness Score Range:\n",
        "- Typically 1-5 or binary (faithful/unfaithful)\n",
        "- Higher scores indicate better alignment with source context\n",
        "- A score of 5 means fully faithful with no hallucinations\n",
        "- A score of 1 indicates significant deviation from source material\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Installation and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q mlflow>=2.8.0 openai pandas numpy matplotlib seaborn plotly\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import mlflow.metrics\n",
        "from mlflow.metrics import make_metric, MetricValue\n",
        "from mlflow.metrics.genai import faithfulness, EvaluationExample\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "\n",
        "print(f\"MLflow version: {mlflow.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up OpenAI API key for LLM-based evaluation\n",
        "# You can set this via environment variable or directly here\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
        "\n",
        "# Verify API key is set\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    print(\"‚ö†Ô∏è Warning: OPENAI_API_KEY not set. Please set it to use LLM-based faithfulness evaluation.\")\n",
        "    print(\"   You can set it using: os.environ['OPENAI_API_KEY'] = 'your-key'\")\n",
        "else:\n",
        "    print(\"‚úÖ OpenAI API key is configured\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Understanding MLflow's Built-in Faithfulness Metric\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MLflow provides a built-in faithfulness metric using LLM-as-judge\n",
        "# Let's examine the default faithfulness metric\n",
        "\n",
        "# Create the built-in faithfulness metric\n",
        "faithfulness_metric = faithfulness(model=\"openai:/gpt-4o-mini\")\n",
        "\n",
        "print(\"üìã MLflow Faithfulness Metric Details:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Metric Name: {faithfulness_metric.name}\")\n",
        "print(f\"Greater is Better: {faithfulness_metric.greater_is_better}\")\n",
        "print(f\"\\nThis metric evaluates whether the model output is\")\n",
        "print(\"faithful to the provided context without hallucinations.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Creating Custom Faithfulness Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FaithfulnessMetrics:\n",
        "    \"\"\"\n",
        "    Comprehensive faithfulness metric implementations for MLflow.\n",
        "    \"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def create_custom_faithfulness_metric(model=\"openai:/gpt-4o-mini\"):\n",
        "        \"\"\"\n",
        "        Create a custom faithfulness metric with detailed evaluation examples.\n",
        "        \n",
        "        Args:\n",
        "            model: The LLM model to use as judge\n",
        "        \"\"\"\n",
        "        # Define evaluation examples for few-shot prompting\n",
        "        examples = [\n",
        "            EvaluationExample(\n",
        "                input=\"What is the capital of France?\",\n",
        "                output=\"The capital of France is Paris, which is located along the Seine River.\",\n",
        "                context=\"France is a country in Western Europe. Paris is the capital and largest city of France, situated on the Seine River.\",\n",
        "                score=5,\n",
        "                justification=\"The answer is completely faithful to the context. It correctly states Paris as the capital and mentions the Seine River, both of which are present in the context.\"\n",
        "            ),\n",
        "            EvaluationExample(\n",
        "                input=\"What is the population of Paris?\",\n",
        "                output=\"Paris has a population of approximately 2.1 million people in the city proper and over 12 million in the metropolitan area.\",\n",
        "                context=\"Paris is the capital of France. It is known for its art, culture, and architecture.\",\n",
        "                score=1,\n",
        "                justification=\"The output contains population statistics that are not present in the context. This is a hallucination since the context only mentions Paris's cultural aspects, not population data.\"\n",
        "            ),\n",
        "            EvaluationExample(\n",
        "                input=\"What are the main features of Python?\",\n",
        "                output=\"Python is a high-level programming language known for its readability and simplicity.\",\n",
        "                context=\"Python is a high-level, interpreted programming language known for its clear syntax, readability, and emphasis on code simplicity. It supports multiple programming paradigms.\",\n",
        "                score=4,\n",
        "                justification=\"The answer is mostly faithful, capturing the key points about Python being high-level and readable. It slightly paraphrases 'clear syntax' as 'readability' which is acceptable. However, it omits the multi-paradigm aspect mentioned in context.\"\n",
        "            )\n",
        "        ]\n",
        "        \n",
        "        # Create metric with examples\n",
        "        custom_faithfulness = faithfulness(\n",
        "            model=model,\n",
        "            examples=examples\n",
        "        )\n",
        "        \n",
        "        return custom_faithfulness\n",
        "    \n",
        "    @staticmethod\n",
        "    def create_strict_faithfulness_metric(model=\"openai:/gpt-4o-mini\"):\n",
        "        \"\"\"\n",
        "        Create a strict faithfulness metric that penalizes any deviation from context.\n",
        "        \"\"\"\n",
        "        strict_examples = [\n",
        "            EvaluationExample(\n",
        "                input=\"Summarize the company's Q3 results.\",\n",
        "                output=\"The company reported revenue of $5.2 billion in Q3.\",\n",
        "                context=\"In Q3, the company reported total revenue of $5.2 billion, representing a 15% year-over-year increase.\",\n",
        "                score=5,\n",
        "                justification=\"The output only includes information directly stated in the context. No additional claims are made.\"\n",
        "            ),\n",
        "            EvaluationExample(\n",
        "                input=\"What were the Q3 profits?\",\n",
        "                output=\"The company made strong profits of approximately $800 million in Q3.\",\n",
        "                context=\"In Q3, the company reported total revenue of $5.2 billion.\",\n",
        "                score=1,\n",
        "                justification=\"STRICT VIOLATION: The profit figure of $800 million is not mentioned in the context. Only revenue is discussed. This is a fabrication.\"\n",
        "            ),\n",
        "            EvaluationExample(\n",
        "                input=\"Describe the weather conditions.\",\n",
        "                output=\"It was a sunny day with clear skies.\",\n",
        "                context=\"The weather report indicated sunny conditions with temperatures around 75¬∞F.\",\n",
        "                score=3,\n",
        "                justification=\"Partially faithful. 'Sunny' is correct, but 'clear skies' is an inference not explicitly stated. Temperature was omitted.\"\n",
        "            )\n",
        "        ]\n",
        "        \n",
        "        return faithfulness(\n",
        "            model=model,\n",
        "            examples=strict_examples\n",
        "        )\n",
        "\n",
        "# Create metric instances\n",
        "custom_faithfulness = FaithfulnessMetrics.create_custom_faithfulness_metric()\n",
        "strict_faithfulness = FaithfulnessMetrics.create_strict_faithfulness_metric()\n",
        "\n",
        "print(\"‚úÖ Custom faithfulness metrics created successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Preparing Sample RAG Evaluation Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create diverse RAG examples with varying faithfulness levels\n",
        "rag_evaluation_data = [\n",
        "    {\n",
        "        \"question\": \"What is the main function of mitochondria?\",\n",
        "        \"context\": \"Mitochondria are membrane-bound organelles found in the cytoplasm of eukaryotic cells. They are often referred to as the 'powerhouse of the cell' because they generate most of the cell's supply of adenosine triphosphate (ATP), used as a source of chemical energy.\",\n",
        "        \"faithful_response\": \"Mitochondria are the powerhouse of the cell, responsible for generating most of the cell's ATP, which serves as chemical energy.\",\n",
        "        \"partially_faithful_response\": \"Mitochondria generate ATP and are found in most animal and plant cells. They have a double membrane structure.\",\n",
        "        \"unfaithful_response\": \"Mitochondria are responsible for protein synthesis and DNA replication in cells.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"When was the Eiffel Tower built?\",\n",
        "        \"context\": \"The Eiffel Tower is a wrought-iron lattice tower located in Paris, France. It was constructed from 1887 to 1889 as the centerpiece of the 1889 World's Fair. Gustave Eiffel's company designed and built the tower.\",\n",
        "        \"faithful_response\": \"The Eiffel Tower was constructed between 1887 and 1889. It was built as the centerpiece of the 1889 World's Fair and was designed by Gustave Eiffel's company.\",\n",
        "        \"partially_faithful_response\": \"The Eiffel Tower was built in 1889 and stands 324 meters tall, making it one of the tallest structures in Paris.\",\n",
        "        \"unfaithful_response\": \"The Eiffel Tower was built in 1920 by the French government to celebrate the end of World War I.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How does photosynthesis work?\",\n",
        "        \"context\": \"Photosynthesis is a process used by plants and other organisms to convert light energy into chemical energy. This chemical energy is stored in carbohydrate molecules synthesized from carbon dioxide and water. Chlorophyll, the green pigment in plants, absorbs light for this process.\",\n",
        "        \"faithful_response\": \"Photosynthesis converts light energy into chemical energy stored in carbohydrates. Plants use carbon dioxide and water, with chlorophyll absorbing the light needed for the process.\",\n",
        "        \"partially_faithful_response\": \"Photosynthesis is how plants make food using sunlight. It occurs primarily in the leaves and requires temperatures between 50-85¬∞F for optimal efficiency.\",\n",
        "        \"unfaithful_response\": \"Photosynthesis is the process where plants absorb nutrients from the soil and convert them into oxygen. It happens mainly at night.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What are the symptoms of vitamin D deficiency?\",\n",
        "        \"context\": \"Vitamin D deficiency can cause bone pain and muscle weakness. In severe cases, it can lead to osteomalacia in adults, characterized by soft bones. Risk factors include limited sun exposure and darker skin pigmentation.\",\n",
        "        \"faithful_response\": \"Vitamin D deficiency can cause bone pain and muscle weakness. Severe deficiency may lead to osteomalacia, a condition where bones become soft.\",\n",
        "        \"partially_faithful_response\": \"Vitamin D deficiency causes bone pain, muscle weakness, fatigue, depression, and increased susceptibility to infections.\",\n",
        "        \"unfaithful_response\": \"Vitamin D deficiency primarily causes skin rashes, hair loss, and vision problems. It can be diagnosed through a simple urine test.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is machine learning?\",\n",
        "        \"context\": \"Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on developing algorithms that can access data and use it to learn for themselves.\",\n",
        "        \"faithful_response\": \"Machine learning is a branch of AI that allows systems to learn from experience without explicit programming. It develops algorithms that can access and learn from data autonomously.\",\n",
        "        \"partially_faithful_response\": \"Machine learning is an AI technology that learns from data. It was invented at Stanford in 1980 and is used primarily for image recognition.\",\n",
        "        \"unfaithful_response\": \"Machine learning is a type of database management system that stores and retrieves information using neural networks.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_rag = pd.DataFrame(rag_evaluation_data)\n",
        "\n",
        "print(f\"üìä Created {len(df_rag)} RAG evaluation examples\")\n",
        "print(\"\\nExample data:\")\n",
        "for idx, row in df_rag.head(2).iterrows():\n",
        "    print(f\"\\n{idx+1}. Question: {row['question']}\")\n",
        "    print(f\"   Context: {row['context'][:100]}...\")\n",
        "    print(f\"   Faithful: {row['faithful_response'][:80]}...\")\n",
        "    print(f\"   Unfaithful: {row['unfaithful_response'][:80]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluating RAG Systems with MLflow Faithfulness\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up MLflow experiment\n",
        "mlflow.set_experiment(\"faithfulness-metrics-demo\")\n",
        "\n",
        "def evaluate_rag_model(model_name, eval_data, model_config=None):\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation of a RAG model using faithfulness metrics.\n",
        "    \n",
        "    Args:\n",
        "        model_name: Name of the model being evaluated\n",
        "        eval_data: DataFrame with columns: inputs, outputs, context\n",
        "        model_config: Optional configuration parameters\n",
        "    \"\"\"\n",
        "    with mlflow.start_run(run_name=model_name):\n",
        "        # Log model configuration\n",
        "        mlflow.log_param(\"model_name\", model_name)\n",
        "        mlflow.log_param(\"num_samples\", len(eval_data))\n",
        "        \n",
        "        if model_config:\n",
        "            for key, value in model_config.items():\n",
        "                mlflow.log_param(key, value)\n",
        "        \n",
        "        # Run evaluation with faithfulness metric\n",
        "        results = mlflow.evaluate(\n",
        "            data=eval_data,\n",
        "            targets=\"ground_truth\",\n",
        "            predictions=\"outputs\",\n",
        "            extra_metrics=[faithfulness_metric],\n",
        "            evaluators=\"default\",\n",
        "            evaluator_config={\n",
        "                \"col_mapping\": {\n",
        "                    \"inputs\": \"inputs\",\n",
        "                    \"context\": \"context\"\n",
        "                }\n",
        "            }\n",
        "        )\n",
        "        \n",
        "        return results\n",
        "\n",
        "# Prepare evaluation datasets for different response types\n",
        "def prepare_eval_data(df, response_column):\n",
        "    \"\"\"Prepare evaluation data in the format expected by MLflow.\"\"\"\n",
        "    return pd.DataFrame({\n",
        "        \"inputs\": df[\"question\"],\n",
        "        \"outputs\": df[response_column],\n",
        "        \"context\": df[\"context\"],\n",
        "        \"ground_truth\": df[\"faithful_response\"]  # Using faithful response as ground truth\n",
        "    })\n",
        "\n",
        "# Evaluate each response type\n",
        "response_types = {\n",
        "    \"Faithful-RAG-Model\": {\n",
        "        \"column\": \"faithful_response\",\n",
        "        \"config\": {\"model_type\": \"rag\", \"retriever\": \"dense\", \"temperature\": 0.1}\n",
        "    },\n",
        "    \"Partial-Faithful-Model\": {\n",
        "        \"column\": \"partially_faithful_response\",\n",
        "        \"config\": {\"model_type\": \"rag\", \"retriever\": \"sparse\", \"temperature\": 0.5}\n",
        "    },\n",
        "    \"Unfaithful-Baseline\": {\n",
        "        \"column\": \"unfaithful_response\",\n",
        "        \"config\": {\"model_type\": \"base_llm\", \"retriever\": \"none\", \"temperature\": 0.9}\n",
        "    }\n",
        "}\n",
        "\n",
        "evaluation_results = {}\n",
        "print(\"üîÑ Evaluating RAG models with faithfulness metric...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for model_name, config in response_types.items():\n",
        "    print(f\"\\nüìä Evaluating: {model_name}\")\n",
        "    eval_data = prepare_eval_data(df_rag, config[\"column\"])\n",
        "    \n",
        "    try:\n",
        "        results = evaluate_rag_model(\n",
        "            model_name,\n",
        "            eval_data,\n",
        "            config[\"config\"]\n",
        "        )\n",
        "        evaluation_results[model_name] = results.metrics\n",
        "        print(f\"   ‚úÖ Completed - Faithfulness Score: {results.metrics.get('faithfulness/v1/mean', 'N/A')}\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è Error: {str(e)}\")\n",
        "        evaluation_results[model_name] = {\"error\": str(e)}\n",
        "\n",
        "print(\"\\n‚úÖ Evaluation completed for all models!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Manual Faithfulness Evaluation (Without API)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For demonstration without API access, let's create a rule-based faithfulness scorer\n",
        "\n",
        "class RuleBasedFaithfulnessScorer:\n",
        "    \"\"\"\n",
        "    A rule-based faithfulness scorer for demonstration purposes.\n",
        "    Uses keyword overlap and NLI-inspired heuristics.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.stopwords = set(['the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', \n",
        "                              'been', 'being', 'have', 'has', 'had', 'do', 'does',\n",
        "                              'did', 'will', 'would', 'could', 'should', 'may',\n",
        "                              'might', 'must', 'shall', 'can', 'to', 'of', 'in',\n",
        "                              'for', 'on', 'with', 'at', 'by', 'from', 'as', 'into',\n",
        "                              'through', 'during', 'before', 'after', 'above',\n",
        "                              'below', 'between', 'and', 'or', 'but', 'if', 'then',\n",
        "                              'because', 'it', 'its', 'this', 'that', 'these', 'those'])\n",
        "    \n",
        "    def extract_keywords(self, text):\n",
        "        \"\"\"Extract meaningful keywords from text.\"\"\"\n",
        "        words = text.lower().replace('.', '').replace(',', '').split()\n",
        "        return set(w for w in words if w not in self.stopwords and len(w) > 2)\n",
        "    \n",
        "    def calculate_overlap_score(self, context, response):\n",
        "        \"\"\"Calculate keyword overlap between context and response.\"\"\"\n",
        "        context_keywords = self.extract_keywords(context)\n",
        "        response_keywords = self.extract_keywords(response)\n",
        "        \n",
        "        if not response_keywords:\n",
        "            return 0.0\n",
        "        \n",
        "        overlap = response_keywords.intersection(context_keywords)\n",
        "        \n",
        "        # Precision: what fraction of response keywords are in context\n",
        "        precision = len(overlap) / len(response_keywords)\n",
        "        \n",
        "        return precision\n",
        "    \n",
        "    def calculate_novelty_penalty(self, context, response):\n",
        "        \"\"\"Penalize novel information not in context (potential hallucinations).\"\"\"\n",
        "        context_keywords = self.extract_keywords(context)\n",
        "        response_keywords = self.extract_keywords(response)\n",
        "        \n",
        "        novel_keywords = response_keywords - context_keywords\n",
        "        \n",
        "        if not response_keywords:\n",
        "            return 1.0\n",
        "        \n",
        "        novelty_ratio = len(novel_keywords) / len(response_keywords)\n",
        "        \n",
        "        # Higher novelty = lower faithfulness\n",
        "        return 1.0 - novelty_ratio\n",
        "    \n",
        "    def score(self, context, response):\n",
        "        \"\"\"\n",
        "        Calculate faithfulness score (1-5 scale).\n",
        "        \n",
        "        Returns:\n",
        "            score: Faithfulness score (1-5)\n",
        "            details: Dictionary with component scores\n",
        "        \"\"\"\n",
        "        overlap = self.calculate_overlap_score(context, response)\n",
        "        novelty_penalty = self.calculate_novelty_penalty(context, response)\n",
        "        \n",
        "        # Combined score (weighted average)\n",
        "        combined = (overlap * 0.4 + novelty_penalty * 0.6)\n",
        "        \n",
        "        # Convert to 1-5 scale\n",
        "        scaled_score = 1 + (combined * 4)\n",
        "        \n",
        "        return {\n",
        "            \"score\": round(scaled_score, 2),\n",
        "            \"overlap\": round(overlap, 3),\n",
        "            \"novelty_penalty_score\": round(novelty_penalty, 3),\n",
        "            \"combined_raw\": round(combined, 3)\n",
        "        }\n",
        "\n",
        "# Create scorer instance\n",
        "rule_scorer = RuleBasedFaithfulnessScorer()\n",
        "\n",
        "# Demonstrate on sample data\n",
        "print(\"üìä Rule-Based Faithfulness Scoring (Demonstration)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for idx, row in df_rag.iterrows():\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Question {idx+1}: {row['question']}\")\n",
        "    print(f\"Context: {row['context'][:100]}...\")\n",
        "    \n",
        "    # Score each response type\n",
        "    responses = [\n",
        "        (\"Faithful\", row['faithful_response']),\n",
        "        (\"Partially Faithful\", row['partially_faithful_response']),\n",
        "        (\"Unfaithful\", row['unfaithful_response'])\n",
        "    ]\n",
        "    \n",
        "    for resp_type, response in responses:\n",
        "        result = rule_scorer.score(row['context'], response)\n",
        "        print(f\"\\n  {resp_type}:\")\n",
        "        print(f\"    Response: {response[:60]}...\")\n",
        "        print(f\"    Score: {result['score']}/5 (Overlap: {result['overlap']}, Novelty: {result['novelty_penalty_score']})\")\n",
        "    \n",
        "    if idx >= 1:  # Show only first 2 examples\n",
        "        print(\"\\n... (showing first 2 examples)\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Comprehensive Results Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive visualizations\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=2,\n",
        "    subplot_titles=(\n",
        "        \"Faithfulness Score by Response Type\",\n",
        "        \"Score Distribution\",\n",
        "        \"Component Scores Breakdown\",\n",
        "        \"Per-Question Faithfulness Heatmap\"\n",
        "    ),\n",
        "    specs=[\n",
        "        [{\"type\": \"bar\"}, {\"type\": \"box\"}],\n",
        "        [{\"type\": \"bar\"}, {\"type\": \"heatmap\"}]\n",
        "    ]\n",
        ")\n",
        "\n",
        "colors = {\n",
        "    \"Faithful\": \"#2ecc71\",\n",
        "    \"Partially Faithful\": \"#f39c12\",\n",
        "    \"Unfaithful\": \"#e74c3c\"\n",
        "}\n",
        "\n",
        "# Plot 1: Average Faithfulness Score by Response Type\n",
        "avg_by_type = results_df.groupby('response_type')['faithfulness_score'].mean().reset_index()\n",
        "avg_by_type = avg_by_type.sort_values('faithfulness_score', ascending=False)\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Bar(\n",
        "        x=avg_by_type['response_type'],\n",
        "        y=avg_by_type['faithfulness_score'],\n",
        "        marker_color=[colors.get(t, '#3498db') for t in avg_by_type['response_type']],\n",
        "        text=avg_by_type['faithfulness_score'].round(2),\n",
        "        textposition='outside'\n",
        "    ),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# Plot 2: Score Distribution (Box Plot)\n",
        "for resp_type in ['Faithful', 'Partially Faithful', 'Unfaithful']:\n",
        "    data = results_df[results_df['response_type'] == resp_type]['faithfulness_score']\n",
        "    fig.add_trace(\n",
        "        go.Box(\n",
        "            y=data,\n",
        "            name=resp_type,\n",
        "            marker_color=colors[resp_type]\n",
        "        ),\n",
        "        row=1, col=2\n",
        "    )\n",
        "\n",
        "# Plot 3: Component Scores (Overlap and Novelty)\n",
        "component_data = results_df.groupby('response_type')[['overlap', 'novelty_score']].mean().reset_index()\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Bar(\n",
        "        name='Overlap Score',\n",
        "        x=component_data['response_type'],\n",
        "        y=component_data['overlap'],\n",
        "        marker_color='#3498db'\n",
        "    ),\n",
        "    row=2, col=1\n",
        ")\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Bar(\n",
        "        name='Novelty Score',\n",
        "        x=component_data['response_type'],\n",
        "        y=component_data['novelty_score'],\n",
        "        marker_color='#9b59b6'\n",
        "    ),\n",
        "    row=2, col=1\n",
        ")\n",
        "\n",
        "# Plot 4: Heatmap of per-question scores\n",
        "pivot_data = results_df.pivot(index='response_type', columns='question_id', values='faithfulness_score')\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Heatmap(\n",
        "        z=pivot_data.values,\n",
        "        x=[f\"Q{i}\" for i in pivot_data.columns],\n",
        "        y=pivot_data.index,\n",
        "        colorscale='RdYlGn',\n",
        "        text=np.round(pivot_data.values, 2),\n",
        "        texttemplate=\"%{text}\",\n",
        "        textfont={\"size\": 10}\n",
        "    ),\n",
        "    row=2, col=2\n",
        ")\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(\n",
        "    title_text=\"Comprehensive Faithfulness Metric Analysis\",\n",
        "    showlegend=True,\n",
        "    height=800,\n",
        "    width=1200,\n",
        "    barmode='group'\n",
        ")\n",
        "\n",
        "fig.update_yaxes(title_text=\"Score (1-5)\", row=1, col=1)\n",
        "fig.update_yaxes(title_text=\"Score (1-5)\", row=1, col=2)\n",
        "fig.update_yaxes(title_text=\"Score (0-1)\", row=2, col=1)\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a detailed radar chart for comparison\n",
        "fig_radar = go.Figure()\n",
        "\n",
        "categories = ['Faithfulness\\nScore', 'Keyword\\nOverlap', 'No\\nHallucination', 'Factual\\nConsistency']\n",
        "\n",
        "# Calculate scores for radar chart\n",
        "for resp_type in ['Faithful', 'Partially Faithful', 'Unfaithful']:\n",
        "    type_data = results_df[results_df['response_type'] == resp_type]\n",
        "    \n",
        "    # Normalize faithfulness to 0-1 scale\n",
        "    faith_norm = type_data['faithfulness_score'].mean() / 5\n",
        "    overlap = type_data['overlap'].mean()\n",
        "    novelty = type_data['novelty_score'].mean()\n",
        "    factual = (faith_norm + overlap) / 2  # Combined metric\n",
        "    \n",
        "    fig_radar.add_trace(go.Scatterpolar(\n",
        "        r=[faith_norm, overlap, novelty, factual],\n",
        "        theta=categories,\n",
        "        fill='toself',\n",
        "        name=resp_type,\n",
        "        marker_color=colors[resp_type]\n",
        "    ))\n",
        "\n",
        "fig_radar.update_layout(\n",
        "    polar=dict(\n",
        "        radialaxis=dict(\n",
        "            visible=True,\n",
        "            range=[0, 1]\n",
        "        )\n",
        "    ),\n",
        "    showlegend=True,\n",
        "    title=\"Faithfulness Dimensions Comparison\",\n",
        "    height=500,\n",
        "    width=700\n",
        ")\n",
        "\n",
        "fig_radar.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Production-Ready Faithfulness Evaluation Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FaithfulnessEvaluationPipeline:\n",
        "    \"\"\"\n",
        "    Production-ready faithfulness evaluation pipeline with MLflow integration.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, experiment_name=\"faithfulness-evaluation\", \n",
        "                 tracking_uri=None, use_llm_judge=True):\n",
        "        self.experiment_name = experiment_name\n",
        "        self.use_llm_judge = use_llm_judge\n",
        "        \n",
        "        if tracking_uri:\n",
        "            mlflow.set_tracking_uri(tracking_uri)\n",
        "        mlflow.set_experiment(experiment_name)\n",
        "        \n",
        "        # Initialize scorers\n",
        "        self.rule_scorer = RuleBasedFaithfulnessScorer()\n",
        "        \n",
        "        if use_llm_judge:\n",
        "            self.llm_metric = faithfulness(model=\"openai:/gpt-4o-mini\")\n",
        "    \n",
        "    def evaluate_model(self,\n",
        "                       model_name: str,\n",
        "                       questions: List[str],\n",
        "                       responses: List[str],\n",
        "                       contexts: List[str],\n",
        "                       metadata: Dict = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Evaluate a RAG model with comprehensive faithfulness metrics.\n",
        "        \"\"\"\n",
        "        with mlflow.start_run(run_name=model_name):\n",
        "            # Log metadata\n",
        "            mlflow.log_param(\"model_name\", model_name)\n",
        "            mlflow.log_param(\"num_samples\", len(questions))\n",
        "            mlflow.log_param(\"use_llm_judge\", self.use_llm_judge)\n",
        "            \n",
        "            if metadata:\n",
        "                for key, value in metadata.items():\n",
        "                    mlflow.log_param(key, value)\n",
        "            \n",
        "            # Calculate rule-based scores\n",
        "            rule_results = self._calculate_rule_based_scores(\n",
        "                questions, responses, contexts\n",
        "            )\n",
        "            \n",
        "            # Log metrics\n",
        "            for metric_name, value in rule_results['aggregate'].items():\n",
        "                mlflow.log_metric(f\"rule_{metric_name}\", value)\n",
        "            \n",
        "            # Log artifacts\n",
        "            self._log_evaluation_artifacts(\n",
        "                questions, responses, contexts, rule_results\n",
        "            )\n",
        "            \n",
        "            return rule_results\n",
        "    \n",
        "    def _calculate_rule_based_scores(self,\n",
        "                                     questions: List[str],\n",
        "                                     responses: List[str],\n",
        "                                     contexts: List[str]) -> Dict:\n",
        "        \"\"\"Calculate rule-based faithfulness scores.\"\"\"\n",
        "        results = {\n",
        "            'individual_scores': [],\n",
        "            'aggregate': {}\n",
        "        }\n",
        "        \n",
        "        for q, r, c in zip(questions, responses, contexts):\n",
        "            score_result = self.rule_scorer.score(c, r)\n",
        "            score_result['question'] = q\n",
        "            score_result['response'] = r[:100]\n",
        "            results['individual_scores'].append(score_result)\n",
        "        \n",
        "        # Calculate aggregates\n",
        "        scores = [s['score'] for s in results['individual_scores']]\n",
        "        results['aggregate'] = {\n",
        "            'faithfulness_mean': np.mean(scores),\n",
        "            'faithfulness_std': np.std(scores),\n",
        "            'faithfulness_min': np.min(scores),\n",
        "            'faithfulness_max': np.max(scores),\n",
        "            'overlap_mean': np.mean([s['overlap'] for s in results['individual_scores']]),\n",
        "            'novelty_mean': np.mean([s['novelty_penalty_score'] for s in results['individual_scores']])\n",
        "        }\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def _log_evaluation_artifacts(self,\n",
        "                                 questions: List[str],\n",
        "                                 responses: List[str],\n",
        "                                 contexts: List[str],\n",
        "                                 results: Dict):\n",
        "        \"\"\"Log evaluation artifacts to MLflow.\"\"\"\n",
        "        report = {\n",
        "            'summary': results['aggregate'],\n",
        "            'sample_analysis': results['individual_scores'][:10]  # Log first 10\n",
        "        }\n",
        "        \n",
        "        mlflow.log_dict(report, \"faithfulness_report.json\")\n",
        "    \n",
        "    def compare_models(self,\n",
        "                      models: Dict[str, Tuple[List[str], Dict]],\n",
        "                      questions: List[str],\n",
        "                      contexts: List[str]) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Compare multiple RAG models.\n",
        "        \n",
        "        Args:\n",
        "            models: Dict of model_name -> (responses, metadata)\n",
        "            questions: List of questions\n",
        "            contexts: List of contexts\n",
        "        \"\"\"\n",
        "        comparison_results = []\n",
        "        \n",
        "        for model_name, (responses, metadata) in models.items():\n",
        "            results = self.evaluate_model(\n",
        "                model_name, questions, responses, contexts, metadata\n",
        "            )\n",
        "            \n",
        "            row = {\n",
        "                'Model': model_name,\n",
        "                'Faithfulness': results['aggregate']['faithfulness_mean'],\n",
        "                'Std': results['aggregate']['faithfulness_std'],\n",
        "                'Overlap': results['aggregate']['overlap_mean'],\n",
        "                'Novelty': results['aggregate']['novelty_mean']\n",
        "            }\n",
        "            comparison_results.append(row)\n",
        "        \n",
        "        return pd.DataFrame(comparison_results)\n",
        "\n",
        "# Demonstrate the pipeline\n",
        "print(\"\\nüöÄ Production Pipeline Demonstration\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "pipeline = FaithfulnessEvaluationPipeline(\n",
        "    experiment_name=\"faithfulness-production-pipeline\",\n",
        "    use_llm_judge=False  # Set to True if you have API access\n",
        ")\n",
        "\n",
        "# Prepare models for comparison\n",
        "questions = df_rag['question'].tolist()\n",
        "contexts = df_rag['context'].tolist()\n",
        "\n",
        "models_to_compare = {\n",
        "    \"RAG-GPT4-Dense\": (\n",
        "        df_rag['faithful_response'].tolist(),\n",
        "        {\"architecture\": \"rag\", \"llm\": \"gpt-4\", \"retriever\": \"dense\"}\n",
        "    ),\n",
        "    \"RAG-GPT35-Sparse\": (\n",
        "        df_rag['partially_faithful_response'].tolist(),\n",
        "        {\"architecture\": \"rag\", \"llm\": \"gpt-3.5\", \"retriever\": \"sparse\"}\n",
        "    ),\n",
        "    \"Base-LLM-NoRAG\": (\n",
        "        df_rag['unfaithful_response'].tolist(),\n",
        "        {\"architecture\": \"base\", \"llm\": \"gpt-3.5\", \"retriever\": \"none\"}\n",
        "    )\n",
        "}\n",
        "\n",
        "# Run comparison\n",
        "comparison_results = pipeline.compare_models(\n",
        "    models_to_compare, questions, contexts\n",
        ")\n",
        "\n",
        "print(\"\\nüìä Production Pipeline Results:\")\n",
        "print(\"=\"*70)\n",
        "print(comparison_results.round(3).to_string(index=False))\n",
        "\n",
        "# Identify best model\n",
        "best_model = comparison_results.loc[comparison_results['Faithfulness'].idxmax()]\n",
        "print(f\"\\nüèÜ Best Model: {best_model['Model']}\")\n",
        "print(f\"   - Faithfulness Score: {best_model['Faithfulness']:.3f}/5\")\n",
        "print(f\"   - Keyword Overlap: {best_model['Overlap']:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Detecting Hallucinations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HallucinationDetector:\n",
        "    \"\"\"\n",
        "    Detect potential hallucinations in generated responses.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.scorer = RuleBasedFaithfulnessScorer()\n",
        "        # Common hallucination indicators\n",
        "        self.hallucination_patterns = [\n",
        "            \"studies show\",\n",
        "            \"research indicates\",\n",
        "            \"according to\",\n",
        "            \"statistics reveal\",\n",
        "            \"experts say\",\n",
        "            \"it is estimated\",\n",
        "            \"approximately\",\n",
        "            \"around\",\n",
        "            \"roughly\"\n",
        "        ]\n",
        "    \n",
        "    def detect_hallucinations(self, context, response):\n",
        "        \"\"\"\n",
        "        Analyze response for potential hallucinations.\n",
        "        \n",
        "        Returns:\n",
        "            Dict with hallucination analysis\n",
        "        \"\"\"\n",
        "        # Get faithfulness score\n",
        "        faith_result = self.scorer.score(context, response)\n",
        "        \n",
        "        # Extract novel claims (potential hallucinations)\n",
        "        context_keywords = self.scorer.extract_keywords(context)\n",
        "        response_keywords = self.scorer.extract_keywords(response)\n",
        "        novel_keywords = response_keywords - context_keywords\n",
        "        \n",
        "        # Check for hallucination patterns\n",
        "        response_lower = response.lower()\n",
        "        found_patterns = [\n",
        "            pattern for pattern in self.hallucination_patterns\n",
        "            if pattern in response_lower\n",
        "        ]\n",
        "        \n",
        "        # Calculate hallucination risk score\n",
        "        novelty_risk = len(novel_keywords) / max(len(response_keywords), 1)\n",
        "        pattern_risk = len(found_patterns) * 0.1\n",
        "        faith_risk = (5 - faith_result['score']) / 4\n",
        "        \n",
        "        overall_risk = min((novelty_risk * 0.4 + pattern_risk + faith_risk * 0.4), 1.0)\n",
        "        \n",
        "        return {\n",
        "            'hallucination_risk': round(overall_risk, 3),\n",
        "            'risk_level': self._get_risk_level(overall_risk),\n",
        "            'faithfulness_score': faith_result['score'],\n",
        "            'novel_claims': list(novel_keywords)[:10],\n",
        "            'suspicious_patterns': found_patterns,\n",
        "            'recommendation': self._get_recommendation(overall_risk)\n",
        "        }\n",
        "    \n",
        "    def _get_risk_level(self, risk):\n",
        "        if risk < 0.2:\n",
        "            return \"LOW\"\n",
        "        elif risk < 0.5:\n",
        "            return \"MEDIUM\"\n",
        "        elif risk < 0.7:\n",
        "            return \"HIGH\"\n",
        "        else:\n",
        "            return \"CRITICAL\"\n",
        "    \n",
        "    def _get_recommendation(self, risk):\n",
        "        if risk < 0.2:\n",
        "            return \"Response appears faithful to context. Safe to use.\"\n",
        "        elif risk < 0.5:\n",
        "            return \"Minor concerns. Review for accuracy before use.\"\n",
        "        elif risk < 0.7:\n",
        "            return \"Significant hallucination risk. Manual verification required.\"\n",
        "        else:\n",
        "            return \"High hallucination detected. Do not use without substantial revision.\"\n",
        "\n",
        "# Demonstrate hallucination detection\n",
        "detector = HallucinationDetector()\n",
        "\n",
        "print(\"\\nüîç Hallucination Detection Analysis\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for idx, row in df_rag.head(3).iterrows():\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Question: {row['question']}\")\n",
        "    \n",
        "    for resp_type, response in [('Faithful', row['faithful_response']), \n",
        "                                 ('Unfaithful', row['unfaithful_response'])]:\n",
        "        result = detector.detect_hallucinations(row['context'], response)\n",
        "        \n",
        "        print(f\"\\n  {resp_type} Response:\")\n",
        "        print(f\"    Response: {response[:60]}...\")\n",
        "        print(f\"    Risk Level: {result['risk_level']} ({result['hallucination_risk']:.1%})\")\n",
        "        print(f\"    Faithfulness: {result['faithfulness_score']}/5\")\n",
        "        if result['novel_claims']:\n",
        "            print(f\"    Novel Claims: {', '.join(result['novel_claims'][:5])}\")\n",
        "        if result['suspicious_patterns']:\n",
        "            print(f\"    Suspicious Patterns: {result['suspicious_patterns']}\")\n",
        "        print(f\"    üí° {result['recommendation']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Best Practices and Recommendations\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "1. **Faithfulness Metric Selection**:\n",
        "   - **LLM-as-Judge**: Most accurate but requires API access\n",
        "   - **Rule-based**: Fast and deterministic for initial screening\n",
        "   - **Hybrid**: Combine both for cost-effective evaluation\n",
        "\n",
        "2. **RAG System Evaluation**:\n",
        "   - Always evaluate faithfulness to retrieved context\n",
        "   - Track both precision (relevant info) and hallucination rate\n",
        "   - Consider multiple retrieval strategies\n",
        "\n",
        "3. **Hallucination Detection**:\n",
        "   - Monitor for claims not supported by context\n",
        "   - Watch for statistical claims and citations\n",
        "   - Use multiple detection methods for robustness\n",
        "\n",
        "4. **MLflow Integration Benefits**:\n",
        "   - Track faithfulness across model versions\n",
        "   - Compare RAG configurations systematically\n",
        "   - Log evaluation artifacts for debugging\n",
        "   - Enable A/B testing for production systems\n",
        "\n",
        "5. **Production Tips**:\n",
        "   - Set faithfulness thresholds for automated rejection\n",
        "   - Implement real-time monitoring in production\n",
        "   - Create feedback loops for continuous improvement\n",
        "   - Balance latency vs. evaluation depth\n",
        "\n",
        "### Faithfulness Score Interpretation:\n",
        "- **5 (Excellent)**: Fully faithful, no hallucinations\n",
        "- **4 (Good)**: Minor omissions but no fabrications\n",
        "- **3 (Moderate)**: Some unsupported claims present\n",
        "- **2 (Poor)**: Significant hallucinations detected\n",
        "- **1 (Critical)**: Mostly fabricated content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final MLflow tracking summary\n",
        "print(\"\\nüìà MLflow Tracking Summary:\")\n",
        "print(\"=\"*60)\n",
        "print(\"To view all experiments and metrics in MLflow UI:\")\n",
        "print(\"\\n1. Run in terminal:\")\n",
        "print(\"   mlflow ui --port 5000\")\n",
        "print(\"\\n2. Open browser:\")\n",
        "print(\"   http://localhost:5000\")\n",
        "print(\"\\n3. Navigate to experiments:\")\n",
        "print(\"   - faithfulness-metrics-demo\")\n",
        "print(\"   - faithfulness-production-pipeline\")\n",
        "print(\"\\n‚úÖ Demo completed successfully!\")\n",
        "print(\"\\nüîó Additional Resources:\")\n",
        "print(\"   - MLflow LLM Evaluation: https://mlflow.org/docs/latest/llms/llm-evaluate/index.html\")\n",
        "print(\"   - Faithfulness in RAG: https://arxiv.org/abs/2307.15992\")\n",
        "print(\"   - Hallucination Detection: https://arxiv.org/abs/2311.14648\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate faithfulness scores for all examples using rule-based scorer\n",
        "def evaluate_all_responses(df, scorer):\n",
        "    \"\"\"Evaluate all response types for all questions.\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for idx, row in df.iterrows():\n",
        "        for resp_type in ['faithful_response', 'partially_faithful_response', 'unfaithful_response']:\n",
        "            score_result = scorer.score(row['context'], row[resp_type])\n",
        "            results.append({\n",
        "                'question_id': idx + 1,\n",
        "                'question': row['question'],\n",
        "                'response_type': resp_type.replace('_response', '').replace('_', ' ').title(),\n",
        "                'faithfulness_score': score_result['score'],\n",
        "                'overlap': score_result['overlap'],\n",
        "                'novelty_score': score_result['novelty_penalty_score']\n",
        "            })\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Generate results\n",
        "results_df = evaluate_all_responses(df_rag, rule_scorer)\n",
        "\n",
        "# Create summary statistics\n",
        "summary = results_df.groupby('response_type').agg({\n",
        "    'faithfulness_score': ['mean', 'std', 'min', 'max'],\n",
        "    'overlap': 'mean',\n",
        "    'novelty_score': 'mean'\n",
        "}).round(3)\n",
        "\n",
        "print(\"\\nüìä Faithfulness Score Summary by Response Type:\")\n",
        "print(\"=\"*70)\n",
        "print(summary.to_string())\n",
        "\n",
        "# Display comparison table\n",
        "print(\"\\n\\nüìà Average Scores Comparison:\")\n",
        "print(\"=\"*70)\n",
        "avg_scores = results_df.groupby('response_type')['faithfulness_score'].mean().sort_values(ascending=False)\n",
        "for resp_type, score in avg_scores.items():\n",
        "    bar = '‚ñà' * int(score * 4)\n",
        "    print(f\"{resp_type:25s}: {bar:20s} {score:.2f}/5\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
