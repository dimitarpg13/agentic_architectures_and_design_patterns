{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dimitarpg13/agentic_architectures_and_design_patterns/blob/main/notebooks/model_evaluation/mlflow_rouge_metric_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhKUKph4gUO2"
      },
      "source": [
        "# MLflow ROUGE Metric Demonstration\n",
        "\n",
        "This notebook demonstrates how to use MLflow's ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metric for evaluating text generation and summarization models.\n",
        "\n",
        "## What is ROUGE?\n",
        "\n",
        "ROUGE is a set of metrics commonly used for evaluating automatic summarization and machine translation tasks. It compares an automatically produced summary or translation against reference summaries (typically human-produced).\n",
        "\n",
        "### ROUGE Variants:\n",
        "- **ROUGE-N**: Overlap of n-grams between the system and reference summaries\n",
        "- **ROUGE-L**: Longest Common Subsequence (LCS) based statistics\n",
        "- **ROUGE-W**: Weighted LCS-based statistics\n",
        "- **ROUGE-S**: Skip-bigram based co-occurrence statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVjvmRojgUO3"
      },
      "source": [
        "## 1. Installation and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JV2E3rCxgUO3"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q mlflow rouge-score transformers torch pandas numpy matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDyU8jeYgUO3"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import mlflow.metrics\n",
        "from mlflow.metrics import make_metric, MetricValue\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rouge_score import rouge_scorer\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# For visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "print(f\"MLflow version: {mlflow.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQgMhAn1gUO4"
      },
      "source": [
        "## 2. Creating Custom ROUGE Metrics with MLflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7kWuzV7gUO4"
      },
      "outputs": [],
      "source": [
        "def create_rouge_metric(rouge_type='rouge1', score_type='fmeasure'):\n",
        "    \"\"\"\n",
        "    Creates a custom ROUGE metric for MLflow evaluation.\n",
        "\n",
        "    Args:\n",
        "        rouge_type: Type of ROUGE metric ('rouge1', 'rouge2', 'rougeL', 'rougeLsum')\n",
        "        score_type: Type of score ('precision', 'recall', 'fmeasure')\n",
        "    \"\"\"\n",
        "    def rouge_metric(predictions, targets):\n",
        "        scorer = rouge_scorer.RougeScorer([rouge_type], use_stemmer=True)\n",
        "        scores = []\n",
        "\n",
        "        for pred, target in zip(predictions, targets):\n",
        "            score = scorer.score(target, pred)\n",
        "            scores.append(getattr(score[rouge_type], score_type))\n",
        "\n",
        "        return MetricValue(\n",
        "            aggregate_results={f\"{rouge_type}_{score_type}\": np.mean(scores)},\n",
        "            scores=scores\n",
        "        )\n",
        "\n",
        "    return make_metric(\n",
        "        eval_fn=rouge_metric,\n",
        "        greater_is_better=True,\n",
        "        name=f\"{rouge_type}_{score_type}\"\n",
        "    )\n",
        "\n",
        "# Create different ROUGE metrics\n",
        "rouge1_f = create_rouge_metric('rouge1', 'fmeasure')\n",
        "rouge1_precision = create_rouge_metric('rouge1', 'precision')\n",
        "rouge1_recall = create_rouge_metric('rouge1', 'recall')\n",
        "rouge2_f = create_rouge_metric('rouge2', 'fmeasure')\n",
        "rougeL_f = create_rouge_metric('rougeL', 'fmeasure')\n",
        "rougeLsum_f = create_rouge_metric('rougeLsum', 'fmeasure')\n",
        "\n",
        "print(\"Custom ROUGE metrics created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5mo4HbcgUO4"
      },
      "source": [
        "## 3. Sample Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdLDn2srgUO4"
      },
      "outputs": [],
      "source": [
        "# Create sample summarization data\n",
        "sample_data = [\n",
        "    {\n",
        "        \"original\": \"\"\"Machine learning is a subset of artificial intelligence that enables\n",
        "        computers to learn from data without being explicitly programmed. It uses algorithms\n",
        "        that iteratively learn from data to improve their accuracy. Deep learning is a subset\n",
        "        of machine learning that uses neural networks with multiple layers.\"\"\",\n",
        "\n",
        "        \"reference_summary\": \"Machine learning allows computers to learn from data using algorithms. Deep learning uses multi-layer neural networks.\",\n",
        "\n",
        "        \"model_summary_good\": \"Machine learning enables computers to learn from data through algorithms. Deep learning employs neural networks with multiple layers.\",\n",
        "\n",
        "        \"model_summary_poor\": \"Artificial intelligence is about computers. Neural networks exist.\"\n",
        "    },\n",
        "    {\n",
        "        \"original\": \"\"\"Natural language processing (NLP) is a field of AI that focuses on the\n",
        "        interaction between computers and human language. It enables machines to understand,\n",
        "        interpret, and generate human language. Applications include translation, sentiment\n",
        "        analysis, and chatbots.\"\"\",\n",
        "\n",
        "        \"reference_summary\": \"NLP enables computers to understand and generate human language, with applications in translation and sentiment analysis.\",\n",
        "\n",
        "        \"model_summary_good\": \"Natural language processing allows machines to comprehend and produce human language, used in translation and sentiment analysis.\",\n",
        "\n",
        "        \"model_summary_poor\": \"Computers can process text. AI has many uses.\"\n",
        "    },\n",
        "    {\n",
        "        \"original\": \"\"\"Climate change refers to long-term shifts in global temperatures and\n",
        "        weather patterns. While climate variations are natural, human activities have been\n",
        "        the dominant driver since the 1900s, primarily through burning fossil fuels which\n",
        "        produces greenhouse gases.\"\"\",\n",
        "\n",
        "        \"reference_summary\": \"Climate change involves long-term temperature shifts, driven mainly by human fossil fuel use since 1900s.\",\n",
        "\n",
        "        \"model_summary_good\": \"Climate change represents long-term temperature changes, primarily caused by human burning of fossil fuels since the 1900s.\",\n",
        "\n",
        "        \"model_summary_poor\": \"Weather changes over time. Humans impact the environment.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Convert to DataFrame for easier manipulation\n",
        "df = pd.DataFrame(sample_data)\n",
        "print(f\"Created {len(df)} sample summarization examples\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L0J_P5NgUO4"
      },
      "source": [
        "## 4. Evaluating Models with MLflow ROUGE Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRF6uFoqgUO4"
      },
      "outputs": [],
      "source": [
        "# Set up MLflow experiment\n",
        "mlflow.set_experiment(\"rouge-metrics-demo\")\n",
        "\n",
        "def evaluate_summarization_model(model_name, predictions, references, extra_metrics=None):\n",
        "    \"\"\"\n",
        "    Evaluate a summarization model using ROUGE metrics and log to MLflow.\n",
        "    \"\"\"\n",
        "    with mlflow.start_run(run_name=model_name):\n",
        "        # Log model parameters\n",
        "        mlflow.log_param(\"model_name\", model_name)\n",
        "        mlflow.log_param(\"num_samples\", len(predictions))\n",
        "\n",
        "        # Create evaluation dataframe\n",
        "        eval_df = pd.DataFrame({\n",
        "            \"predictions\": predictions,\n",
        "            \"targets\": references\n",
        "        })\n",
        "\n",
        "        # Evaluate with all ROUGE metrics\n",
        "        metrics = [rouge1_f, rouge1_precision, rouge1_recall, rouge2_f, rougeL_f, rougeLsum_f]\n",
        "\n",
        "        results = mlflow.evaluate(\n",
        "            data=eval_df,\n",
        "            targets=\"targets\",\n",
        "            predictions=\"predictions\",\n",
        "            extra_metrics=metrics,\n",
        "            evaluators=\"default\"\n",
        "        )\n",
        "\n",
        "        # Log additional custom metrics if provided\n",
        "        if extra_metrics:\n",
        "            for key, value in extra_metrics.items():\n",
        "                mlflow.log_metric(key, value)\n",
        "\n",
        "        # Extract and return scores\n",
        "        scores = {}\n",
        "        for metric in metrics:\n",
        "            metric_name = metric.name\n",
        "            if metric_name in results.metrics:\n",
        "                scores[metric_name] = results.metrics[metric_name]\n",
        "\n",
        "        return scores, results\n",
        "\n",
        "# Evaluate \"good\" model\n",
        "good_scores, good_results = evaluate_summarization_model(\n",
        "    \"good_summarizer\",\n",
        "    df[\"model_summary_good\"].tolist(),\n",
        "    df[\"reference_summary\"].tolist(),\n",
        "    extra_metrics={\"model_quality\": 0.9}\n",
        ")\n",
        "\n",
        "# Evaluate \"poor\" model\n",
        "poor_scores, poor_results = evaluate_summarization_model(\n",
        "    \"poor_summarizer\",\n",
        "    df[\"model_summary_poor\"].tolist(),\n",
        "    df[\"reference_summary\"].tolist(),\n",
        "    extra_metrics={\"model_quality\": 0.3}\n",
        ")\n",
        "\n",
        "print(\"Evaluation completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDqLN9B8gUO5"
      },
      "source": [
        "## 5. Comparing Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPsgdSHQgUO5"
      },
      "outputs": [],
      "source": [
        "# Create comparison DataFrame\n",
        "comparison_data = {\n",
        "    \"Metric\": list(good_scores.keys()),\n",
        "    \"Good Model\": list(good_scores.values()),\n",
        "    \"Poor Model\": list(poor_scores.values())\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "comparison_df[\"Improvement\"] = comparison_df[\"Good Model\"] - comparison_df[\"Poor Model\"]\n",
        "comparison_df[\"Improvement %\"] = (comparison_df[\"Improvement\"] / comparison_df[\"Poor Model\"] * 100).round(2)\n",
        "\n",
        "print(\"\\nüìä Model Comparison Results:\")\n",
        "print(\"=\"*60)\n",
        "print(comparison_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDP7CdiQgUO5"
      },
      "source": [
        "## 6. Visualizing ROUGE Scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24WrwADdgUO5"
      },
      "outputs": [],
      "source": [
        "# Create visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Plot 1: Bar comparison of all metrics\n",
        "ax1 = axes[0, 0]\n",
        "metrics_plot = comparison_df[[\"Metric\", \"Good Model\", \"Poor Model\"]].set_index(\"Metric\")\n",
        "metrics_plot.plot(kind=\"bar\", ax=ax1, color=[\"#2ecc71\", \"#e74c3c\"])\n",
        "ax1.set_title(\"ROUGE Metrics Comparison\", fontsize=14, fontweight='bold')\n",
        "ax1.set_ylabel(\"Score\", fontsize=12)\n",
        "ax1.set_xlabel(\"Metric\", fontsize=12)\n",
        "ax1.legend(title=\"Model\", loc=\"upper right\")\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, ha='right')\n",
        "\n",
        "# Plot 2: ROUGE-1 breakdown (Precision, Recall, F-measure)\n",
        "ax2 = axes[0, 1]\n",
        "rouge1_metrics = comparison_df[comparison_df[\"Metric\"].str.contains(\"rouge1\")]\n",
        "rouge1_data = rouge1_metrics[[\"Good Model\", \"Poor Model\"]].values.T\n",
        "rouge1_labels = [m.split('_')[1] for m in rouge1_metrics[\"Metric\"]]\n",
        "\n",
        "x = np.arange(len(rouge1_labels))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax2.bar(x - width/2, rouge1_data[0], width, label='Good Model', color='#2ecc71')\n",
        "bars2 = ax2.bar(x + width/2, rouge1_data[1], width, label='Poor Model', color='#e74c3c')\n",
        "\n",
        "ax2.set_title(\"ROUGE-1 Detailed Breakdown\", fontsize=14, fontweight='bold')\n",
        "ax2.set_ylabel(\"Score\", fontsize=12)\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels(rouge1_labels)\n",
        "ax2.legend()\n",
        "ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Plot 3: Improvement percentages\n",
        "ax3 = axes[1, 0]\n",
        "improvements = comparison_df[comparison_df[\"Improvement %\"].notna()]\n",
        "colors = ['#3498db' if x > 0 else '#e74c3c' for x in improvements[\"Improvement %\"]]\n",
        "bars = ax3.bar(range(len(improvements)), improvements[\"Improvement %\"], color=colors)\n",
        "ax3.set_title(\"Improvement of Good Model over Poor Model (%)\", fontsize=14, fontweight='bold')\n",
        "ax3.set_ylabel(\"Improvement (%)\", fontsize=12)\n",
        "ax3.set_xlabel(\"Metric\", fontsize=12)\n",
        "ax3.set_xticks(range(len(improvements)))\n",
        "ax3.set_xticklabels(improvements[\"Metric\"], rotation=45, ha='right')\n",
        "ax3.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "ax3.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, val in zip(bars, improvements[\"Improvement %\"]):\n",
        "    height = bar.get_height()\n",
        "    ax3.text(bar.get_x() + bar.get_width()/2., height + (5 if height > 0 else -15),\n",
        "             f'{val:.1f}%', ha='center', va='bottom' if height > 0 else 'top', fontsize=10)\n",
        "\n",
        "# Plot 4: Sample-wise comparison\n",
        "ax4 = axes[1, 1]\n",
        "sample_scores_good = []\n",
        "sample_scores_poor = []\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "for i in range(len(df)):\n",
        "    good_score = scorer.score(df.iloc[i][\"reference_summary\"], df.iloc[i][\"model_summary_good\"])\n",
        "    poor_score = scorer.score(df.iloc[i][\"reference_summary\"], df.iloc[i][\"model_summary_poor\"])\n",
        "    sample_scores_good.append(good_score['rouge1'].fmeasure)\n",
        "    sample_scores_poor.append(poor_score['rouge1'].fmeasure)\n",
        "\n",
        "x_samples = range(1, len(df) + 1)\n",
        "ax4.plot(x_samples, sample_scores_good, 'o-', color='#2ecc71', label='Good Model', linewidth=2, markersize=8)\n",
        "ax4.plot(x_samples, sample_scores_poor, 's-', color='#e74c3c', label='Poor Model', linewidth=2, markersize=8)\n",
        "ax4.set_title(\"Sample-wise ROUGE-1 F-measure\", fontsize=14, fontweight='bold')\n",
        "ax4.set_xlabel(\"Sample\", fontsize=12)\n",
        "ax4.set_ylabel(\"ROUGE-1 F-measure\", fontsize=12)\n",
        "ax4.set_xticks(x_samples)\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMqW5-KogUO5"
      },
      "source": [
        "## 7. Advanced: Custom Evaluation with Multiple ROUGE Variants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VM81ocrKgUO5"
      },
      "outputs": [],
      "source": [
        "def comprehensive_rouge_evaluation(predictions, references):\n",
        "    \"\"\"\n",
        "    Perform comprehensive ROUGE evaluation with all available metrics.\n",
        "    \"\"\"\n",
        "    # Initialize scorer with all ROUGE types\n",
        "    scorer = rouge_scorer.RougeScorer(\n",
        "        ['rouge1', 'rouge2', 'rouge3', 'rouge4', 'rougeL', 'rougeLsum'],\n",
        "        use_stemmer=True\n",
        "    )\n",
        "\n",
        "    all_scores = []\n",
        "\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        scores = scorer.score(ref, pred)\n",
        "\n",
        "        score_dict = {}\n",
        "        for rouge_type, score in scores.items():\n",
        "            score_dict[f\"{rouge_type}_precision\"] = score.precision\n",
        "            score_dict[f\"{rouge_type}_recall\"] = score.recall\n",
        "            score_dict[f\"{rouge_type}_fmeasure\"] = score.fmeasure\n",
        "\n",
        "        all_scores.append(score_dict)\n",
        "\n",
        "    # Calculate averages\n",
        "    avg_scores = {}\n",
        "    for key in all_scores[0].keys():\n",
        "        avg_scores[key] = np.mean([s[key] for s in all_scores])\n",
        "\n",
        "    return pd.DataFrame(all_scores), avg_scores\n",
        "\n",
        "# Evaluate both models comprehensively\n",
        "good_detailed, good_avg = comprehensive_rouge_evaluation(\n",
        "    df[\"model_summary_good\"].tolist(),\n",
        "    df[\"reference_summary\"].tolist()\n",
        ")\n",
        "\n",
        "poor_detailed, poor_avg = comprehensive_rouge_evaluation(\n",
        "    df[\"model_summary_poor\"].tolist(),\n",
        "    df[\"reference_summary\"].tolist()\n",
        ")\n",
        "\n",
        "# Create comprehensive comparison\n",
        "comprehensive_comparison = pd.DataFrame({\n",
        "    \"Metric\": list(good_avg.keys()),\n",
        "    \"Good Model\": list(good_avg.values()),\n",
        "    \"Poor Model\": list(poor_avg.values())\n",
        "})\n",
        "comprehensive_comparison[\"Difference\"] = comprehensive_comparison[\"Good Model\"] - comprehensive_comparison[\"Poor Model\"]\n",
        "\n",
        "# Display top performing metrics for good model\n",
        "print(\"\\nüèÜ Top 10 Metrics (Good Model):\")\n",
        "print(\"=\"*50)\n",
        "top_metrics = comprehensive_comparison.nlargest(10, \"Good Model\")[[\"Metric\", \"Good Model\"]]\n",
        "for _, row in top_metrics.iterrows():\n",
        "    print(f\"{row['Metric']:30s}: {row['Good Model']:.4f}\")\n",
        "\n",
        "print(\"\\nüìà Largest Improvements:\")\n",
        "print(\"=\"*50)\n",
        "improvements = comprehensive_comparison.nlargest(10, \"Difference\")[[\"Metric\", \"Difference\"]]\n",
        "for _, row in improvements.iterrows():\n",
        "    print(f\"{row['Metric']:30s}: +{row['Difference']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7yL9tWIgUO6"
      },
      "source": [
        "## 8. Batch Evaluation with MLflow Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0hpI3J-gUO6"
      },
      "outputs": [],
      "source": [
        "def batch_evaluate_models(model_configs, test_data):\n",
        "    \"\"\"\n",
        "    Evaluate multiple model configurations and track with MLflow.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for config in model_configs:\n",
        "        with mlflow.start_run(run_name=config['name']):\n",
        "            # Log configuration\n",
        "            mlflow.log_params(config['params'])\n",
        "\n",
        "            # Simulate model predictions (in practice, these would come from your model)\n",
        "            if config['quality'] == 'high':\n",
        "                predictions = test_data['model_summary_good'].tolist()\n",
        "            elif config['quality'] == 'medium':\n",
        "                # Simulate medium quality by mixing good and poor\n",
        "                predictions = [\n",
        "                    good if i % 2 == 0 else poor\n",
        "                    for i, (good, poor) in enumerate(zip(\n",
        "                        test_data['model_summary_good'].tolist(),\n",
        "                        test_data['model_summary_poor'].tolist()\n",
        "                    ))\n",
        "                ]\n",
        "            else:\n",
        "                predictions = test_data['model_summary_poor'].tolist()\n",
        "\n",
        "            references = test_data['reference_summary'].tolist()\n",
        "\n",
        "            # Calculate ROUGE scores\n",
        "            scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "            scores = []\n",
        "            for pred, ref in zip(predictions, references):\n",
        "                score = scorer.score(ref, pred)\n",
        "                scores.append({\n",
        "                    'rouge1_f': score['rouge1'].fmeasure,\n",
        "                    'rouge2_f': score['rouge2'].fmeasure,\n",
        "                    'rougeL_f': score['rougeL'].fmeasure\n",
        "                })\n",
        "\n",
        "            # Calculate and log average scores\n",
        "            avg_scores = {k: np.mean([s[k] for s in scores]) for k in scores[0].keys()}\n",
        "            mlflow.log_metrics(avg_scores)\n",
        "\n",
        "            # Log model artifacts (in practice, you'd save actual model files)\n",
        "            mlflow.log_dict(config, \"config.json\")\n",
        "\n",
        "            # Store results\n",
        "            results.append({\n",
        "                'model': config['name'],\n",
        "                **avg_scores,\n",
        "                **config['params']\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Define model configurations to test\n",
        "model_configs = [\n",
        "    {\n",
        "        'name': 'transformer_base',\n",
        "        'quality': 'high',\n",
        "        'params': {'architecture': 'transformer', 'layers': 6, 'learning_rate': 0.001}\n",
        "    },\n",
        "    {\n",
        "        'name': 'transformer_large',\n",
        "        'quality': 'high',\n",
        "        'params': {'architecture': 'transformer', 'layers': 12, 'learning_rate': 0.0005}\n",
        "    },\n",
        "    {\n",
        "        'name': 'lstm_model',\n",
        "        'quality': 'medium',\n",
        "        'params': {'architecture': 'lstm', 'layers': 2, 'learning_rate': 0.01}\n",
        "    },\n",
        "    {\n",
        "        'name': 'baseline_extractive',\n",
        "        'quality': 'low',\n",
        "        'params': {'architecture': 'extractive', 'layers': 1, 'learning_rate': 0.1}\n",
        "    }\n",
        "]\n",
        "\n",
        "# Run batch evaluation\n",
        "batch_results = batch_evaluate_models(model_configs, df)\n",
        "\n",
        "print(\"\\nüîÑ Batch Evaluation Results:\")\n",
        "print(\"=\"*80)\n",
        "print(batch_results.to_string(index=False))\n",
        "\n",
        "# Find best model\n",
        "best_model_idx = batch_results['rouge1_f'].idxmax()\n",
        "best_model = batch_results.loc[best_model_idx]\n",
        "print(f\"\\nü•á Best Model: {best_model['model']}\")\n",
        "print(f\"   ROUGE-1 F1: {best_model['rouge1_f']:.4f}\")\n",
        "print(f\"   ROUGE-2 F1: {best_model['rouge2_f']:.4f}\")\n",
        "print(f\"   ROUGE-L F1: {best_model['rougeL_f']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-5qkQsDgUO6"
      },
      "source": [
        "## 9. Creating a Reusable ROUGE Evaluation Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19lnJo8EgUO6"
      },
      "outputs": [],
      "source": [
        "class ROUGEEvaluator:\n",
        "    \"\"\"\n",
        "    A reusable ROUGE evaluation pipeline with MLflow integration.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, experiment_name=\"rouge-evaluation\", use_stemmer=True):\n",
        "        self.experiment_name = experiment_name\n",
        "        self.use_stemmer = use_stemmer\n",
        "        self.scorer = rouge_scorer.RougeScorer(\n",
        "            ['rouge1', 'rouge2', 'rougeL', 'rougeLsum'],\n",
        "            use_stemmer=use_stemmer\n",
        "        )\n",
        "        mlflow.set_experiment(experiment_name)\n",
        "\n",
        "    def evaluate_single(self, prediction, reference):\n",
        "        \"\"\"Evaluate a single prediction-reference pair.\"\"\"\n",
        "        return self.scorer.score(reference, prediction)\n",
        "\n",
        "    def evaluate_batch(self, predictions, references, model_name=None, log_to_mlflow=True):\n",
        "        \"\"\"\n",
        "        Evaluate a batch of predictions.\n",
        "\n",
        "        Args:\n",
        "            predictions: List of predicted summaries\n",
        "            references: List of reference summaries\n",
        "            model_name: Name for MLflow logging\n",
        "            log_to_mlflow: Whether to log results to MLflow\n",
        "        \"\"\"\n",
        "        if len(predictions) != len(references):\n",
        "            raise ValueError(\"Predictions and references must have the same length\")\n",
        "\n",
        "        all_scores = []\n",
        "        for pred, ref in zip(predictions, references):\n",
        "            scores = self.evaluate_single(pred, ref)\n",
        "            all_scores.append(scores)\n",
        "\n",
        "        # Calculate aggregate metrics\n",
        "        aggregate_scores = self._aggregate_scores(all_scores)\n",
        "\n",
        "        # Log to MLflow if requested\n",
        "        if log_to_mlflow:\n",
        "            self._log_to_mlflow(aggregate_scores, model_name, len(predictions))\n",
        "\n",
        "        return aggregate_scores, all_scores\n",
        "\n",
        "    def _aggregate_scores(self, all_scores):\n",
        "        \"\"\"Aggregate individual scores into summary statistics.\"\"\"\n",
        "        aggregate = {}\n",
        "\n",
        "        for rouge_type in all_scores[0].keys():\n",
        "            precisions = [s[rouge_type].precision for s in all_scores]\n",
        "            recalls = [s[rouge_type].recall for s in all_scores]\n",
        "            fmeasures = [s[rouge_type].fmeasure for s in all_scores]\n",
        "\n",
        "            aggregate[rouge_type] = {\n",
        "                'precision': {\n",
        "                    'mean': np.mean(precisions),\n",
        "                    'std': np.std(precisions),\n",
        "                    'min': np.min(precisions),\n",
        "                    'max': np.max(precisions)\n",
        "                },\n",
        "                'recall': {\n",
        "                    'mean': np.mean(recalls),\n",
        "                    'std': np.std(recalls),\n",
        "                    'min': np.min(recalls),\n",
        "                    'max': np.max(recalls)\n",
        "                },\n",
        "                'fmeasure': {\n",
        "                    'mean': np.mean(fmeasures),\n",
        "                    'std': np.std(fmeasures),\n",
        "                    'min': np.min(fmeasures),\n",
        "                    'max': np.max(fmeasures)\n",
        "                }\n",
        "            }\n",
        "\n",
        "        return aggregate\n",
        "\n",
        "    def _log_to_mlflow(self, aggregate_scores, model_name, num_samples):\n",
        "        \"\"\"Log evaluation results to MLflow.\"\"\"\n",
        "        with mlflow.start_run(run_name=model_name or \"rouge-evaluation\"):\n",
        "            # Log parameters\n",
        "            mlflow.log_param(\"num_samples\", num_samples)\n",
        "            mlflow.log_param(\"use_stemmer\", self.use_stemmer)\n",
        "            if model_name:\n",
        "                mlflow.log_param(\"model_name\", model_name)\n",
        "\n",
        "            # Log metrics\n",
        "            for rouge_type, scores in aggregate_scores.items():\n",
        "                for metric_type, values in scores.items():\n",
        "                    for stat_name, stat_value in values.items():\n",
        "                        metric_name = f\"{rouge_type}_{metric_type}_{stat_name}\"\n",
        "                        mlflow.log_metric(metric_name, stat_value)\n",
        "\n",
        "    def compare_models(self, model_results):\n",
        "        \"\"\"\n",
        "        Compare multiple model evaluation results.\n",
        "\n",
        "        Args:\n",
        "            model_results: Dict of {model_name: (predictions, references)}\n",
        "        \"\"\"\n",
        "        comparison_data = []\n",
        "\n",
        "        for model_name, (predictions, references) in model_results.items():\n",
        "            aggregate_scores, _ = self.evaluate_batch(\n",
        "                predictions, references, model_name, log_to_mlflow=True\n",
        "            )\n",
        "\n",
        "            # Extract key metrics for comparison\n",
        "            row = {'model': model_name}\n",
        "            for rouge_type in ['rouge1', 'rouge2', 'rougeL']:\n",
        "                row[f\"{rouge_type}_f1\"] = aggregate_scores[rouge_type]['fmeasure']['mean']\n",
        "                row[f\"{rouge_type}_precision\"] = aggregate_scores[rouge_type]['precision']['mean']\n",
        "                row[f\"{rouge_type}_recall\"] = aggregate_scores[rouge_type]['recall']['mean']\n",
        "\n",
        "            comparison_data.append(row)\n",
        "\n",
        "        return pd.DataFrame(comparison_data)\n",
        "\n",
        "# Demonstrate the pipeline\n",
        "evaluator = ROUGEEvaluator(experiment_name=\"rouge-pipeline-demo\")\n",
        "\n",
        "# Prepare model results for comparison\n",
        "model_results = {\n",
        "    \"High-Quality Model\": (df[\"model_summary_good\"].tolist(), df[\"reference_summary\"].tolist()),\n",
        "    \"Low-Quality Model\": (df[\"model_summary_poor\"].tolist(), df[\"reference_summary\"].tolist())\n",
        "}\n",
        "\n",
        "# Compare models\n",
        "comparison = evaluator.compare_models(model_results)\n",
        "\n",
        "print(\"\\nüéØ Pipeline Evaluation Results:\")\n",
        "print(\"=\"*80)\n",
        "print(comparison.round(4).to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhUCUphFgUO7"
      },
      "source": [
        "## 10. Best Practices and Tips\n",
        "\n",
        "### Key Insights from this Demo:\n",
        "\n",
        "1. **ROUGE Metric Selection**:\n",
        "   - **ROUGE-1**: Good for measuring unigram overlap (individual word matches)\n",
        "   - **ROUGE-2**: Captures bigram overlap (phrase-level similarity)\n",
        "   - **ROUGE-L**: Based on longest common subsequence (structural similarity)\n",
        "   - **ROUGE-Lsum**: Better for multi-sentence summaries\n",
        "\n",
        "2. **MLflow Integration Benefits**:\n",
        "   - Automatic experiment tracking\n",
        "   - Easy model comparison\n",
        "   - Reproducible evaluations\n",
        "   - Metric visualization in MLflow UI\n",
        "\n",
        "3. **Evaluation Considerations**:\n",
        "   - Always use multiple ROUGE variants for comprehensive evaluation\n",
        "   - Consider both precision and recall, not just F-measure\n",
        "   - Use stemming for more robust matching\n",
        "   - Evaluate on diverse test sets\n",
        "\n",
        "4. **Production Tips**:\n",
        "   - Create reusable evaluation pipelines\n",
        "   - Log all hyperparameters and configurations\n",
        "   - Track both aggregate and sample-level metrics\n",
        "   - Set up automated evaluation for model updates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Puxr-66gUO7"
      },
      "outputs": [],
      "source": [
        "# View MLflow UI command (run in terminal)\n",
        "print(\"\\nüìä To view the MLflow UI with all logged experiments:\")\n",
        "print(\"Run this command in your terminal:\")\n",
        "print(\"\\n    mlflow ui --port 5000\\n\")\n",
        "print(\"Then open: http://localhost:5000\")\n",
        "print(\"\\n‚úÖ Demo completed successfully!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}