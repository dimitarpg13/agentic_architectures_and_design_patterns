{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic Workflow for Semantic Search through Confluence Pages\n",
    "\n",
    "This notebook implements a production-ready agentic workflow for semantic search through Confluence pages using:\n",
    "- **Confluence API** for page crawling and content extraction\n",
    "- **OpenAI/Cohere embeddings** for semantic representation\n",
    "- **ChromaDB/Pinecone** for vector storage\n",
    "- **LangGraph** for agentic workflow orchestration\n",
    "- **Braintrust** for observability and monitoring\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "Confluence Root URL ‚Üí Crawler ‚Üí Content Processor ‚Üí Embeddings ‚Üí Vector DB\n",
    "                                                                      ‚Üì\n",
    "User Query ‚Üí Agent ‚Üí Tools ‚Üí Semantic Search ‚Üí Response Generation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q atlassian-python-api \\\n",
    "              langchain \\\n",
    "              langchain-openai \\\n",
    "              langchain-cohere \\\n",
    "              langgraph \\\n",
    "              chromadb \\\n",
    "              pinecone-client \\\n",
    "              beautifulsoup4 \\\n",
    "              lxml \\\n",
    "              tiktoken \\\n",
    "              python-dotenv \\\n",
    "              braintrust \\\n",
    "              numpy \\\n",
    "              pandas \\\n",
    "              tqdm \\\n",
    "              tenacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# External imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "from bs4 import BeautifulSoup\n",
    "from atlassian import Confluence\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma, Pinecone\n",
    "from langchain.tools import Tool\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# Observability\n",
    "import braintrust\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration for Confluence semantic search agent\"\"\"\n",
    "    \n",
    "    # Confluence settings\n",
    "    confluence_url: str = os.getenv('CONFLUENCE_URL', 'https://your-domain.atlassian.net')\n",
    "    confluence_username: str = os.getenv('CONFLUENCE_USERNAME', '')\n",
    "    confluence_api_token: str = os.getenv('CONFLUENCE_API_TOKEN', '')\n",
    "    \n",
    "    # Embedding settings\n",
    "    embedding_provider: str = 'openai'  # 'openai' or 'cohere'\n",
    "    openai_api_key: str = os.getenv('OPENAI_API_KEY', '')\n",
    "    cohere_api_key: str = os.getenv('COHERE_API_KEY', '')\n",
    "    embedding_model: str = 'text-embedding-3-small'\n",
    "    \n",
    "    # Vector DB settings\n",
    "    vector_db_provider: str = 'chroma'  # 'chroma' or 'pinecone'\n",
    "    pinecone_api_key: str = os.getenv('PINECONE_API_KEY', '')\n",
    "    pinecone_environment: str = os.getenv('PINECONE_ENVIRONMENT', '')\n",
    "    pinecone_index: str = 'confluence-search'\n",
    "    chroma_persist_dir: str = './chroma_db'\n",
    "    \n",
    "    # LLM settings\n",
    "    llm_model: str = 'gpt-4o-mini'\n",
    "    llm_temperature: float = 0.0\n",
    "    \n",
    "    # Crawling settings\n",
    "    max_depth: int = 3\n",
    "    max_pages: int = 100\n",
    "    batch_size: int = 10\n",
    "    max_workers: int = 5\n",
    "    \n",
    "    # Text processing\n",
    "    chunk_size: int = 1000\n",
    "    chunk_overlap: int = 200\n",
    "    \n",
    "    # Search settings\n",
    "    top_k: int = 5\n",
    "    score_threshold: float = 0.7\n",
    "    \n",
    "    # Observability\n",
    "    braintrust_api_key: str = os.getenv('BRAINTRUST_API_KEY', '')\n",
    "    enable_monitoring: bool = True\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Validate configuration\n",
    "if not config.confluence_username or not config.confluence_api_token:\n",
    "    logger.warning(\"Confluence credentials not configured. Please set CONFLUENCE_USERNAME and CONFLUENCE_API_TOKEN\")\n",
    "if not config.openai_api_key and config.embedding_provider == 'openai':\n",
    "    logger.warning(\"OpenAI API key not configured. Please set OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Confluence Content Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ConfluencePage:\n",
    "    \"\"\"Represents a Confluence page with metadata\"\"\"\n",
    "    id: str\n",
    "    title: str\n",
    "    url: str\n",
    "    content: str\n",
    "    space_key: str\n",
    "    created_by: str\n",
    "    created_date: str\n",
    "    last_modified: str\n",
    "    parent_id: Optional[str] = None\n",
    "    labels: List[str] = field(default_factory=list)\n",
    "    attachments: List[Dict] = field(default_factory=list)\n",
    "    child_pages: List[str] = field(default_factory=list)\n",
    "\n",
    "class ConfluenceCrawler:\n",
    "    \"\"\"Crawls Confluence pages starting from a root URL\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.confluence = Confluence(\n",
    "            url=config.confluence_url,\n",
    "            username=config.confluence_username,\n",
    "            password=config.confluence_api_token,\n",
    "            cloud=True\n",
    "        )\n",
    "        self.visited_pages = set()\n",
    "        self.pages = []\n",
    "        \n",
    "    def extract_page_id_from_url(self, url: str) -> Optional[str]:\n",
    "        \"\"\"Extract page ID from Confluence URL\"\"\"\n",
    "        patterns = [\n",
    "            r'/pages/(\\d+)/',\n",
    "            r'pageId=(\\d+)',\n",
    "            r'/pages/viewpage.action\\?pageId=(\\d+)'\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, url)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "        return None\n",
    "    \n",
    "    def clean_html_content(self, html_content: str) -> str:\n",
    "        \"\"\"Clean HTML content and extract text\"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'lxml')\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for element in soup(['script', 'style', 'meta', 'link']):\n",
    "            element.decompose()\n",
    "        \n",
    "        # Extract text\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        \n",
    "        # Clean up whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "    def fetch_page(self, page_id: str) -> Optional[ConfluencePage]:\n",
    "        \"\"\"Fetch a single Confluence page with retry logic\"\"\"\n",
    "        try:\n",
    "            # Get page details\n",
    "            page = self.confluence.get_page_by_id(\n",
    "                page_id,\n",
    "                expand='body.storage,metadata.labels,children.page,history,version'\n",
    "            )\n",
    "            \n",
    "            # Extract content\n",
    "            content_html = page.get('body', {}).get('storage', {}).get('value', '')\n",
    "            content_text = self.clean_html_content(content_html)\n",
    "            \n",
    "            # Extract metadata\n",
    "            labels = [label['name'] for label in page.get('metadata', {}).get('labels', {}).get('results', [])]\n",
    "            \n",
    "            # Get child pages\n",
    "            child_pages = []\n",
    "            if 'children' in page and 'page' in page['children']:\n",
    "                child_pages = [child['id'] for child in page['children']['page'].get('results', [])]\n",
    "            \n",
    "            # Create page object\n",
    "            confluence_page = ConfluencePage(\n",
    "                id=page['id'],\n",
    "                title=page['title'],\n",
    "                url=f\"{self.config.confluence_url}/wiki/spaces/{page['space']['key']}/pages/{page['id']}\",\n",
    "                content=content_text,\n",
    "                space_key=page['space']['key'],\n",
    "                created_by=page['history']['createdBy']['displayName'],\n",
    "                created_date=page['history']['createdDate'],\n",
    "                last_modified=page['version']['when'],\n",
    "                labels=labels,\n",
    "                child_pages=child_pages\n",
    "            )\n",
    "            \n",
    "            return confluence_page\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching page {page_id}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def crawl_from_url(self, root_url: str, max_depth: Optional[int] = None) -> List[ConfluencePage]:\n",
    "        \"\"\"Crawl Confluence pages starting from a root URL\"\"\"\n",
    "        max_depth = max_depth or self.config.max_depth\n",
    "        \n",
    "        # Extract root page ID\n",
    "        root_page_id = self.extract_page_id_from_url(root_url)\n",
    "        if not root_page_id:\n",
    "            raise ValueError(f\"Could not extract page ID from URL: {root_url}\")\n",
    "        \n",
    "        logger.info(f\"Starting crawl from page ID: {root_page_id}\")\n",
    "        \n",
    "        # BFS crawling\n",
    "        queue = [(root_page_id, 0)]\n",
    "        self.visited_pages = set()\n",
    "        self.pages = []\n",
    "        \n",
    "        with tqdm(total=self.config.max_pages, desc=\"Crawling pages\") as pbar:\n",
    "            while queue and len(self.pages) < self.config.max_pages:\n",
    "                page_id, depth = queue.pop(0)\n",
    "                \n",
    "                if page_id in self.visited_pages or depth > max_depth:\n",
    "                    continue\n",
    "                \n",
    "                self.visited_pages.add(page_id)\n",
    "                \n",
    "                # Fetch page\n",
    "                page = self.fetch_page(page_id)\n",
    "                if page:\n",
    "                    self.pages.append(page)\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "                    # Add child pages to queue\n",
    "                    for child_id in page.child_pages:\n",
    "                        if child_id not in self.visited_pages:\n",
    "                            queue.append((child_id, depth + 1))\n",
    "        \n",
    "        logger.info(f\"Crawled {len(self.pages)} pages\")\n",
    "        return self.pages\n",
    "    \n",
    "    def crawl_space(self, space_key: str, limit: int = 100) -> List[ConfluencePage]:\n",
    "        \"\"\"Crawl all pages in a Confluence space\"\"\"\n",
    "        logger.info(f\"Crawling space: {space_key}\")\n",
    "        \n",
    "        self.pages = []\n",
    "        start = 0\n",
    "        \n",
    "        with tqdm(total=limit, desc=\"Crawling space pages\") as pbar:\n",
    "            while len(self.pages) < limit:\n",
    "                # Get pages in space\n",
    "                results = self.confluence.get_all_pages_from_space(\n",
    "                    space_key,\n",
    "                    start=start,\n",
    "                    limit=min(25, limit - len(self.pages))\n",
    "                )\n",
    "                \n",
    "                if not results:\n",
    "                    break\n",
    "                \n",
    "                # Fetch each page\n",
    "                for page_summary in results:\n",
    "                    page = self.fetch_page(page_summary['id'])\n",
    "                    if page:\n",
    "                        self.pages.append(page)\n",
    "                        pbar.update(1)\n",
    "                \n",
    "                start += 25\n",
    "        \n",
    "        logger.info(f\"Crawled {len(self.pages)} pages from space {space_key}\")\n",
    "        return self.pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Document Processing and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    \"\"\"Process Confluence pages into embedded documents\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        \n",
    "        # Initialize text splitter\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=config.chunk_size,\n",
    "            chunk_overlap=config.chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "        )\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        if config.embedding_provider == 'openai':\n",
    "            self.embeddings = OpenAIEmbeddings(\n",
    "                model=config.embedding_model,\n",
    "                openai_api_key=config.openai_api_key\n",
    "            )\n",
    "        elif config.embedding_provider == 'cohere':\n",
    "            self.embeddings = CohereEmbeddings(\n",
    "                model=\"embed-english-v3.0\",\n",
    "                cohere_api_key=config.cohere_api_key\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported embedding provider: {config.embedding_provider}\")\n",
    "    \n",
    "    def create_document_id(self, page: ConfluencePage, chunk_index: int) -> str:\n",
    "        \"\"\"Create unique document ID\"\"\"\n",
    "        content = f\"{page.id}_{chunk_index}\"\n",
    "        return hashlib.md5(content.encode()).hexdigest()\n",
    "    \n",
    "    def process_pages(self, pages: List[ConfluencePage]) -> List[Document]:\n",
    "        \"\"\"Process Confluence pages into LangChain documents\"\"\"\n",
    "        documents = []\n",
    "        \n",
    "        for page in tqdm(pages, desc=\"Processing pages\"):\n",
    "            # Skip pages with minimal content\n",
    "            if len(page.content) < 50:\n",
    "                continue\n",
    "            \n",
    "            # Split content into chunks\n",
    "            texts = self.text_splitter.split_text(page.content)\n",
    "            \n",
    "            # Create documents with metadata\n",
    "            for i, text in enumerate(texts):\n",
    "                metadata = {\n",
    "                    'page_id': page.id,\n",
    "                    'page_title': page.title,\n",
    "                    'page_url': page.url,\n",
    "                    'space_key': page.space_key,\n",
    "                    'created_by': page.created_by,\n",
    "                    'created_date': page.created_date,\n",
    "                    'last_modified': page.last_modified,\n",
    "                    'labels': ', '.join(page.labels),\n",
    "                    'chunk_index': i,\n",
    "                    'total_chunks': len(texts),\n",
    "                    'doc_id': self.create_document_id(page, i)\n",
    "                }\n",
    "                \n",
    "                doc = Document(\n",
    "                    page_content=text,\n",
    "                    metadata=metadata\n",
    "                )\n",
    "                documents.append(doc)\n",
    "        \n",
    "        logger.info(f\"Created {len(documents)} document chunks from {len(pages)} pages\")\n",
    "        return documents\n",
    "    \n",
    "    def batch_embed_documents(self, documents: List[Document], batch_size: int = 100) -> List[Tuple[Document, List[float]]]:\n",
    "        \"\"\"Embed documents in batches\"\"\"\n",
    "        embedded_docs = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(documents), batch_size), desc=\"Embedding documents\"):\n",
    "            batch = documents[i:i + batch_size]\n",
    "            texts = [doc.page_content for doc in batch]\n",
    "            \n",
    "            try:\n",
    "                embeddings = self.embeddings.embed_documents(texts)\n",
    "                for doc, embedding in zip(batch, embeddings):\n",
    "                    embedded_docs.append((doc, embedding))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error embedding batch {i}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return embedded_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Vector Database Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages vector storage and retrieval\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config, embeddings):\n",
    "        self.config = config\n",
    "        self.embeddings = embeddings\n",
    "        self.vectorstore = None\n",
    "        \n",
    "        # Initialize vector store\n",
    "        if config.vector_db_provider == 'chroma':\n",
    "            self._init_chroma()\n",
    "        elif config.vector_db_provider == 'pinecone':\n",
    "            self._init_pinecone()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported vector DB provider: {config.vector_db_provider}\")\n",
    "    \n",
    "    def _init_chroma(self):\n",
    "        \"\"\"Initialize ChromaDB\"\"\"\n",
    "        import chromadb\n",
    "        from chromadb.config import Settings\n",
    "        \n",
    "        # Create or load collection\n",
    "        self.vectorstore = Chroma(\n",
    "            collection_name=\"confluence_pages\",\n",
    "            embedding_function=self.embeddings,\n",
    "            persist_directory=self.config.chroma_persist_dir,\n",
    "            collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    "        )\n",
    "        logger.info(\"Initialized ChromaDB vector store\")\n",
    "    \n",
    "    def _init_pinecone(self):\n",
    "        \"\"\"Initialize Pinecone\"\"\"\n",
    "        import pinecone\n",
    "        \n",
    "        # Initialize Pinecone\n",
    "        pinecone.init(\n",
    "            api_key=self.config.pinecone_api_key,\n",
    "            environment=self.config.pinecone_environment\n",
    "        )\n",
    "        \n",
    "        # Create index if it doesn't exist\n",
    "        if self.config.pinecone_index not in pinecone.list_indexes():\n",
    "            pinecone.create_index(\n",
    "                name=self.config.pinecone_index,\n",
    "                dimension=1536,  # OpenAI embedding dimension\n",
    "                metric='cosine'\n",
    "            )\n",
    "        \n",
    "        # Initialize vector store\n",
    "        from langchain.vectorstores import Pinecone as PineconeVectorStore\n",
    "        self.vectorstore = PineconeVectorStore(\n",
    "            index_name=self.config.pinecone_index,\n",
    "            embedding=self.embeddings\n",
    "        )\n",
    "        logger.info(\"Initialized Pinecone vector store\")\n",
    "    \n",
    "    def add_documents(self, documents: List[Document], batch_size: int = 100):\n",
    "        \"\"\"Add documents to vector store\"\"\"\n",
    "        logger.info(f\"Adding {len(documents)} documents to vector store\")\n",
    "        \n",
    "        for i in tqdm(range(0, len(documents), batch_size), desc=\"Adding to vector store\"):\n",
    "            batch = documents[i:i + batch_size]\n",
    "            try:\n",
    "                self.vectorstore.add_documents(batch)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error adding batch {i}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Persist if using Chroma\n",
    "        if self.config.vector_db_provider == 'chroma':\n",
    "            self.vectorstore.persist()\n",
    "        \n",
    "        logger.info(\"Documents added to vector store\")\n",
    "    \n",
    "    def similarity_search(self, query: str, k: int = None, score_threshold: float = None) -> List[Document]:\n",
    "        \"\"\"Perform similarity search\"\"\"\n",
    "        k = k or self.config.top_k\n",
    "        score_threshold = score_threshold or self.config.score_threshold\n",
    "        \n",
    "        # Search with score\n",
    "        results_with_scores = self.vectorstore.similarity_search_with_score(query, k=k)\n",
    "        \n",
    "        # Filter by score threshold\n",
    "        filtered_results = [\n",
    "            doc for doc, score in results_with_scores\n",
    "            if score >= score_threshold\n",
    "        ]\n",
    "        \n",
    "        return filtered_results\n",
    "    \n",
    "    def mmr_search(self, query: str, k: int = None, fetch_k: int = 20, lambda_mult: float = 0.5) -> List[Document]:\n",
    "        \"\"\"Perform MMR (Maximal Marginal Relevance) search for diversity\"\"\"\n",
    "        k = k or self.config.top_k\n",
    "        \n",
    "        return self.vectorstore.max_marginal_relevance_search(\n",
    "            query,\n",
    "            k=k,\n",
    "            fetch_k=fetch_k,\n",
    "            lambda_mult=lambda_mult\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Agentic Tools Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfluenceSearchTools:\n",
    "    \"\"\"Tools for Confluence semantic search\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, config: Config):\n",
    "        self.vector_store = vector_store\n",
    "        self.config = config\n",
    "        \n",
    "    def semantic_search(self, query: str, top_k: int = 5) -> str:\n",
    "        \"\"\"Search for relevant Confluence pages using semantic similarity\"\"\"\n",
    "        try:\n",
    "            results = self.vector_store.similarity_search(query, k=top_k)\n",
    "            \n",
    "            if not results:\n",
    "                return \"No relevant pages found for your query.\"\n",
    "            \n",
    "            # Format results\n",
    "            formatted_results = []\n",
    "            for i, doc in enumerate(results, 1):\n",
    "                formatted_results.append(\n",
    "                    f\"**Result {i}:**\\n\"\n",
    "                    f\"Title: {doc.metadata.get('page_title', 'Unknown')}\\n\"\n",
    "                    f\"URL: {doc.metadata.get('page_url', 'N/A')}\\n\"\n",
    "                    f\"Space: {doc.metadata.get('space_key', 'Unknown')}\\n\"\n",
    "                    f\"Last Modified: {doc.metadata.get('last_modified', 'Unknown')}\\n\"\n",
    "                    f\"Content: {doc.page_content[:500]}...\\n\"\n",
    "                )\n",
    "            \n",
    "            return \"\\n\\n\".join(formatted_results)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in semantic search: {str(e)}\")\n",
    "            return f\"Error performing search: {str(e)}\"\n",
    "    \n",
    "    def diverse_search(self, query: str, top_k: int = 5) -> str:\n",
    "        \"\"\"Search with diversity using MMR algorithm\"\"\"\n",
    "        try:\n",
    "            results = self.vector_store.mmr_search(query, k=top_k)\n",
    "            \n",
    "            if not results:\n",
    "                return \"No relevant pages found for your query.\"\n",
    "            \n",
    "            # Format results\n",
    "            formatted_results = []\n",
    "            for i, doc in enumerate(results, 1):\n",
    "                formatted_results.append(\n",
    "                    f\"**Result {i}:**\\n\"\n",
    "                    f\"Title: {doc.metadata.get('page_title', 'Unknown')}\\n\"\n",
    "                    f\"Space: {doc.metadata.get('space_key', 'Unknown')}\\n\"\n",
    "                    f\"Content Preview: {doc.page_content[:300]}...\\n\"\n",
    "                )\n",
    "            \n",
    "            return \"\\n\\n\".join(formatted_results)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in diverse search: {str(e)}\")\n",
    "            return f\"Error performing search: {str(e)}\"\n",
    "    \n",
    "    def search_by_metadata(self, space_key: str = None, author: str = None, labels: str = None) -> str:\n",
    "        \"\"\"Search pages by metadata filters\"\"\"\n",
    "        try:\n",
    "            # Build filter\n",
    "            filter_dict = {}\n",
    "            if space_key:\n",
    "                filter_dict['space_key'] = space_key\n",
    "            if author:\n",
    "                filter_dict['created_by'] = author\n",
    "            \n",
    "            # Note: This is a simplified version. Real implementation would need\n",
    "            # proper metadata filtering support in your vector store\n",
    "            results = self.vector_store.vectorstore.similarity_search(\n",
    "                \"\",  # Empty query for metadata-only search\n",
    "                k=10,\n",
    "                filter=filter_dict if filter_dict else None\n",
    "            )\n",
    "            \n",
    "            if not results:\n",
    "                return \"No pages found matching the metadata criteria.\"\n",
    "            \n",
    "            # Format results\n",
    "            formatted_results = []\n",
    "            for doc in results[:5]:\n",
    "                formatted_results.append(\n",
    "                    f\"- {doc.metadata.get('page_title', 'Unknown')} \"\n",
    "                    f\"(Space: {doc.metadata.get('space_key', 'Unknown')}, \"\n",
    "                    f\"Author: {doc.metadata.get('created_by', 'Unknown')})\"\n",
    "                )\n",
    "            \n",
    "            return \"\\n\".join(formatted_results)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in metadata search: {str(e)}\")\n",
    "            return f\"Error performing metadata search: {str(e)}\"\n",
    "    \n",
    "    def get_page_summary(self, page_title: str) -> str:\n",
    "        \"\"\"Get a summary of a specific Confluence page\"\"\"\n",
    "        try:\n",
    "            # Search for the specific page\n",
    "            results = self.vector_store.similarity_search(f\"title: {page_title}\", k=3)\n",
    "            \n",
    "            # Find the most relevant page\n",
    "            target_page = None\n",
    "            for doc in results:\n",
    "                if page_title.lower() in doc.metadata.get('page_title', '').lower():\n",
    "                    target_page = doc\n",
    "                    break\n",
    "            \n",
    "            if not target_page:\n",
    "                return f\"No page found with title: {page_title}\"\n",
    "            \n",
    "            # Create summary\n",
    "            summary = (\n",
    "                f\"**Page: {target_page.metadata.get('page_title', 'Unknown')}**\\n\\n\"\n",
    "                f\"URL: {target_page.metadata.get('page_url', 'N/A')}\\n\"\n",
    "                f\"Space: {target_page.metadata.get('space_key', 'Unknown')}\\n\"\n",
    "                f\"Author: {target_page.metadata.get('created_by', 'Unknown')}\\n\"\n",
    "                f\"Last Modified: {target_page.metadata.get('last_modified', 'Unknown')}\\n\"\n",
    "                f\"Labels: {target_page.metadata.get('labels', 'None')}\\n\\n\"\n",
    "                f\"Content Preview:\\n{target_page.page_content[:1000]}...\"\n",
    "            )\n",
    "            \n",
    "            return summary\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting page summary: {str(e)}\")\n",
    "            return f\"Error getting page summary: {str(e)}\"\n",
    "    \n",
    "    def get_tools(self) -> List[Tool]:\n",
    "        \"\"\"Get LangChain tools\"\"\"\n",
    "        return [\n",
    "            Tool(\n",
    "                name=\"semantic_search\",\n",
    "                func=self.semantic_search,\n",
    "                description=\"Search Confluence pages using semantic similarity. Input should be a search query.\"\n",
    "            ),\n",
    "            Tool(\n",
    "                name=\"diverse_search\",\n",
    "                func=self.diverse_search,\n",
    "                description=\"Search Confluence with diversity to get varied results. Input should be a search query.\"\n",
    "            ),\n",
    "            Tool(\n",
    "                name=\"search_by_metadata\",\n",
    "                func=self.search_by_metadata,\n",
    "                description=\"Search pages by metadata like space_key, author, or labels. Input format: 'space_key=SPACE author=Name'.\"\n",
    "            ),\n",
    "            Tool(\n",
    "                name=\"get_page_summary\",\n",
    "                func=self.get_page_summary,\n",
    "                description=\"Get a detailed summary of a specific Confluence page. Input should be the page title.\"\n",
    "            )\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LangGraph Agent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage, ToolMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"Agent state for LangGraph\"\"\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    \n",
    "class ConfluenceSearchAgent:\n",
    "    \"\"\"LangGraph-based agent for Confluence search\"\"\"\n",
    "    \n",
    "    def __init__(self, tools: List[Tool], config: Config):\n",
    "        self.tools = tools\n",
    "        self.config = config\n",
    "        \n",
    "        # Initialize LLM\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=config.llm_model,\n",
    "            temperature=config.llm_temperature,\n",
    "            openai_api_key=config.openai_api_key\n",
    "        )\n",
    "        \n",
    "        # Bind tools to LLM\n",
    "        self.llm_with_tools = self.llm.bind_tools(tools)\n",
    "        \n",
    "        # Create graph\n",
    "        self.graph = self._build_graph()\n",
    "        \n",
    "        # Memory for conversation history\n",
    "        self.memory = MemorySaver()\n",
    "        \n",
    "        # Compile graph\n",
    "        self.app = self.graph.compile(checkpointer=self.memory)\n",
    "        \n",
    "    def _build_graph(self) -> StateGraph:\n",
    "        \"\"\"Build the agent graph\"\"\"\n",
    "        \n",
    "        # Define the graph\n",
    "        workflow = StateGraph(AgentState)\n",
    "        \n",
    "        # Define nodes\n",
    "        def agent(state: AgentState) -> dict:\n",
    "            \"\"\"Agent node that decides on actions\"\"\"\n",
    "            messages = state[\"messages\"]\n",
    "            \n",
    "            # Add system message if this is the first message\n",
    "            if len(messages) == 1:\n",
    "                system_prompt = (\n",
    "                    \"You are a helpful assistant specialized in searching and retrieving information \"\n",
    "                    \"from Confluence pages. You have access to semantic search tools that can find \"\n",
    "                    \"relevant documentation based on queries. Always provide clear, concise answers \"\n",
    "                    \"and include links to relevant pages when available. If you're not sure about \"\n",
    "                    \"something, use the search tools to find the information.\"\n",
    "                )\n",
    "                messages = [SystemMessage(content=system_prompt)] + messages\n",
    "            \n",
    "            # Get response from LLM\n",
    "            response = self.llm_with_tools.invoke(messages)\n",
    "            \n",
    "            return {\"messages\": [response]}\n",
    "        \n",
    "        # Add nodes\n",
    "        workflow.add_node(\"agent\", agent)\n",
    "        workflow.add_node(\"tools\", ToolNode(self.tools))\n",
    "        \n",
    "        # Set entry point\n",
    "        workflow.set_entry_point(\"agent\")\n",
    "        \n",
    "        # Add conditional edges\n",
    "        workflow.add_conditional_edges(\n",
    "            \"agent\",\n",
    "            tools_condition,\n",
    "            {\n",
    "                \"tools\": \"tools\",\n",
    "                \"__end__\": END\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Add edge from tools back to agent\n",
    "        workflow.add_edge(\"tools\", \"agent\")\n",
    "        \n",
    "        return workflow\n",
    "    \n",
    "    def search(self, query: str, thread_id: str = \"default\") -> str:\n",
    "        \"\"\"Execute a search query\"\"\"\n",
    "        \n",
    "        # Create initial message\n",
    "        initial_state = {\n",
    "            \"messages\": [HumanMessage(content=query)]\n",
    "        }\n",
    "        \n",
    "        # Run the agent\n",
    "        config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "        \n",
    "        try:\n",
    "            result = self.app.invoke(initial_state, config)\n",
    "            \n",
    "            # Extract the final response\n",
    "            messages = result[\"messages\"]\n",
    "            \n",
    "            # Find the last AI message\n",
    "            for message in reversed(messages):\n",
    "                if isinstance(message, AIMessage) and not message.tool_calls:\n",
    "                    return message.content\n",
    "            \n",
    "            return \"No response generated.\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in agent search: {str(e)}\")\n",
    "            return f\"Error processing query: {str(e)}\"\n",
    "    \n",
    "    def stream_search(self, query: str, thread_id: str = \"default\"):\n",
    "        \"\"\"Stream search results\"\"\"\n",
    "        \n",
    "        initial_state = {\n",
    "            \"messages\": [HumanMessage(content=query)]\n",
    "        }\n",
    "        \n",
    "        config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "        \n",
    "        try:\n",
    "            for event in self.app.stream(initial_state, config):\n",
    "                for value in event.values():\n",
    "                    if \"messages\" in value:\n",
    "                        for message in value[\"messages\"]:\n",
    "                            if isinstance(message, AIMessage):\n",
    "                                if message.content:\n",
    "                                    yield message.content\n",
    "                                if message.tool_calls:\n",
    "                                    yield f\"\\nüîß Using tool: {message.tool_calls[0]['name']}\\n\"\n",
    "                            elif isinstance(message, ToolMessage):\n",
    "                                yield f\"\\nüìä Tool result received\\n\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in stream search: {str(e)}\")\n",
    "            yield f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Observability with Braintrust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObservabilityManager:\n",
    "    \"\"\"Manage observability and monitoring with Braintrust\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.enabled = config.enable_monitoring and config.braintrust_api_key\n",
    "        \n",
    "        if self.enabled:\n",
    "            try:\n",
    "                braintrust.init(api_key=config.braintrust_api_key)\n",
    "                self.project = braintrust.init_project(\"confluence-search\")\n",
    "                logger.info(\"Braintrust observability initialized\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to initialize Braintrust: {str(e)}\")\n",
    "                self.enabled = False\n",
    "    \n",
    "    def log_search(self, query: str, results: str, latency: float, metadata: Dict = None):\n",
    "        \"\"\"Log a search interaction\"\"\"\n",
    "        if not self.enabled:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            experiment = braintrust.init_experiment(\n",
    "                project=\"confluence-search\",\n",
    "                name=f\"search-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "            )\n",
    "            \n",
    "            experiment.log(\n",
    "                input=query,\n",
    "                output=results,\n",
    "                metadata={\n",
    "                    \"latency_ms\": latency * 1000,\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                    **(metadata or {})\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            experiment.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to log to Braintrust: {str(e)}\")\n",
    "    \n",
    "    def log_crawl_metrics(self, pages_crawled: int, duration: float, errors: int = 0):\n",
    "        \"\"\"Log crawling metrics\"\"\"\n",
    "        if not self.enabled:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            experiment = braintrust.init_experiment(\n",
    "                project=\"confluence-search\",\n",
    "                name=f\"crawl-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "            )\n",
    "            \n",
    "            experiment.log(\n",
    "                input=\"crawl_operation\",\n",
    "                output=f\"Crawled {pages_crawled} pages\",\n",
    "                metadata={\n",
    "                    \"pages_crawled\": pages_crawled,\n",
    "                    \"duration_seconds\": duration,\n",
    "                    \"errors\": errors,\n",
    "                    \"pages_per_second\": pages_crawled / duration if duration > 0 else 0\n",
    "                },\n",
    "                scores={\n",
    "                    \"success_rate\": (pages_crawled - errors) / pages_crawled if pages_crawled > 0 else 0\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            experiment.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to log crawl metrics: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Main Pipeline Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfluenceSearchPipeline:\n",
    "    \"\"\"Main pipeline for Confluence semantic search\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config = None):\n",
    "        self.config = config or Config()\n",
    "        self.crawler = None\n",
    "        self.processor = None\n",
    "        self.vector_store = None\n",
    "        self.agent = None\n",
    "        self.observability = ObservabilityManager(self.config)\n",
    "        \n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize all components\"\"\"\n",
    "        logger.info(\"Initializing Confluence Search Pipeline\")\n",
    "        \n",
    "        # Initialize components\n",
    "        self.crawler = ConfluenceCrawler(self.config)\n",
    "        self.processor = DocumentProcessor(self.config)\n",
    "        self.vector_store = VectorStore(self.config, self.processor.embeddings)\n",
    "        \n",
    "        # Initialize tools and agent\n",
    "        tools_manager = ConfluenceSearchTools(self.vector_store, self.config)\n",
    "        tools = tools_manager.get_tools()\n",
    "        self.agent = ConfluenceSearchAgent(tools, self.config)\n",
    "        \n",
    "        logger.info(\"Pipeline initialized successfully\")\n",
    "    \n",
    "    def index_confluence_url(self, root_url: str, max_depth: int = None) -> Dict[str, Any]:\n",
    "        \"\"\"Index Confluence pages starting from a URL\"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Crawl pages\n",
    "            logger.info(f\"Starting crawl from: {root_url}\")\n",
    "            pages = self.crawler.crawl_from_url(root_url, max_depth)\n",
    "            \n",
    "            if not pages:\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"message\": \"No pages found to index\"\n",
    "                }\n",
    "            \n",
    "            # Process pages\n",
    "            logger.info(f\"Processing {len(pages)} pages\")\n",
    "            documents = self.processor.process_pages(pages)\n",
    "            \n",
    "            # Add to vector store\n",
    "            logger.info(f\"Adding {len(documents)} documents to vector store\")\n",
    "            self.vector_store.add_documents(documents)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            duration = time.time() - start_time\n",
    "            \n",
    "            # Log metrics\n",
    "            self.observability.log_crawl_metrics(\n",
    "                pages_crawled=len(pages),\n",
    "                duration=duration\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"pages_indexed\": len(pages),\n",
    "                \"documents_created\": len(documents),\n",
    "                \"duration_seconds\": duration,\n",
    "                \"pages\": [{\n",
    "                    \"title\": p.title,\n",
    "                    \"url\": p.url,\n",
    "                    \"space\": p.space_key\n",
    "                } for p in pages[:10]]  # First 10 pages\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error indexing Confluence: {str(e)}\")\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"message\": str(e)\n",
    "            }\n",
    "    \n",
    "    def search(self, query: str, use_agent: bool = True) -> str:\n",
    "        \"\"\"Search for information\"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            if use_agent:\n",
    "                # Use agent for intelligent search\n",
    "                result = self.agent.search(query)\n",
    "            else:\n",
    "                # Direct vector search\n",
    "                docs = self.vector_store.similarity_search(query)\n",
    "                if docs:\n",
    "                    result = \"\\n\\n\".join([\n",
    "                        f\"**{doc.metadata.get('page_title', 'Unknown')}**\\n{doc.page_content[:500]}...\"\n",
    "                        for doc in docs\n",
    "                    ])\n",
    "                else:\n",
    "                    result = \"No relevant documents found.\"\n",
    "            \n",
    "            # Log search\n",
    "            latency = time.time() - start_time\n",
    "            self.observability.log_search(\n",
    "                query=query,\n",
    "                results=result,\n",
    "                latency=latency,\n",
    "                metadata={\"use_agent\": use_agent}\n",
    "            )\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during search: {str(e)}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    def interactive_search(self):\n",
    "        \"\"\"Interactive search interface\"\"\"\n",
    "        print(\"\\nüîç Confluence Semantic Search Agent\")\n",
    "        print(\"=\"*50)\n",
    "        print(\"Type 'exit' to quit, 'help' for commands\\n\")\n",
    "        \n",
    "        while True:\n",
    "            query = input(\"\\nüí¨ Your query: \").strip()\n",
    "            \n",
    "            if query.lower() == 'exit':\n",
    "                break\n",
    "            elif query.lower() == 'help':\n",
    "                print(\"\\nAvailable commands:\")\n",
    "                print(\"  - Any search query: Search Confluence pages\")\n",
    "                print(\"  - 'exit': Quit the application\")\n",
    "                print(\"  - 'help': Show this help message\")\n",
    "                continue\n",
    "            elif not query:\n",
    "                continue\n",
    "            \n",
    "            print(\"\\nü§î Thinking...\")\n",
    "            \n",
    "            # Stream results\n",
    "            for chunk in self.agent.stream_search(query):\n",
    "                print(chunk, end='', flush=True)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Usage Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Basic Setup and Indexing\n",
    "def example_basic_indexing():\n",
    "    \"\"\"Example of basic indexing workflow\"\"\"\n",
    "    \n",
    "    # Create configuration\n",
    "    config = Config(\n",
    "        confluence_url=\"https://your-domain.atlassian.net\",\n",
    "        confluence_username=\"your-email@example.com\",\n",
    "        confluence_api_token=\"your-api-token\",\n",
    "        openai_api_key=\"your-openai-key\"\n",
    "    )\n",
    "    \n",
    "    # Initialize pipeline\n",
    "    pipeline = ConfluenceSearchPipeline(config)\n",
    "    pipeline.initialize()\n",
    "    \n",
    "    # Index Confluence pages\n",
    "    root_url = \"https://your-domain.atlassian.net/wiki/spaces/DOCS/pages/123456/Documentation\"\n",
    "    result = pipeline.index_confluence_url(root_url, max_depth=2)\n",
    "    \n",
    "    print(f\"Indexing result: {json.dumps(result, indent=2)}\")\n",
    "    \n",
    "    # Perform search\n",
    "    query = \"How to set up authentication?\"\n",
    "    answer = pipeline.search(query)\n",
    "    print(f\"\\nSearch result:\\n{answer}\")\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "# Uncomment to run:\n",
    "# pipeline = example_basic_indexing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Advanced Search with Multiple Tools\n",
    "def example_advanced_search():\n",
    "    \"\"\"Example of using different search strategies\"\"\"\n",
    "    \n",
    "    # Assuming pipeline is already initialized and indexed\n",
    "    pipeline = ConfluenceSearchPipeline()\n",
    "    pipeline.initialize()\n",
    "    \n",
    "    # Different search examples\n",
    "    queries = [\n",
    "        \"Find all pages about API documentation\",\n",
    "        \"What are the deployment procedures?\",\n",
    "        \"Show me pages created by John Doe\",\n",
    "        \"Get a summary of the Security Guidelines page\"\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        result = pipeline.search(query, use_agent=True)\n",
    "        print(result)\n",
    "\n",
    "# Uncomment to run:\n",
    "# example_advanced_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Interactive Mode\n",
    "def run_interactive_mode():\n",
    "    \"\"\"Run the interactive search interface\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Setting up Confluence Semantic Search Agent...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Initialize with environment variables\n",
    "    pipeline = ConfluenceSearchPipeline()\n",
    "    pipeline.initialize()\n",
    "    \n",
    "    # Check if we need to index first\n",
    "    choice = input(\"\\nDo you want to index Confluence pages first? (y/n): \")\n",
    "    \n",
    "    if choice.lower() == 'y':\n",
    "        root_url = input(\"Enter Confluence page URL to start indexing: \")\n",
    "        max_depth = int(input(\"Enter max crawl depth (default 3): \") or \"3\")\n",
    "        \n",
    "        print(\"\\nüìö Indexing Confluence pages...\")\n",
    "        result = pipeline.index_confluence_url(root_url, max_depth)\n",
    "        print(f\"\\n‚úÖ Indexed {result.get('pages_indexed', 0)} pages\")\n",
    "    \n",
    "    # Start interactive search\n",
    "    pipeline.interactive_search()\n",
    "    \n",
    "    print(\"\\nThank you for using Confluence Search Agent!\")\n",
    "\n",
    "# Uncomment to run:\n",
    "# run_interactive_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Quick Start Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick start - Run this cell to get started quickly\n",
    "def quick_start():\n",
    "    \"\"\"\n",
    "    Quick start guide for the Confluence Semantic Search Agent.\n",
    "    \n",
    "    Before running, make sure to:\n",
    "    1. Set up your .env file with:\n",
    "       - CONFLUENCE_URL\n",
    "       - CONFLUENCE_USERNAME\n",
    "       - CONFLUENCE_API_TOKEN\n",
    "       - OPENAI_API_KEY\n",
    "       - (Optional) BRAINTRUST_API_KEY for monitoring\n",
    "    \n",
    "    2. Install required packages (run the first cell)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üöÄ Confluence Semantic Search Agent - Quick Start\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Check environment variables\n",
    "    required_vars = ['CONFLUENCE_URL', 'CONFLUENCE_USERNAME', 'CONFLUENCE_API_TOKEN', 'OPENAI_API_KEY']\n",
    "    missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
    "    \n",
    "    if missing_vars:\n",
    "        print(\"‚ö†Ô∏è  Missing environment variables:\")\n",
    "        for var in missing_vars:\n",
    "            print(f\"   - {var}\")\n",
    "        print(\"\\nPlease set these in your .env file or environment.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize pipeline\n",
    "    print(\"\\nüì¶ Initializing pipeline...\")\n",
    "    pipeline = ConfluenceSearchPipeline()\n",
    "    pipeline.initialize()\n",
    "    print(\"‚úÖ Pipeline initialized\")\n",
    "    \n",
    "    # Get user input for indexing\n",
    "    print(\"\\nüìö Let's index some Confluence pages!\")\n",
    "    root_url = input(\"Enter a Confluence page URL to start indexing: \")\n",
    "    \n",
    "    if root_url:\n",
    "        print(\"\\nüîÑ Indexing pages (this may take a few minutes)...\")\n",
    "        result = pipeline.index_confluence_url(root_url, max_depth=2)\n",
    "        \n",
    "        if result['status'] == 'success':\n",
    "            print(f\"\\n‚úÖ Successfully indexed {result['pages_indexed']} pages!\")\n",
    "            print(f\"   Created {result['documents_created']} searchable chunks\")\n",
    "            print(f\"   Time taken: {result['duration_seconds']:.2f} seconds\")\n",
    "        else:\n",
    "            print(f\"\\n‚ùå Indexing failed: {result['message']}\")\n",
    "            return\n",
    "    \n",
    "    # Demo search\n",
    "    print(\"\\nüîç Let's try a search!\")\n",
    "    demo_query = input(\"Enter your search query: \")\n",
    "    \n",
    "    if demo_query:\n",
    "        print(\"\\nü§î Searching...\\n\")\n",
    "        result = pipeline.search(demo_query)\n",
    "        print(result)\n",
    "    \n",
    "    # Offer interactive mode\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    choice = input(\"\\nWould you like to continue with interactive search? (y/n): \")\n",
    "    \n",
    "    if choice.lower() == 'y':\n",
    "        pipeline.interactive_search()\n",
    "    \n",
    "    print(\"\\nüëã Thank you for using Confluence Semantic Search Agent!\")\n",
    "    return pipeline\n",
    "\n",
    "# Run the quick start\n",
    "# pipeline = quick_start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Troubleshooting and Best Practices\n",
    "\n",
    "### Common Issues and Solutions\n",
    "\n",
    "1. **Authentication Errors**\n",
    "   - Ensure your Confluence API token is valid\n",
    "   - Check that you're using the correct username (email for cloud instances)\n",
    "   - Verify the Confluence URL format\n",
    "\n",
    "2. **Slow Indexing**\n",
    "   - Reduce `max_depth` to limit crawling depth\n",
    "   - Decrease `max_pages` to index fewer pages\n",
    "   - Use `batch_size` parameter for embedding operations\n",
    "\n",
    "3. **Memory Issues**\n",
    "   - Use smaller chunk sizes\n",
    "   - Implement pagination for large spaces\n",
    "   - Consider using Pinecone for serverless vector storage\n",
    "\n",
    "4. **Search Quality**\n",
    "   - Adjust `chunk_size` and `chunk_overlap` for better context\n",
    "   - Use MMR search for more diverse results\n",
    "   - Fine-tune the system prompt for the agent\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Incremental Indexing**: Index frequently accessed spaces first\n",
    "2. **Regular Updates**: Schedule periodic re-indexing for updated content\n",
    "3. **Monitor Performance**: Use Braintrust to track search quality\n",
    "4. **Access Control**: Implement proper authentication for sensitive content\n",
    "5. **Caching**: Cache frequently accessed pages to reduce API calls\n",
    "\n",
    "### Production Deployment\n",
    "\n",
    "For production use:\n",
    "- Use environment-specific configuration\n",
    "- Implement proper error handling and retries\n",
    "- Set up monitoring and alerting\n",
    "- Use a persistent vector database\n",
    "- Implement rate limiting for API calls\n",
    "- Add user authentication and authorization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}