{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Contradiction Detection with MLflow Faithfulness Metrics\n",
        "\n",
        "This notebook demonstrates how to use MLflow faithfulness metrics to detect **contradictions** in LLM outputs. A contradiction occurs when the model generates content that directly opposes or negates information stated in the provided context.\n",
        "\n",
        "## What is Contradiction?\n",
        "\n",
        "A contradiction is when an LLM generates:\n",
        "- **Direct negation** of facts in the context (e.g., context says \"X is true\", answer says \"X is false\")\n",
        "- **Opposite values** for numeric data (e.g., \"increased\" vs \"decreased\")\n",
        "- **Conflicting attributes** (e.g., \"red\" vs \"blue\", \"large\" vs \"small\")\n",
        "- **Temporal inconsistencies** (e.g., \"before\" vs \"after\", wrong dates)\n",
        "- **Logical impossibilities** given the context\n",
        "\n",
        "### Why Contradiction Detection Matters:\n",
        "| Impact | Example |\n",
        "|--------|---------|\n",
        "| **Misinformation** | Spreading false information that contradicts sources |\n",
        "| **Trust Erosion** | Users lose confidence when AI contradicts its sources |\n",
        "| **Critical Errors** | Medical/legal/financial contradictions can cause harm |\n",
        "| **Confusion** | Users don't know what to believe |\n",
        "\n",
        "### Faithfulness Score for Contradiction:\n",
        "| Score Range | Interpretation | Risk Level |\n",
        "|-------------|----------------|------------|\n",
        "| >= 0.70 | Consistent Response | Safe |\n",
        "| 0.45 - 0.70 | Minor Inconsistency | Low |\n",
        "| 0.25 - 0.45 | Likely Contradiction | Medium |\n",
        "| < 0.25 | Clear Contradiction | High |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q mlflow sentence-transformers pandas numpy scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%restart_python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from typing import List, Dict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "print(\"âœ… Libraries loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Contradiction Detector\n",
        "\n",
        "We use NLI (Natural Language Inference) as the primary signal for contradiction detection, since NLI models are specifically trained to identify when one text contradicts another. The model outputs three classes: entailment, neutral, and **contradiction**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ContradictionDetector:\n",
        "    \"\"\"\n",
        "    Detects contradictions where the generated answer directly opposes\n",
        "    or negates information in the provided context.\n",
        "    \n",
        "    Uses NLI model's contradiction score as the primary signal.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, \n",
        "                 embedding_model: str = \"all-MiniLM-L6-v2\",\n",
        "                 nli_model: str = \"cross-encoder/nli-deberta-v3-small\"):\n",
        "        \"\"\"Initialize with embedding and NLI models.\"\"\"\n",
        "        print(f\"Loading embedding model: {embedding_model}...\")\n",
        "        self.embedding_model = SentenceTransformer(embedding_model)\n",
        "        \n",
        "        print(f\"Loading NLI model: {nli_model}...\")\n",
        "        self.nli_model = CrossEncoder(nli_model)\n",
        "        \n",
        "        print(\"âœ… Contradiction detector ready!\")\n",
        "\n",
        "    def compute_semantic_similarity(self, answer: str, context: str) -> float:\n",
        "        \"\"\"Compute semantic similarity between answer and context.\"\"\"\n",
        "        answer_embedding = self.embedding_model.encode([answer])[0]\n",
        "        context_embedding = self.embedding_model.encode([context])[0]\n",
        "        similarity = cosine_similarity([answer_embedding], [context_embedding])[0][0]\n",
        "        return max(0, min(1, (similarity + 1) / 2))\n",
        "\n",
        "    def compute_nli_scores(self, answer: str, context: str) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Compute NLI scores: contradiction, neutral, entailment.\n",
        "        Returns all three scores for analysis.\n",
        "        \"\"\"\n",
        "        scores = self.nli_model.predict([(context, answer)])[0]\n",
        "        \n",
        "        if isinstance(scores, (int, float)):\n",
        "            # Single score model - use sigmoid\n",
        "            entailment = 1 / (1 + np.exp(-scores))\n",
        "            return {\n",
        "                \"contradiction\": 1 - entailment,\n",
        "                \"neutral\": 0.0,\n",
        "                \"entailment\": entailment\n",
        "            }\n",
        "        \n",
        "        # Multi-class model: [contradiction, neutral, entailment]\n",
        "        return {\n",
        "            \"contradiction\": float(scores[0]),\n",
        "            \"neutral\": float(scores[1]),\n",
        "            \"entailment\": float(scores[2])\n",
        "        }\n",
        "\n",
        "    def compute_token_overlap(self, answer: str, context: str) -> float:\n",
        "        \"\"\"Compute token overlap between answer and context.\"\"\"\n",
        "        stop_words = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', \n",
        "                      'to', 'of', 'in', 'for', 'on', 'with', 'at', 'by', 'from',\n",
        "                      'and', 'but', 'or', 'it', 'its', 'this', 'that', 'who', 'not'}\n",
        "        \n",
        "        answer_tokens = set(answer.lower().split()) - stop_words\n",
        "        context_tokens = set(context.lower().split()) - stop_words\n",
        "        \n",
        "        if not answer_tokens:\n",
        "            return 1.0\n",
        "        \n",
        "        overlap = answer_tokens.intersection(context_tokens)\n",
        "        return len(overlap) / len(answer_tokens)\n",
        "\n",
        "    def detect_contradiction(self, answer: str, context: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Detect if an answer contradicts the context.\n",
        "        \n",
        "        Returns:\n",
        "            Dictionary with scores and contradiction verdict\n",
        "        \"\"\"\n",
        "        semantic = self.compute_semantic_similarity(answer, context)\n",
        "        nli_scores = self.compute_nli_scores(answer, context)\n",
        "        overlap = self.compute_token_overlap(answer, context)\n",
        "        \n",
        "        # For contradiction detection, we focus on the contradiction score\n",
        "        # High contradiction score = bad (contradicts context)\n",
        "        # High entailment score = good (supported by context)\n",
        "        \n",
        "        # Faithfulness score: high entailment + low contradiction = good\n",
        "        # We invert the scoring: higher score = less contradiction\n",
        "        faithfulness = nli_scores[\"entailment\"] - (0.5 * nli_scores[\"contradiction\"])\n",
        "        faithfulness = max(0, min(1, (faithfulness + 0.5)))  # Normalize to 0-1\n",
        "        \n",
        "        # Combined score weighted toward NLI\n",
        "        combined = 0.60 * faithfulness + 0.25 * semantic + 0.15 * overlap\n",
        "        \n",
        "        # Determine contradiction category based on contradiction score\n",
        "        contradiction_score = nli_scores[\"contradiction\"]\n",
        "        \n",
        "        if contradiction_score >= 0.7:\n",
        "            category = \"ğŸš« Clear Contradiction\"\n",
        "            is_contradiction = True\n",
        "            risk = \"High\"\n",
        "        elif contradiction_score >= 0.4:\n",
        "            category = \"âŒ Likely Contradiction\"\n",
        "            is_contradiction = True\n",
        "            risk = \"Medium\"\n",
        "        elif contradiction_score >= 0.2:\n",
        "            category = \"âš ï¸ Minor Inconsistency\"\n",
        "            is_contradiction = True\n",
        "            risk = \"Low\"\n",
        "        else:\n",
        "            category = \"âœ… Consistent Response\"\n",
        "            is_contradiction = False\n",
        "            risk = \"Low\"\n",
        "        \n",
        "        return {\n",
        "            \"semantic_score\": semantic,\n",
        "            \"nli_entailment\": nli_scores[\"entailment\"],\n",
        "            \"nli_neutral\": nli_scores[\"neutral\"],\n",
        "            \"nli_contradiction\": nli_scores[\"contradiction\"],\n",
        "            \"overlap_score\": overlap,\n",
        "            \"faithfulness_score\": faithfulness,\n",
        "            \"combined_score\": combined,\n",
        "            \"category\": category,\n",
        "            \"is_contradiction\": is_contradiction,\n",
        "            \"risk_level\": risk\n",
        "        }\n",
        "\n",
        "# Initialize the detector\n",
        "detector = ContradictionDetector()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Test Examples: Detecting Contradictions\n",
        "\n",
        "We'll test several types of contradictions:\n",
        "1. **Numeric Contradictions** - Wrong numbers, percentages, dates\n",
        "2. **Logical Contradictions** - Direct negation of facts\n",
        "3. **Attribute Contradictions** - Wrong properties or characteristics\n",
        "4. **Temporal Contradictions** - Wrong sequence or timing\n",
        "5. **Directional Contradictions** - Opposite directions (increase/decrease, up/down)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define test examples with context, consistent answer, and contradictory answer\n",
        "test_examples = [\n",
        "    {\n",
        "        \"name\": \"Example 1: Numeric Contradiction\",\n",
        "        \"category\": \"Numeric\",\n",
        "        \"context\": \"The company reported revenue of $50 million in Q3 2024, representing a 25% increase from the previous year.\",\n",
        "        \"consistent_answer\": \"The company's Q3 2024 revenue was $50 million, up 25% year-over-year.\",\n",
        "        \"contradictory_answer\": \"The company's Q3 2024 revenue was $30 million, representing a 10% decrease from last year.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Example 2: Logical Contradiction\",\n",
        "        \"category\": \"Logical\",\n",
        "        \"context\": \"The study concluded that the new drug was effective in treating the condition, with 85% of patients showing improvement.\",\n",
        "        \"consistent_answer\": \"The drug proved effective, with 85% of patients improving in the study.\",\n",
        "        \"contradictory_answer\": \"The study found that the drug was ineffective, with most patients showing no improvement.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Example 3: Attribute Contradiction\",\n",
        "        \"category\": \"Attribute\",\n",
        "        \"context\": \"The electric vehicle has a range of 300 miles on a single charge and can accelerate from 0 to 60 mph in 4 seconds.\",\n",
        "        \"consistent_answer\": \"The EV offers 300 miles of range and reaches 60 mph in just 4 seconds.\",\n",
        "        \"contradictory_answer\": \"The EV has a limited range of only 150 miles and slow acceleration, taking 12 seconds to reach 60 mph.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Example 4: Temporal Contradiction\",\n",
        "        \"category\": \"Temporal\",\n",
        "        \"context\": \"World War II ended in 1945 after Germany surrendered in May and Japan surrendered in September.\",\n",
        "        \"consistent_answer\": \"WWII concluded in 1945 with Germany's surrender in May and Japan's in September.\",\n",
        "        \"contradictory_answer\": \"World War II ended in 1942, with Japan surrendering before Germany.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Example 5: Directional Contradiction\",\n",
        "        \"category\": \"Directional\",\n",
        "        \"context\": \"Global temperatures have risen by 1.1Â°C since pre-industrial times, primarily due to greenhouse gas emissions.\",\n",
        "        \"consistent_answer\": \"Global temperatures have increased by 1.1Â°C from pre-industrial levels due to greenhouse gases.\",\n",
        "        \"contradictory_answer\": \"Global temperatures have decreased significantly since pre-industrial times, despite increased emissions.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Example 6: Outcome Contradiction\",\n",
        "        \"category\": \"Outcome\",\n",
        "        \"context\": \"The patient's surgery was successful, and they were discharged from the hospital after 3 days of recovery.\",\n",
        "        \"consistent_answer\": \"The surgery went well, and the patient left the hospital after a 3-day recovery.\",\n",
        "        \"contradictory_answer\": \"The surgery failed, and the patient remained in critical condition for several weeks.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Example 7: Identity Contradiction\",\n",
        "        \"category\": \"Identity\",\n",
        "        \"context\": \"Python is a high-level, interpreted programming language created by Guido van Rossum in 1991.\",\n",
        "        \"consistent_answer\": \"Python, created by Guido van Rossum in 1991, is an interpreted high-level language.\",\n",
        "        \"contradictory_answer\": \"Python is a low-level, compiled language developed by Bill Gates in the 1970s.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"ğŸ“‹ Loaded {len(test_examples)} contradiction test examples across {len(set(e['category'] for e in test_examples))} categories\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Running Contradiction Detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_detection_result(name: str, category: str, context: str, answer: str, result: Dict, answer_type: str):\n",
        "    \"\"\"Pretty print the contradiction detection result.\"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"ğŸ“ {name}\")\n",
        "    print(f\"   Category: {category} | Type: {answer_type}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"\\nğŸ“„ Context: {context[:100]}...\")\n",
        "    print(f\"\\nğŸ’¬ Answer: {answer}\")\n",
        "    print(f\"\\nğŸ“Š NLI SCORES:\")\n",
        "    print(f\"   â€¢ Entailment:     {result['nli_entailment']:.3f} (supports context)\")\n",
        "    print(f\"   â€¢ Neutral:        {result['nli_neutral']:.3f} (unrelated)\")\n",
        "    print(f\"   â€¢ Contradiction:  {result['nli_contradiction']:.3f} â† KEY METRIC\")\n",
        "    print(f\"\\nğŸ“Š OTHER SCORES:\")\n",
        "    print(f\"   â€¢ Semantic:       {result['semantic_score']:.3f}\")\n",
        "    print(f\"   â€¢ Overlap:        {result['overlap_score']:.3f}\")\n",
        "    print(f\"   â€¢ Combined:       {result['combined_score']:.3f}\")\n",
        "    print(f\"\\nğŸ¯ VERDICT: {result['category']}\")\n",
        "    print(f\"   â€¢ Is Contradiction: {'YES âŒ' if result['is_contradiction'] else 'NO âœ“'}\")\n",
        "    print(f\"   â€¢ Risk Level: {result['risk_level']}\")\n",
        "\n",
        "# Run detection on all examples\n",
        "print(\"ğŸ” CONTRADICTION DETECTION RESULTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for example in test_examples:\n",
        "    # Test consistent answer\n",
        "    consistent_result = detector.detect_contradiction(\n",
        "        example[\"consistent_answer\"], \n",
        "        example[\"context\"]\n",
        "    )\n",
        "    print_detection_result(\n",
        "        example[\"name\"], \n",
        "        example[\"category\"],\n",
        "        example[\"context\"], \n",
        "        example[\"consistent_answer\"], \n",
        "        consistent_result, \n",
        "        \"CONSISTENT ANSWER\"\n",
        "    )\n",
        "    all_results.append({\n",
        "        \"example\": example[\"name\"],\n",
        "        \"category\": example[\"category\"],\n",
        "        \"type\": \"Consistent\",\n",
        "        \"contradiction_score\": consistent_result[\"nli_contradiction\"],\n",
        "        \"combined_score\": consistent_result[\"combined_score\"],\n",
        "        \"is_contradiction\": consistent_result[\"is_contradiction\"],\n",
        "        \"verdict\": consistent_result[\"category\"]\n",
        "    })\n",
        "    \n",
        "    # Test contradictory answer\n",
        "    contradictory_result = detector.detect_contradiction(\n",
        "        example[\"contradictory_answer\"], \n",
        "        example[\"context\"]\n",
        "    )\n",
        "    print_detection_result(\n",
        "        example[\"name\"], \n",
        "        example[\"category\"],\n",
        "        example[\"context\"], \n",
        "        example[\"contradictory_answer\"], \n",
        "        contradictory_result, \n",
        "        \"CONTRADICTORY ANSWER\"\n",
        "    )\n",
        "    all_results.append({\n",
        "        \"example\": example[\"name\"],\n",
        "        \"category\": example[\"category\"],\n",
        "        \"type\": \"Contradictory\",\n",
        "        \"contradiction_score\": contradictory_result[\"nli_contradiction\"],\n",
        "        \"combined_score\": contradictory_result[\"combined_score\"],\n",
        "        \"is_contradiction\": contradictory_result[\"is_contradiction\"],\n",
        "        \"verdict\": contradictory_result[\"category\"]\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Summary Results Table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary DataFrame\n",
        "results_df = pd.DataFrame(all_results)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ“Š SUMMARY: CONTRADICTION DETECTION RESULTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Format the dataframe for display\n",
        "display_df = results_df.copy()\n",
        "display_df[\"contradiction_score\"] = display_df[\"contradiction_score\"].apply(lambda x: f\"{x:.3f}\")\n",
        "display_df[\"combined_score\"] = display_df[\"combined_score\"].apply(lambda x: f\"{x:.3f}\")\n",
        "display_df[\"is_contradiction\"] = display_df[\"is_contradiction\"].apply(lambda x: \"âŒ YES\" if x else \"âœ“ NO\")\n",
        "display_df.columns = [\"Example\", \"Category\", \"Answer Type\", \"Contradiction\", \"Combined\", \"Detected?\", \"Verdict\"]\n",
        "\n",
        "print(\"\\n\")\n",
        "print(display_df.to_string(index=False))\n",
        "\n",
        "# Calculate accuracy\n",
        "consistent_correct = sum(1 for r in all_results if r[\"type\"] == \"Consistent\" and not r[\"is_contradiction\"])\n",
        "contradictory_detected = sum(1 for r in all_results if r[\"type\"] == \"Contradictory\" and r[\"is_contradiction\"])\n",
        "total = len(test_examples)\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"ğŸ“ˆ DETECTION ACCURACY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"   â€¢ Consistent answers correctly identified:  {consistent_correct}/{total} ({consistent_correct/total*100:.0f}%)\")\n",
        "print(f\"   â€¢ Contradictions detected:                  {contradictory_detected}/{total} ({contradictory_detected/total*100:.0f}%)\")\n",
        "print(f\"   â€¢ Overall accuracy:                         {(consistent_correct+contradictory_detected)/(total*2)*100:.0f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Analysis by Contradiction Category\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze by category\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ“Š ANALYSIS BY CONTRADICTION CATEGORY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "categories = results_df[\"category\"].unique()\n",
        "\n",
        "for cat in categories:\n",
        "    cat_results = results_df[results_df[\"category\"] == cat]\n",
        "    consistent_contr = cat_results[cat_results[\"type\"] == \"Consistent\"][\"contradiction_score\"].values[0]\n",
        "    contradictory_contr = cat_results[cat_results[\"type\"] == \"Contradictory\"][\"contradiction_score\"].values[0]\n",
        "    score_gap = contradictory_contr - consistent_contr\n",
        "    \n",
        "    print(f\"\\nğŸ“Œ {cat}:\")\n",
        "    print(f\"   Consistent - Contradiction Score:     {consistent_contr:.3f}\")\n",
        "    print(f\"   Contradictory - Contradiction Score:  {contradictory_contr:.3f}\")\n",
        "    print(f\"   Detection Gap:                        {score_gap:.3f} {'âœ“' if score_gap > 0.30 else 'âš ï¸'}\")\n",
        "\n",
        "# Overall statistics\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"ğŸ“Š OVERALL STATISTICS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "consistent_scores = results_df[results_df[\"type\"] == \"Consistent\"][\"contradiction_score\"]\n",
        "contradictory_scores = results_df[results_df[\"type\"] == \"Contradictory\"][\"contradiction_score\"]\n",
        "\n",
        "print(f\"\\nğŸ“— CONSISTENT ANSWERS (Contradiction Scores - lower is better):\")\n",
        "print(f\"   Mean:  {consistent_scores.mean():.3f}\")\n",
        "print(f\"   Min:   {consistent_scores.min():.3f}\")\n",
        "print(f\"   Max:   {consistent_scores.max():.3f}\")\n",
        "\n",
        "print(f\"\\nğŸ“• CONTRADICTORY ANSWERS (Contradiction Scores - higher indicates contradiction):\")\n",
        "print(f\"   Mean:  {contradictory_scores.mean():.3f}\")\n",
        "print(f\"   Min:   {contradictory_scores.min():.3f}\")\n",
        "print(f\"   Max:   {contradictory_scores.max():.3f}\")\n",
        "\n",
        "print(f\"\\nğŸ“‰ SEPARATION:\")\n",
        "avg_gap = contradictory_scores.mean() - consistent_scores.mean()\n",
        "print(f\"   Average Gap: {avg_gap:.3f}\")\n",
        "print(f\"   Separation Quality: {'Excellent âœ“' if avg_gap > 0.40 else 'Good âœ“' if avg_gap > 0.25 else 'Moderate âš ï¸' if avg_gap > 0.15 else 'Poor âŒ'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Interactive Testing: Try Your Own Examples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_contradiction(context: str, answer: str):\n",
        "    \"\"\"\n",
        "    Test if an answer contradicts a given context.\n",
        "    \n",
        "    Usage:\n",
        "        test_contradiction(\n",
        "            context=\"The sky is blue during clear days.\",\n",
        "            answer=\"The sky appears red during clear days.\"\n",
        "        )\n",
        "    \"\"\"\n",
        "    result = detector.detect_contradiction(answer, context)\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(\"ğŸ” CONTRADICTION TEST\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nğŸ“„ Context:\\n{context}\")\n",
        "    print(f\"\\nğŸ’¬ Answer:\\n{answer}\")\n",
        "    print(f\"\\nğŸ“Š NLI SCORES:\")\n",
        "    print(f\"   â€¢ Entailment:     {result['nli_entailment']:.3f}\")\n",
        "    print(f\"   â€¢ Neutral:        {result['nli_neutral']:.3f}\")\n",
        "    print(f\"   â€¢ Contradiction:  {result['nli_contradiction']:.3f} â† KEY\")\n",
        "    print(f\"\\nğŸ“Š OTHER SCORES:\")\n",
        "    print(f\"   â€¢ Semantic:       {result['semantic_score']:.3f}\")\n",
        "    print(f\"   â€¢ Combined:       {result['combined_score']:.3f}\")\n",
        "    print(f\"\\nğŸ¯ RESULT: {result['category']}\")\n",
        "    print(f\"   Risk Level: {result['risk_level']}\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    return result\n",
        "\n",
        "# Example: Consistent answer\n",
        "print(\"TEST 1: Consistent Answer\")\n",
        "test_contradiction(\n",
        "    context=\"The Eiffel Tower is located in Paris, France, and stands 330 meters tall.\",\n",
        "    answer=\"The Eiffel Tower, at 330 meters, is a famous landmark in Paris, France.\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Contradictory answer\n",
        "print(\"TEST 2: Contradictory Answer\")\n",
        "test_contradiction(\n",
        "    context=\"The Eiffel Tower is located in Paris, France, and stands 330 meters tall.\",\n",
        "    answer=\"The Eiffel Tower is in London, England, and is only 100 meters tall.\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Subtle contradiction\n",
        "print(\"TEST 3: Subtle Contradiction (Directional)\")\n",
        "test_contradiction(\n",
        "    context=\"Company profits increased by 20% in the fourth quarter of 2024.\",\n",
        "    answer=\"Company profits declined significantly in the fourth quarter of 2024.\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Score Comparison Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visual comparison of contradiction scores\n",
        "consistent_scores = results_df[results_df[\"type\"] == \"Consistent\"][\"contradiction_score\"]\n",
        "contradictory_scores = results_df[results_df[\"type\"] == \"Contradictory\"][\"contradiction_score\"]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ğŸ“Š CONTRADICTION SCORE COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\n(Higher contradiction score = more contradictory)\")\n",
        "\n",
        "print(\"\\nğŸ“— CONSISTENT ANSWERS:\")\n",
        "print(f\"   Average:  {np.mean(consistent_scores):.3f}\")\n",
        "print(f\"   Min:      {np.min(consistent_scores):.3f}\")\n",
        "print(f\"   Max:      {np.max(consistent_scores):.3f}\")\n",
        "\n",
        "print(\"\\nğŸ“• CONTRADICTORY ANSWERS:\")\n",
        "print(f\"   Average:  {np.mean(contradictory_scores):.3f}\")\n",
        "print(f\"   Min:      {np.min(contradictory_scores):.3f}\")\n",
        "print(f\"   Max:      {np.max(contradictory_scores):.3f}\")\n",
        "\n",
        "print(\"\\nğŸ“‰ SEPARATION:\")\n",
        "score_diff = np.mean(contradictory_scores) - np.mean(consistent_scores)\n",
        "print(f\"   Score Gap: {score_diff:.3f}\")\n",
        "\n",
        "# Visual bar representation (inverted - lower is better for consistent)\n",
        "print(\"\\nğŸ“Š VISUAL COMPARISON (Contradiction Score):\")\n",
        "print(f\"   Consistent:    {'â–‘' * int(np.mean(consistent_scores) * 20):<20} {np.mean(consistent_scores):.2f} (low = good)\")\n",
        "print(f\"   Contradictory: {'â–ˆ' * int(np.mean(contradictory_scores) * 20):<20} {np.mean(contradictory_scores):.2f} (high = bad)\")\n",
        "print(f\"   Threshold:     {'â”€' * 8}â”‚{'â”€' * 11}  0.40 (contradiction threshold)\")\n",
        "\n",
        "# Per-category visualization\n",
        "print(\"\\nğŸ“Š CONTRADICTION SCORES BY CATEGORY:\")\n",
        "for cat in categories:\n",
        "    cat_data = results_df[results_df[\"category\"] == cat]\n",
        "    c_score = cat_data[cat_data[\"type\"] == \"Consistent\"][\"contradiction_score\"].values[0]\n",
        "    x_score = cat_data[cat_data[\"type\"] == \"Contradictory\"][\"contradiction_score\"].values[0]\n",
        "    print(f\"   {cat:12} Consistent: {c_score:.2f} {'â–‘' * int(c_score * 10):<10}  |  Contradictory: {x_score:.2f} {'â–ˆ' * int(x_score * 10):<10}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Key Takeaways\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\"\"\n",
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "â•‘                    KEY TAKEAWAYS: CONTRADICTION DETECTION                    â•‘\n",
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "âœ… WHAT WE DEMONSTRATED:\n",
        "\n",
        "1. CONTRADICTION TYPES DETECTED:\n",
        "   â€¢ Numeric contradictions (wrong numbers, percentages)\n",
        "   â€¢ Logical contradictions (direct negations)\n",
        "   â€¢ Attribute contradictions (wrong properties)\n",
        "   â€¢ Temporal contradictions (wrong dates, sequences)\n",
        "   â€¢ Directional contradictions (increase vs decrease)\n",
        "   â€¢ Outcome contradictions (success vs failure)\n",
        "   â€¢ Identity contradictions (wrong creators, origins)\n",
        "\n",
        "2. WHY NLI IS IDEAL FOR CONTRADICTION:\n",
        "   â€¢ NLI models are trained specifically for this task\n",
        "   â€¢ Output includes explicit \"contradiction\" score\n",
        "   â€¢ Can detect subtle logical inconsistencies\n",
        "   â€¢ Works across different domains\n",
        "\n",
        "3. KEY METRIC: NLI CONTRADICTION SCORE\n",
        "   â€¢ Low score (< 0.2): Answer is consistent with context\n",
        "   â€¢ Medium score (0.2 - 0.4): Minor inconsistency\n",
        "   â€¢ High score (0.4 - 0.7): Likely contradiction\n",
        "   â€¢ Very high (> 0.7): Clear contradiction\n",
        "\n",
        "4. RECOMMENDED THRESHOLDS:\n",
        "   â€¢ Contradiction < 0.20: Safe - consistent response\n",
        "   â€¢ Contradiction 0.20-0.40: Review - minor issues\n",
        "   â€¢ Contradiction 0.40-0.70: Flag - likely contradiction\n",
        "   â€¢ Contradiction > 0.70: Block - clear contradiction\n",
        "\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "ğŸ”— NEXT STEPS:\n",
        "   â€¢ Integrate with MLflow for production monitoring\n",
        "   â€¢ Set up automated alerts for high contradiction scores\n",
        "   â€¢ Combine with fact-checking systems\n",
        "   â€¢ Build domain-specific contradiction rules\n",
        "\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "âš ï¸ IMPORTANT NOTES:\n",
        "   â€¢ NLI models may struggle with very domain-specific content\n",
        "   â€¢ Subtle contradictions may have lower scores\n",
        "   â€¢ Consider context length limitations of NLI models\n",
        "   â€¢ Use ensemble approaches for high-stakes applications\n",
        "\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\"\"\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
