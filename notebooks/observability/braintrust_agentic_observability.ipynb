{
 "cells": [
   {
      "cell_type": "markdown",
      "id": "6a44f002",
      "metadata": {
        "id": "6a44f002"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dimitarpg13/agentic_architectures_and_design_patterns/blob/main/notebooks/observability/braintrust_agentic_observability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic System Observability with Braintrust\n",
    "\n",
    "This notebook demonstrates how to integrate **Braintrust** observability into an agentic AI system. We'll build a research assistant agent that uses multiple tools and track its behavior using Braintrust's logging and evaluation capabilities.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Setting up Braintrust for agent observability\n",
    "2. Creating a multi-step agent with tools\n",
    "3. Tracking agent decisions, tool calls, and reasoning\n",
    "4. Evaluating agent performance with custom metrics\n",
    "5. Analyzing agent behavior through Braintrust's dashboard\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "User Query ‚Üí Agent (with Braintrust tracking)\n",
    "              ‚Üì\n",
    "         Tool Selection\n",
    "              ‚Üì\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚Üì         ‚Üì         ‚Üì\n",
    "  Search   Calculator  Wikipedia\n",
    "    ‚Üì         ‚Üì         ‚Üì\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "              ‚Üì\n",
    "         Synthesis\n",
    "              ‚Üì\n",
    "    Final Response (logged to Braintrust)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install braintrust anthropic langgraph langchain langchain-anthropic wikipedia-api python-dotenv --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from typing import TypedDict, Annotated, List, Dict, Any\n",
    "import json\n",
    "\n",
    "# Braintrust imports\n",
    "import braintrust\n",
    "from braintrust import current_span, traced\n",
    "\n",
    "# LangChain and LangGraph imports\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "# Tool imports\n",
    "import wikipediaapi\n",
    "import operator\n",
    "\n",
    "print(\"‚úì All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configure Braintrust\n",
    "\n",
    "Initialize Braintrust with your API key. Get your key from: https://www.braintrust.dev/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API keys\n",
    "os.environ[\"BRAINTRUST_API_KEY\"] = \"your-braintrust-api-key-here\"\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-api-key-here\"\n",
    "\n",
    "# Initialize Braintrust project\n",
    "PROJECT_NAME = \"agentic-research-assistant\"\n",
    "\n",
    "print(f\"‚úì Braintrust configured for project: {PROJECT_NAME}\")\n",
    "print(f\"‚úì View results at: https://www.braintrust.dev/app/{PROJECT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Agent Tools\n",
    "\n",
    "Create tools that the agent can use, instrumented with Braintrust tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentTools:\n",
    "    \"\"\"Collection of tools available to the agent, instrumented with Braintrust.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.wiki = wikipediaapi.Wikipedia(\n",
    "            user_agent='ResearchAgent/1.0',\n",
    "            language='en'\n",
    "        )\n",
    "    \n",
    "    @traced\n",
    "    def search_wikipedia(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Search Wikipedia for information.\"\"\"\n",
    "        # Log inputs to Braintrust\n",
    "        current_span().log(input={\"query\": query, \"tool\": \"wikipedia\"})\n",
    "        \n",
    "        try:\n",
    "            page = self.wiki.page(query)\n",
    "            \n",
    "            if page.exists():\n",
    "                result = {\n",
    "                    \"title\": page.title,\n",
    "                    \"summary\": page.summary[:500],  # First 500 chars\n",
    "                    \"url\": page.fullurl,\n",
    "                    \"success\": True\n",
    "                }\n",
    "            else:\n",
    "                result = {\n",
    "                    \"error\": \"Page not found\",\n",
    "                    \"success\": False\n",
    "                }\n",
    "            \n",
    "            # Log outputs to Braintrust\n",
    "            current_span().log(\n",
    "                output=result,\n",
    "                metadata={\"query_length\": len(query)}\n",
    "            )\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_result = {\"error\": str(e), \"success\": False}\n",
    "            current_span().log(output=error_result)\n",
    "            return error_result\n",
    "    \n",
    "    @traced\n",
    "    def calculator(self, expression: str) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate mathematical expressions safely.\"\"\"\n",
    "        current_span().log(input={\"expression\": expression, \"tool\": \"calculator\"})\n",
    "        \n",
    "        try:\n",
    "            # Safe evaluation (restricted namespace)\n",
    "            allowed_names = {\"abs\": abs, \"round\": round, \"min\": min, \"max\": max}\n",
    "            result = eval(expression, {\"__builtins__\": {}}, allowed_names)\n",
    "            \n",
    "            output = {\n",
    "                \"result\": result,\n",
    "                \"success\": True\n",
    "            }\n",
    "            \n",
    "            current_span().log(\n",
    "                output=output,\n",
    "                metadata={\"expression_length\": len(expression)}\n",
    "            )\n",
    "            \n",
    "            return output\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_result = {\"error\": str(e), \"success\": False}\n",
    "            current_span().log(output=error_result)\n",
    "            return error_result\n",
    "    \n",
    "    @traced\n",
    "    def web_search_simulator(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Simulated web search (in production, use a real search API).\"\"\"\n",
    "        current_span().log(input={\"query\": query, \"tool\": \"web_search\"})\n",
    "        \n",
    "        # Simulated results\n",
    "        result = {\n",
    "            \"results\": [\n",
    "                {\"title\": f\"Result 1 for {query}\", \"snippet\": \"Relevant information...\"},\n",
    "                {\"title\": f\"Result 2 for {query}\", \"snippet\": \"More details...\"}\n",
    "            ],\n",
    "            \"success\": True\n",
    "        }\n",
    "        \n",
    "        current_span().log(\n",
    "            output=result,\n",
    "            metadata={\"num_results\": len(result[\"results\"])}\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "\n",
    "tools = AgentTools()\n",
    "print(\"‚úì Agent tools initialized with Braintrust tracing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Agent State and Graph\n",
    "\n",
    "Create a LangGraph-based agent with state management and tool execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"State of the research agent.\"\"\"\n",
    "    messages: Annotated[List, add_messages]\n",
    "    iterations: int\n",
    "    tool_calls: List[Dict[str, Any]]\n",
    "    final_answer: str\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=2000\n",
    ")\n",
    "\n",
    "# Tool definitions for Claude\n",
    "tool_definitions = [\n",
    "    {\n",
    "        \"name\": \"search_wikipedia\",\n",
    "        \"description\": \"Search Wikipedia for factual information on any topic.\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The search query or topic\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"calculator\",\n",
    "        \"description\": \"Evaluate mathematical expressions. Supports +, -, *, /, **, and common functions.\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"expression\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Mathematical expression to evaluate\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"expression\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"web_search_simulator\",\n",
    "        \"description\": \"Search the web for current information (simulated for demo).\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Search query\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"‚úì Agent state and LLM configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implement Agent Nodes\n",
    "\n",
    "Define the agent's reasoning, tool execution, and synthesis nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@traced(name=\"agent_reasoning\")\n",
    "def agent_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Main reasoning node - decides what to do next.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Log the current state to Braintrust\n",
    "    current_span().log(\n",
    "        input={\n",
    "            \"iteration\": state[\"iterations\"],\n",
    "            \"message_count\": len(messages),\n",
    "            \"tool_calls_so_far\": len(state.get(\"tool_calls\", []))\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Call LLM with tools\n",
    "    response = llm.invoke(\n",
    "        messages,\n",
    "        tools=tool_definitions\n",
    "    )\n",
    "    \n",
    "    # Log LLM decision\n",
    "    current_span().log(\n",
    "        output={\n",
    "            \"has_tool_calls\": hasattr(response, 'tool_calls') and len(response.tool_calls) > 0,\n",
    "            \"response_type\": response.__class__.__name__\n",
    "        },\n",
    "        metadata={\n",
    "            \"model\": \"claude-sonnet-4-20250514\",\n",
    "            \"iteration\": state[\"iterations\"]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "        \"iterations\": state[\"iterations\"] + 1\n",
    "    }\n",
    "\n",
    "@traced(name=\"tool_execution\")\n",
    "def tool_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Execute tools requested by the agent.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    tool_calls = state.get(\"tool_calls\", [])\n",
    "    tool_messages = []\n",
    "    \n",
    "    # Execute each tool call\n",
    "    if hasattr(last_message, 'tool_calls'):\n",
    "        for tool_call in last_message.tool_calls:\n",
    "            tool_name = tool_call[\"name\"]\n",
    "            tool_input = tool_call[\"input\"]\n",
    "            tool_id = tool_call[\"id\"]\n",
    "            \n",
    "            # Log tool execution start\n",
    "            current_span().log(\n",
    "                input={\n",
    "                    \"tool_name\": tool_name,\n",
    "                    \"tool_input\": tool_input\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Execute the tool\n",
    "            if tool_name == \"search_wikipedia\":\n",
    "                result = tools.search_wikipedia(tool_input[\"query\"])\n",
    "            elif tool_name == \"calculator\":\n",
    "                result = tools.calculator(tool_input[\"expression\"])\n",
    "            elif tool_name == \"web_search_simulator\":\n",
    "                result = tools.web_search_simulator(tool_input[\"query\"])\n",
    "            else:\n",
    "                result = {\"error\": f\"Unknown tool: {tool_name}\"}\n",
    "            \n",
    "            # Create tool message\n",
    "            tool_message = ToolMessage(\n",
    "                content=json.dumps(result),\n",
    "                tool_call_id=tool_id\n",
    "            )\n",
    "            tool_messages.append(tool_message)\n",
    "            \n",
    "            # Track tool call\n",
    "            tool_calls.append({\n",
    "                \"tool\": tool_name,\n",
    "                \"input\": tool_input,\n",
    "                \"output\": result,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            })\n",
    "    \n",
    "    # Log tool execution summary\n",
    "    current_span().log(\n",
    "        output={\n",
    "            \"tools_executed\": len(tool_messages),\n",
    "            \"total_tool_calls\": len(tool_calls)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"messages\": tool_messages,\n",
    "        \"tool_calls\": tool_calls\n",
    "    }\n",
    "\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    \"\"\"Decide whether to continue or end.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # If the LLM makes a tool call, execute tools\n",
    "    if hasattr(last_message, 'tool_calls') and len(last_message.tool_calls) > 0:\n",
    "        return \"continue\"\n",
    "    \n",
    "    # If max iterations reached, end\n",
    "    if state[\"iterations\"] >= 5:\n",
    "        return \"end\"\n",
    "    \n",
    "    # Otherwise, we have a final answer\n",
    "    return \"end\"\n",
    "\n",
    "print(\"‚úì Agent nodes defined with Braintrust tracing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build the Agent Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"agent\", agent_node)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# Add conditional edges\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"tools\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "# Tools always go back to agent\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# Compile the graph\n",
    "agent_graph = workflow.compile()\n",
    "\n",
    "print(\"‚úì Agent graph compiled successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Agent with Braintrust Logging\n",
    "\n",
    "Execute the agent and track everything in Braintrust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@traced(name=\"research_agent_execution\")\n",
    "def run_research_agent(query: str) -> Dict[str, Any]:\n",
    "    \"\"\"Run the research agent with full Braintrust tracking.\"\"\"\n",
    "    \n",
    "    # Log the input query\n",
    "    current_span().log(input={\"query\": query})\n",
    "    \n",
    "    # Initialize state\n",
    "    initial_state = {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=f\"\"\"You are a research assistant. Answer the following question using the tools available to you.\n",
    "            \n",
    "Question: {query}\n",
    "\n",
    "Think step by step:\n",
    "1. What information do you need?\n",
    "2. Which tools should you use?\n",
    "3. How will you synthesize the results?\n",
    "\n",
    "Provide a comprehensive answer.\"\"\")\n",
    "        ],\n",
    "        \"iterations\": 0,\n",
    "        \"tool_calls\": [],\n",
    "        \"final_answer\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Run the agent\n",
    "    final_state = agent_graph.invoke(initial_state)\n",
    "    \n",
    "    # Extract final answer\n",
    "    final_message = final_state[\"messages\"][-1]\n",
    "    final_answer = final_message.content if hasattr(final_message, 'content') else str(final_message)\n",
    "    \n",
    "    # Prepare result\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"answer\": final_answer,\n",
    "        \"iterations\": final_state[\"iterations\"],\n",
    "        \"tool_calls\": final_state.get(\"tool_calls\", []),\n",
    "        \"num_messages\": len(final_state[\"messages\"])\n",
    "    }\n",
    "    \n",
    "    # Log comprehensive output to Braintrust\n",
    "    current_span().log(\n",
    "        output=result,\n",
    "        metadata={\n",
    "            \"query_length\": len(query),\n",
    "            \"answer_length\": len(final_answer),\n",
    "            \"tools_used\": len(set(tc[\"tool\"] for tc in result[\"tool_calls\"]))\n",
    "        },\n",
    "        metrics={\n",
    "            \"iterations\": result[\"iterations\"],\n",
    "            \"tool_calls\": len(result[\"tool_calls\"]),\n",
    "            \"message_count\": result[\"num_messages\"]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"‚úì Agent execution function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test the Agent with Different Queries\n",
    "\n",
    "Run multiple queries and see how Braintrust tracks everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What is the capital of France and what's its population?\",\n",
    "    \"Calculate the compound interest on $10,000 at 5% annual rate for 10 years using the formula A = P(1 + r)^t\",\n",
    "    \"Tell me about the Eiffel Tower and when it was built\"\n",
    "]\n",
    "\n",
    "# Run experiments with Braintrust\n",
    "def run_experiments():\n",
    "    \"\"\"Run multiple experiments and log to Braintrust.\"\"\"\n",
    "    \n",
    "    with braintrust.init(project=PROJECT_NAME) as bt:\n",
    "        for i, query in enumerate(test_queries):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Query {i+1}: {query}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            # Run the agent\n",
    "            result = run_research_agent(query)\n",
    "            \n",
    "            # Log to Braintrust experiment\n",
    "            bt.log(\n",
    "                input={\"query\": query},\n",
    "                output={\"answer\": result[\"answer\"]},\n",
    "                metadata={\n",
    "                    \"iterations\": result[\"iterations\"],\n",
    "                    \"tool_calls\": len(result[\"tool_calls\"]),\n",
    "                    \"tools_used\": [tc[\"tool\"] for tc in result[\"tool_calls\"]]\n",
    "                },\n",
    "                metrics={\n",
    "                    \"iterations\": result[\"iterations\"],\n",
    "                    \"tool_count\": len(result[\"tool_calls\"])\n",
    "                },\n",
    "                scores={\n",
    "                    \"efficiency\": 1.0 / max(result[\"iterations\"], 1)  # Lower iterations = higher efficiency\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Display results\n",
    "            print(f\"\\nüìù Answer: {result['answer'][:200]}...\")\n",
    "            print(f\"\\nüìä Metrics:\")\n",
    "            print(f\"  - Iterations: {result['iterations']}\")\n",
    "            print(f\"  - Tool calls: {len(result['tool_calls'])}\")\n",
    "            print(f\"  - Tools used: {', '.join(set(tc['tool'] for tc in result['tool_calls']))}\")\n",
    "\n",
    "# Run if API keys are set\n",
    "if os.getenv(\"BRAINTRUST_API_KEY\", \"\").startswith(\"your-\"):\n",
    "    print(\"‚ö†Ô∏è  Please set your API keys in the configuration cell above before running experiments.\")\n",
    "else:\n",
    "    print(\"Running experiments with Braintrust tracking...\\n\")\n",
    "    run_experiments()\n",
    "    print(f\"\\n‚úÖ All experiments complete! View results at: https://www.braintrust.dev/app/{PROJECT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Custom Evaluation Metrics\n",
    "\n",
    "Define custom evaluators for agent performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braintrust import Eval\n",
    "\n",
    "def evaluate_answer_quality(output: str, expected: str = None) -> float:\n",
    "    \"\"\"Simple quality metric based on answer length and structure.\"\"\"\n",
    "    if not output:\n",
    "        return 0.0\n",
    "    \n",
    "    score = 0.0\n",
    "    \n",
    "    # Length check (reasonable answers are 50-1000 chars)\n",
    "    if 50 <= len(output) <= 1000:\n",
    "        score += 0.3\n",
    "    \n",
    "    # Contains multiple sentences\n",
    "    if output.count('.') >= 2:\n",
    "        score += 0.3\n",
    "    \n",
    "    # Not just an error message\n",
    "    if 'error' not in output.lower():\n",
    "        score += 0.2\n",
    "    \n",
    "    # Has numerical data if expected\n",
    "    if any(char.isdigit() for char in output):\n",
    "        score += 0.2\n",
    "    \n",
    "    return min(score, 1.0)\n",
    "\n",
    "def evaluate_efficiency(iterations: int, tool_calls: int) -> float:\n",
    "    \"\"\"Evaluate how efficiently the agent solved the problem.\"\"\"\n",
    "    # Perfect score for 1-2 iterations with minimal tools\n",
    "    efficiency = 1.0\n",
    "    \n",
    "    if iterations > 2:\n",
    "        efficiency -= (iterations - 2) * 0.1\n",
    "    \n",
    "    if tool_calls > 3:\n",
    "        efficiency -= (tool_calls - 3) * 0.05\n",
    "    \n",
    "    return max(efficiency, 0.0)\n",
    "\n",
    "def evaluate_tool_selection(tool_calls: List[Dict], query: str) -> float:\n",
    "    \"\"\"Evaluate whether appropriate tools were selected.\"\"\"\n",
    "    query_lower = query.lower()\n",
    "    tools_used = set(tc[\"tool\"] for tc in tool_calls)\n",
    "    \n",
    "    score = 0.5  # Base score for making any tool call\n",
    "    \n",
    "    # Check if appropriate tools were used\n",
    "    if \"calculate\" in query_lower or \"math\" in query_lower:\n",
    "        if \"calculator\" in tools_used:\n",
    "            score += 0.5\n",
    "    \n",
    "    if any(word in query_lower for word in [\"what is\", \"tell me about\", \"who is\"]):\n",
    "        if \"search_wikipedia\" in tools_used:\n",
    "            score += 0.3\n",
    "    \n",
    "    return min(score, 1.0)\n",
    "\n",
    "print(\"‚úì Custom evaluation metrics defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Run Comprehensive Evaluation\n",
    "\n",
    "Evaluate agent performance across multiple dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation():\n",
    "    \"\"\"Run comprehensive evaluation with custom metrics.\"\"\"\n",
    "    \n",
    "    evaluation_queries = [\n",
    "        {\n",
    "            \"query\": \"What is machine learning?\",\n",
    "            \"expected_tools\": [\"search_wikipedia\"]\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"Calculate 15% of 8500\",\n",
    "            \"expected_tools\": [\"calculator\"]\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"Who invented the telephone and when?\",\n",
    "            \"expected_tools\": [\"search_wikipedia\"]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    with braintrust.init(project=PROJECT_NAME, experiment=\"comprehensive-eval\") as bt:\n",
    "        for test_case in evaluation_queries:\n",
    "            query = test_case[\"query\"]\n",
    "            \n",
    "            # Run agent\n",
    "            result = run_research_agent(query)\n",
    "            \n",
    "            # Calculate scores\n",
    "            quality_score = evaluate_answer_quality(result[\"answer\"])\n",
    "            efficiency_score = evaluate_efficiency(\n",
    "                result[\"iterations\"],\n",
    "                len(result[\"tool_calls\"])\n",
    "            )\n",
    "            tool_selection_score = evaluate_tool_selection(\n",
    "                result[\"tool_calls\"],\n",
    "                query\n",
    "            )\n",
    "            \n",
    "            # Overall score (weighted average)\n",
    "            overall_score = (\n",
    "                quality_score * 0.5 +\n",
    "                efficiency_score * 0.3 +\n",
    "                tool_selection_score * 0.2\n",
    "            )\n",
    "            \n",
    "            # Log to Braintrust\n",
    "            bt.log(\n",
    "                input={\"query\": query},\n",
    "                output={\"answer\": result[\"answer\"]},\n",
    "                scores={\n",
    "                    \"overall\": overall_score,\n",
    "                    \"quality\": quality_score,\n",
    "                    \"efficiency\": efficiency_score,\n",
    "                    \"tool_selection\": tool_selection_score\n",
    "                },\n",
    "                metadata={\n",
    "                    \"iterations\": result[\"iterations\"],\n",
    "                    \"tool_calls\": len(result[\"tool_calls\"]),\n",
    "                    \"expected_tools\": test_case[\"expected_tools\"]\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            print(f\"\\nQuery: {query}\")\n",
    "            print(f\"Scores - Overall: {overall_score:.2f} | Quality: {quality_score:.2f} | Efficiency: {efficiency_score:.2f} | Tool Selection: {tool_selection_score:.2f}\")\n",
    "\n",
    "if not os.getenv(\"BRAINTRUST_API_KEY\", \"\").startswith(\"your-\"):\n",
    "    run_evaluation()\n",
    "    print(f\"\\n‚úÖ Evaluation complete! View detailed results at: https://www.braintrust.dev/app/{PROJECT_NAME}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Set API keys to run evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Analyze Results in Braintrust Dashboard\n",
    "\n",
    "After running the experiments, you can view comprehensive analytics in the Braintrust dashboard:\n",
    "\n",
    "### What You'll See:\n",
    "\n",
    "1. **Trace Timeline**: Visual representation of agent execution flow\n",
    "2. **Tool Usage Patterns**: Which tools were called and when\n",
    "3. **Performance Metrics**: Latency, token usage, iteration counts\n",
    "4. **Score Distributions**: How your agent performs across different queries\n",
    "5. **Error Tracking**: Any failures or issues during execution\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "- **Efficiency**: Are iterations and tool calls optimal?\n",
    "- **Accuracy**: Does the agent select appropriate tools?\n",
    "- **Quality**: Are the answers comprehensive and correct?\n",
    "- **Consistency**: How does performance vary across similar queries?\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Compare different prompting strategies\n",
    "2. A/B test different models or temperatures\n",
    "3. Identify failure patterns\n",
    "4. Optimize tool selection logic\n",
    "5. Set up alerts for performance degradation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "‚úÖ **Braintrust Integration**: Comprehensive tracking of agent behavior\n",
    "‚úÖ **Agentic Design**: Multi-tool agent with reasoning and tool selection\n",
    "‚úÖ **Custom Metrics**: Evaluation of quality, efficiency, and tool selection\n",
    "‚úÖ **Production Patterns**: Proper error handling, logging, and state management\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Observability is crucial** for understanding and improving agent behavior\n",
    "2. **@traced decorator** makes it easy to instrument any function\n",
    "3. **Custom metrics** help evaluate agent performance beyond simple accuracy\n",
    "4. **Braintrust dashboard** provides powerful visualization and analysis tools\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- [Braintrust Documentation](https://www.braintrust.dev/docs)\n",
    "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
    "- [Anthropic API Reference](https://docs.anthropic.com/)\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to build production agent systems with full observability! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
