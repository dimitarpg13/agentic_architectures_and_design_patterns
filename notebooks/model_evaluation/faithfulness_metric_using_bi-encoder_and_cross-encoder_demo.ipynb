{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MLflow Faithfulness Metric Demonstration\n",
        "\n",
        "This notebook provides a comprehensive guide to using MLflow's Faithfulness metric for evaluating RAG (Retrieval-Augmented Generation) systems and other context-grounded text generation applications.\n",
        "\n",
        "## What is Faithfulness?\n",
        "\n",
        "Faithfulness measures whether a generated answer is factually consistent with and supported by the provided context/source documents. It's a critical metric for detecting hallucinations in RAG systems.\n",
        "\n",
        "### Key Characteristics:\n",
        "- **Context-grounded**: Evaluates if the answer is supported by the given context\n",
        "- **Hallucination detection**: Identifies when models generate unsupported information\n",
        "- **Range**: Typically 0-1, where 1 means fully faithful to context\n",
        "- **Use cases**: RAG pipelines, document QA, summarization with sources\n",
        "\n",
        "### Why Faithfulness Matters:\n",
        "| Problem | Impact | Faithfulness Helps |\n",
        "|---------|--------|-------------------|\n",
        "| **Hallucination** | Model invents facts | Detects unsupported claims |\n",
        "| **Misattribution** | Wrong source cited | Verifies context alignment |\n",
        "| **Extrapolation** | Goes beyond context | Flags additions |\n",
        "| **Contradiction** | Conflicts with source | Identifies inconsistencies |\n",
        "\n",
        "### Faithfulness vs Other Metrics:\n",
        "| Metric | What it Measures | Best For |\n",
        "|--------|-----------------|----------|\n",
        "| **Faithfulness** | Answer supported by context | RAG, grounded generation |\n",
        "| **Answer Similarity** | Semantic match to reference | QA evaluation |\n",
        "| **Relevance** | Answer addresses question | Response quality |\n",
        "| **ROUGE/BLEU** | Text overlap | Summarization, translation |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Installation and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q mlflow sentence-transformers pandas numpy matplotlib seaborn plotly scikit-learn transformers torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%restart_python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import mlflow.metrics\n",
        "from mlflow.metrics import make_metric, MetricValue\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Sentence embeddings for semantic similarity\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "\n",
        "print(f\"MLflow version: {mlflow.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_email = (\n",
        "    dbutils.notebook.entry_point.getDbutils()\n",
        "    .notebook()\n",
        "    .getContext()\n",
        "    .tags()\n",
        "    .get(\"user\")\n",
        "    .get()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Creating Custom Faithfulness Metrics for MLflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FaithfulnessMetrics:\n",
        "    \"\"\"\n",
        "    Comprehensive faithfulness metrics for MLflow evaluation.\n",
        "    Measures whether generated answers are supported by the provided context.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, \n",
        "                 embedding_model: str = \"all-MiniLM-L6-v2\",\n",
        "                 nli_model: str = \"cross-encoder/nli-deberta-v3-small\"):\n",
        "        \"\"\"\n",
        "        Initialize with embedding and NLI models.\n",
        "        \n",
        "        Args:\n",
        "            embedding_model: Sentence transformer model for semantic similarity\n",
        "            nli_model: Cross-encoder model for natural language inference\n",
        "        \"\"\"\n",
        "        print(f\"Loading embedding model: {embedding_model}...\")\n",
        "        self.embedding_model = SentenceTransformer(embedding_model)\n",
        "        self.embedding_model_name = embedding_model\n",
        "        \n",
        "        print(f\"Loading NLI model: {nli_model}...\")\n",
        "        self.nli_model = CrossEncoder(nli_model)\n",
        "        self.nli_model_name = nli_model\n",
        "        \n",
        "        print(\"âœ… Models loaded successfully!\")\n",
        "\n",
        "    @staticmethod\n",
        "    def _to_string_list(data):\n",
        "        \"\"\"Convert input data to a list of strings.\"\"\"\n",
        "        if hasattr(data, 'tolist'):\n",
        "            return [str(x) for x in data.tolist()]\n",
        "        if hasattr(data, '__iter__') and not isinstance(data, str):\n",
        "            return [str(x) for x in data]\n",
        "        return [str(data)]\n",
        "\n",
        "    @staticmethod\n",
        "    def split_into_claims(text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Split text into individual claims/sentences for verification.\n",
        "        \n",
        "        Args:\n",
        "            text: The answer text to split\n",
        "            \n",
        "        Returns:\n",
        "            List of individual claims/sentences\n",
        "        \"\"\"\n",
        "        # Split on sentence boundaries\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
        "        # Filter out empty strings and very short fragments\n",
        "        claims = [s.strip() for s in sentences if len(s.strip()) > 10]\n",
        "        return claims if claims else [text]\n",
        "\n",
        "    def compute_semantic_faithfulness(self, answer: str, context: str) -> float:\n",
        "        \"\"\"\n",
        "        Compute faithfulness using semantic similarity between answer and context.\n",
        "        \n",
        "        Args:\n",
        "            answer: Generated answer\n",
        "            context: Source context/document\n",
        "            \n",
        "        Returns:\n",
        "            Faithfulness score between 0 and 1\n",
        "        \"\"\"\n",
        "        # Encode both texts\n",
        "        answer_embedding = self.embedding_model.encode([answer])[0]\n",
        "        context_embedding = self.embedding_model.encode([context])[0]\n",
        "        \n",
        "        # Compute cosine similarity\n",
        "        similarity = cosine_similarity([answer_embedding], [context_embedding])[0][0]\n",
        "        \n",
        "        # Normalize to 0-1 range\n",
        "        return max(0, min(1, (similarity + 1) / 2))\n",
        "\n",
        "    def compute_nli_faithfulness(self, answer: str, context: str) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Compute faithfulness using Natural Language Inference.\n",
        "        Checks if the context entails the answer.\n",
        "        \n",
        "        Args:\n",
        "            answer: Generated answer\n",
        "            context: Source context/document\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary with entailment, neutral, and contradiction scores\n",
        "        \"\"\"\n",
        "        # NLI models expect (premise, hypothesis) pairs\n",
        "        # Here: context is premise, answer is hypothesis\n",
        "        scores = self.nli_model.predict([(context, answer)])[0]\n",
        "        \n",
        "        # Most NLI models return [contradiction, neutral, entailment] or similar\n",
        "        # The cross-encoder returns a single score for entailment likelihood\n",
        "        if isinstance(scores, (int, float)):\n",
        "            # Single score model - treat as entailment score\n",
        "            entailment_score = 1 / (1 + np.exp(-scores))  # sigmoid\n",
        "            return {\n",
        "                \"entailment\": entailment_score,\n",
        "                \"faithfulness\": entailment_score\n",
        "            }\n",
        "        else:\n",
        "            # Multi-class model\n",
        "            return {\n",
        "                \"contradiction\": float(scores[0]),\n",
        "                \"neutral\": float(scores[1]),\n",
        "                \"entailment\": float(scores[2]),\n",
        "                \"faithfulness\": float(scores[2])  # Use entailment as faithfulness\n",
        "            }\n",
        "\n",
        "    def compute_claim_faithfulness(self, answer: str, context: str) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Compute faithfulness by verifying individual claims in the answer.\n",
        "        \n",
        "        Args:\n",
        "            answer: Generated answer\n",
        "            context: Source context/document\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary with per-claim and aggregate faithfulness scores\n",
        "        \"\"\"\n",
        "        claims = self.split_into_claims(answer)\n",
        "        \n",
        "        if not claims:\n",
        "            return {\"faithfulness\": 1.0, \"num_claims\": 0, \"supported_claims\": 0}\n",
        "        \n",
        "        claim_scores = []\n",
        "        for claim in claims:\n",
        "            # Check if this claim is supported by context using NLI\n",
        "            nli_result = self.compute_nli_faithfulness(claim, context)\n",
        "            claim_scores.append(nli_result[\"faithfulness\"])\n",
        "        \n",
        "        return {\n",
        "            \"faithfulness\": np.mean(claim_scores),\n",
        "            \"min_claim_score\": np.min(claim_scores),\n",
        "            \"max_claim_score\": np.max(claim_scores),\n",
        "            \"num_claims\": len(claims),\n",
        "            \"supported_claims\": sum(1 for s in claim_scores if s >= 0.5),\n",
        "            \"claim_scores\": claim_scores\n",
        "        }\n",
        "\n",
        "    def compute_token_overlap_faithfulness(self, answer: str, context: str) -> float:\n",
        "        \"\"\"\n",
        "        Simple token overlap based faithfulness (baseline).\n",
        "        \n",
        "        Args:\n",
        "            answer: Generated answer\n",
        "            context: Source context/document\n",
        "            \n",
        "        Returns:\n",
        "            Proportion of answer tokens found in context\n",
        "        \"\"\"\n",
        "        # Tokenize (simple word-based)\n",
        "        answer_tokens = set(answer.lower().split())\n",
        "        context_tokens = set(context.lower().split())\n",
        "        \n",
        "        # Remove common stop words for better signal\n",
        "        stop_words = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', \n",
        "                      'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will',\n",
        "                      'would', 'could', 'should', 'may', 'might', 'must', 'shall',\n",
        "                      'can', 'need', 'dare', 'ought', 'used', 'to', 'of', 'in',\n",
        "                      'for', 'on', 'with', 'at', 'by', 'from', 'as', 'into',\n",
        "                      'through', 'during', 'before', 'after', 'above', 'below',\n",
        "                      'between', 'under', 'again', 'further', 'then', 'once',\n",
        "                      'and', 'but', 'or', 'nor', 'so', 'yet', 'both', 'either',\n",
        "                      'neither', 'not', 'only', 'own', 'same', 'than', 'too',\n",
        "                      'very', 'just', 'also', 'now', 'it', 'its', 'this', 'that'}\n",
        "        \n",
        "        answer_content = answer_tokens - stop_words\n",
        "        context_content = context_tokens - stop_words\n",
        "        \n",
        "        if not answer_content:\n",
        "            return 1.0  # Empty answer is technically faithful\n",
        "        \n",
        "        overlap = answer_content.intersection(context_content)\n",
        "        return len(overlap) / len(answer_content)\n",
        "\n",
        "    def create_semantic_faithfulness_metric(self):\n",
        "        \"\"\"\n",
        "        Create MLflow metric for semantic similarity-based faithfulness.\n",
        "        \"\"\"\n",
        "        def semantic_faithfulness_metric(predictions, targets, metrics):\n",
        "            predictions = self._to_string_list(predictions)\n",
        "            # For faithfulness, targets should contain the context\n",
        "            contexts = self._to_string_list(targets)\n",
        "\n",
        "            scores = [\n",
        "                self.compute_semantic_faithfulness(pred, ctx)\n",
        "                for pred, ctx in zip(predictions, contexts)\n",
        "            ]\n",
        "\n",
        "            return MetricValue(\n",
        "                aggregate_results={\n",
        "                    \"semantic_faithfulness_mean\": np.mean(scores),\n",
        "                    \"semantic_faithfulness_std\": np.std(scores),\n",
        "                    \"semantic_faithfulness_min\": np.min(scores),\n",
        "                    \"semantic_faithfulness_max\": np.max(scores)\n",
        "                },\n",
        "                scores=scores\n",
        "            )\n",
        "\n",
        "        return make_metric(\n",
        "            eval_fn=semantic_faithfulness_metric,\n",
        "            greater_is_better=True,\n",
        "            name=\"semantic_faithfulness\"\n",
        "        )\n",
        "\n",
        "    def create_nli_faithfulness_metric(self):\n",
        "        \"\"\"\n",
        "        Create MLflow metric for NLI-based faithfulness.\n",
        "        \"\"\"\n",
        "        def nli_faithfulness_metric(predictions, targets, metrics):\n",
        "            predictions = self._to_string_list(predictions)\n",
        "            contexts = self._to_string_list(targets)\n",
        "\n",
        "            scores = []\n",
        "            entailment_scores = []\n",
        "            \n",
        "            for pred, ctx in zip(predictions, contexts):\n",
        "                result = self.compute_nli_faithfulness(pred, ctx)\n",
        "                scores.append(result[\"faithfulness\"])\n",
        "                entailment_scores.append(result.get(\"entailment\", result[\"faithfulness\"]))\n",
        "\n",
        "            return MetricValue(\n",
        "                aggregate_results={\n",
        "                    \"nli_faithfulness_mean\": np.mean(scores),\n",
        "                    \"nli_faithfulness_std\": np.std(scores),\n",
        "                    \"entailment_mean\": np.mean(entailment_scores),\n",
        "                    \"high_faithfulness_ratio\": sum(1 for s in scores if s >= 0.7) / len(scores)\n",
        "                },\n",
        "                scores=scores\n",
        "            )\n",
        "\n",
        "        return make_metric(\n",
        "            eval_fn=nli_faithfulness_metric,\n",
        "            greater_is_better=True,\n",
        "            name=\"nli_faithfulness\"\n",
        "        )\n",
        "\n",
        "    def create_token_overlap_metric(self):\n",
        "        \"\"\"\n",
        "        Create MLflow metric for token overlap faithfulness (baseline).\n",
        "        \"\"\"\n",
        "        def token_overlap_metric(predictions, targets, metrics):\n",
        "            predictions = self._to_string_list(predictions)\n",
        "            contexts = self._to_string_list(targets)\n",
        "\n",
        "            scores = [\n",
        "                self.compute_token_overlap_faithfulness(pred, ctx)\n",
        "                for pred, ctx in zip(predictions, contexts)\n",
        "            ]\n",
        "\n",
        "            return MetricValue(\n",
        "                aggregate_results={\n",
        "                    \"token_overlap_mean\": np.mean(scores),\n",
        "                    \"token_overlap_std\": np.std(scores),\n",
        "                    \"token_overlap_min\": np.min(scores),\n",
        "                    \"token_overlap_max\": np.max(scores)\n",
        "                },\n",
        "                scores=scores\n",
        "            )\n",
        "\n",
        "        return make_metric(\n",
        "            eval_fn=token_overlap_metric,\n",
        "            greater_is_better=True,\n",
        "            name=\"token_overlap_faithfulness\"\n",
        "        )\n",
        "\n",
        "    def create_comprehensive_faithfulness_metric(self):\n",
        "        \"\"\"\n",
        "        Create comprehensive metric combining multiple faithfulness approaches.\n",
        "        \"\"\"\n",
        "        def comprehensive_metric(predictions, targets, metrics):\n",
        "            predictions = self._to_string_list(predictions)\n",
        "            contexts = self._to_string_list(targets)\n",
        "\n",
        "            semantic_scores = []\n",
        "            nli_scores = []\n",
        "            overlap_scores = []\n",
        "            combined_scores = []\n",
        "\n",
        "            for pred, ctx in zip(predictions, contexts):\n",
        "                sem = self.compute_semantic_faithfulness(pred, ctx)\n",
        "                nli = self.compute_nli_faithfulness(pred, ctx)[\"faithfulness\"]\n",
        "                overlap = self.compute_token_overlap_faithfulness(pred, ctx)\n",
        "                \n",
        "                semantic_scores.append(sem)\n",
        "                nli_scores.append(nli)\n",
        "                overlap_scores.append(overlap)\n",
        "                \n",
        "                # Weighted combination (NLI most important, then semantic, then overlap)\n",
        "                combined = 0.5 * nli + 0.35 * sem + 0.15 * overlap\n",
        "                combined_scores.append(combined)\n",
        "\n",
        "            aggregate_results = {\n",
        "                # Combined score\n",
        "                \"faithfulness_combined\": np.mean(combined_scores),\n",
        "                \"faithfulness_std\": np.std(combined_scores),\n",
        "                \n",
        "                # Individual approaches\n",
        "                \"semantic_faithfulness\": np.mean(semantic_scores),\n",
        "                \"nli_faithfulness\": np.mean(nli_scores),\n",
        "                \"token_overlap\": np.mean(overlap_scores),\n",
        "                \n",
        "                # Quality distribution\n",
        "                \"high_faithfulness_count\": sum(1 for s in combined_scores if s >= 0.8),\n",
        "                \"medium_faithfulness_count\": sum(1 for s in combined_scores if 0.5 <= s < 0.8),\n",
        "                \"low_faithfulness_count\": sum(1 for s in combined_scores if s < 0.5),\n",
        "                \n",
        "                # Percentages\n",
        "                \"high_faithfulness_pct\": sum(1 for s in combined_scores if s >= 0.8) / len(combined_scores) * 100,\n",
        "                \"low_faithfulness_pct\": sum(1 for s in combined_scores if s < 0.5) / len(combined_scores) * 100\n",
        "            }\n",
        "\n",
        "            return MetricValue(\n",
        "                aggregate_results=aggregate_results,\n",
        "                scores=combined_scores\n",
        "            )\n",
        "\n",
        "        return make_metric(\n",
        "            eval_fn=comprehensive_metric,\n",
        "            greater_is_better=True,\n",
        "            name=\"comprehensive_faithfulness\"\n",
        "        )\n",
        "\n",
        "# Initialize the metrics\n",
        "faithfulness_metrics = FaithfulnessMetrics()\n",
        "\n",
        "# Create metric instances\n",
        "semantic_faithfulness = faithfulness_metrics.create_semantic_faithfulness_metric()\n",
        "nli_faithfulness = faithfulness_metrics.create_nli_faithfulness_metric()\n",
        "token_overlap = faithfulness_metrics.create_token_overlap_metric()\n",
        "comprehensive_faithfulness = faithfulness_metrics.create_comprehensive_faithfulness_metric()\n",
        "\n",
        "print(\"\\nâœ… Custom Faithfulness metrics created successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample RAG data for evaluation\n",
        "# Each example has: question, context, and different quality answers\n",
        "\n",
        "rag_data = [\n",
        "    {\n",
        "        \"question\": \"What is machine learning?\",\n",
        "        \"context\": \"Machine learning is a subset of artificial intelligence that enables computers to learn from data without being explicitly programmed. It uses algorithms to identify patterns in data and make predictions or decisions.\",\n",
        "        \"faithful_answer\": \"Machine learning is a subset of AI that allows computers to learn from data without explicit programming, using algorithms to find patterns.\",\n",
        "        \"partial_faithful\": \"Machine learning is a type of AI that helps computers learn. It's used in many applications like image recognition.\",\n",
        "        \"unfaithful_answer\": \"Machine learning was invented by Alan Turing in 1950 and requires quantum computers to function properly.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What causes climate change?\",\n",
        "        \"context\": \"Climate change is primarily driven by human activities, especially the burning of fossil fuels like coal, oil, and natural gas. These activities release greenhouse gases, particularly carbon dioxide, into the atmosphere, trapping heat and raising global temperatures.\",\n",
        "        \"faithful_answer\": \"Climate change is mainly caused by human activities, particularly burning fossil fuels which release greenhouse gases like carbon dioxide that trap heat.\",\n",
        "        \"partial_faithful\": \"Climate change is caused by greenhouse gases. The temperature is rising globally.\",\n",
        "        \"unfaithful_answer\": \"Climate change is primarily caused by changes in Earth's orbit and volcanic activity, with human impact being minimal.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How does photosynthesis work?\",\n",
        "        \"context\": \"Photosynthesis is the process by which plants convert light energy, usually from the sun, into chemical energy stored in glucose. Plants absorb carbon dioxide from the air and water from the soil, using chlorophyll to capture light energy and produce glucose and oxygen.\",\n",
        "        \"faithful_answer\": \"Plants use photosynthesis to convert light energy into glucose by absorbing CO2 and water, using chlorophyll to capture light and producing glucose and oxygen.\",\n",
        "        \"partial_faithful\": \"Photosynthesis converts light into food for plants. Plants need sunlight to grow.\",\n",
        "        \"unfaithful_answer\": \"Photosynthesis is how plants breathe oxygen and release carbon dioxide at night through their roots.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the capital of France?\",\n",
        "        \"context\": \"France is a country in Western Europe. Its capital city is Paris, which is also its largest city with a population of about 2.2 million in the city proper. Paris is known for the Eiffel Tower and the Louvre Museum.\",\n",
        "        \"faithful_answer\": \"The capital of France is Paris, which is also its largest city with about 2.2 million people, known for the Eiffel Tower and Louvre Museum.\",\n",
        "        \"partial_faithful\": \"Paris is the capital of France. It's a beautiful city.\",\n",
        "        \"unfaithful_answer\": \"The capital of France is Lyon, which became the capital in 2010 after a national referendum.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How does the immune system work?\",\n",
        "        \"context\": \"The immune system protects the body against pathogens like bacteria and viruses. It consists of white blood cells, antibodies, and other components. When a pathogen enters the body, the immune system recognizes it as foreign and mounts a response to eliminate it.\",\n",
        "        \"faithful_answer\": \"The immune system protects against pathogens using white blood cells and antibodies. It recognizes foreign invaders and eliminates them.\",\n",
        "        \"partial_faithful\": \"The immune system fights diseases. White blood cells are important for health.\",\n",
        "        \"unfaithful_answer\": \"The immune system is located primarily in the brain and strengthens automatically with age without any external factors.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is DNA?\",\n",
        "        \"context\": \"DNA (deoxyribonucleic acid) is a molecule that carries genetic instructions for living organisms. It has a double helix structure discovered by Watson and Crick. DNA contains four bases: adenine, thymine, guanine, and cytosine.\",\n",
        "        \"faithful_answer\": \"DNA is a molecule carrying genetic instructions, with a double helix structure. It contains four bases: adenine, thymine, guanine, and cytosine.\",\n",
        "        \"partial_faithful\": \"DNA contains genetic information. It's found in all living things.\",\n",
        "        \"unfaithful_answer\": \"DNA is a type of protein that was discovered in 1990. It has a single helix structure and contains only two bases.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_rag = pd.DataFrame(rag_data)\n",
        "\n",
        "# Calculate faithfulness scores for initial analysis\n",
        "print(\"ðŸ“Š Computing initial faithfulness scores...\")\n",
        "\n",
        "for answer_type in ['faithful_answer', 'partial_faithful', 'unfaithful_answer']:\n",
        "    df_rag[f'{answer_type}_semantic'] = [\n",
        "        faithfulness_metrics.compute_semantic_faithfulness(row[answer_type], row['context'])\n",
        "        for _, row in df_rag.iterrows()\n",
        "    ]\n",
        "    df_rag[f'{answer_type}_nli'] = [\n",
        "        faithfulness_metrics.compute_nli_faithfulness(row[answer_type], row['context'])['faithfulness']\n",
        "        for _, row in df_rag.iterrows()\n",
        "    ]\n",
        "    df_rag[f'{answer_type}_overlap'] = [\n",
        "        faithfulness_metrics.compute_token_overlap_faithfulness(row[answer_type], row['context'])\n",
        "        for _, row in df_rag.iterrows()\n",
        "    ]\n",
        "\n",
        "print(f\"\\nðŸ“Š Created {len(df_rag)} RAG examples\")\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"Sample Data Preview:\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "for idx, row in df_rag.head(3).iterrows():\n",
        "    print(f\"\\n{idx+1}. Question: {row['question']}\")\n",
        "    print(f\"   Context: {row['context'][:80]}...\")\n",
        "    print(f\"\\n   Faithful Answer: {row['faithful_answer'][:60]}...\")\n",
        "    print(f\"   â†’ Semantic: {row['faithful_answer_semantic']:.3f}, NLI: {row['faithful_answer_nli']:.3f}\")\n",
        "    print(f\"\\n   Partial Faithful: {row['partial_faithful'][:60]}...\")\n",
        "    print(f\"   â†’ Semantic: {row['partial_faithful_semantic']:.3f}, NLI: {row['partial_faithful_nli']:.3f}\")\n",
        "    print(f\"\\n   Unfaithful: {row['unfaithful_answer'][:60]}...\")\n",
        "    print(f\"   â†’ Semantic: {row['unfaithful_answer_semantic']:.3f}, NLI: {row['unfaithful_answer_nli']:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Comprehensive Results Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison table\n",
        "comparison_data = []\n",
        "\n",
        "for model_name, results in evaluation_results.items():\n",
        "    detailed = results[\"detailed\"]\n",
        "    row = {\n",
        "        \"Model\": model_name,\n",
        "        \"Faithfulness (Combined)\": detailed[\"faithfulness_mean\"],\n",
        "        \"Semantic\": detailed[\"semantic_mean\"],\n",
        "        \"NLI\": detailed[\"nli_mean\"],\n",
        "        \"Token Overlap\": detailed[\"overlap_mean\"],\n",
        "        \"High Quality\": int(detailed[\"high_faithfulness\"]),\n",
        "        \"Low Quality\": int(detailed[\"low_faithfulness\"]),\n",
        "        \"Hallucinations\": int(detailed[\"potential_hallucinations\"])\n",
        "    }\n",
        "    comparison_data.append(row)\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "print(\"\\nðŸ“Š Model Comparison Results:\")\n",
        "print(\"=\"*120)\n",
        "\n",
        "# Format percentages for display\n",
        "display_df = comparison_df.copy()\n",
        "for col in [\"Faithfulness (Combined)\", \"Semantic\", \"NLI\", \"Token Overlap\"]:\n",
        "    display_df[col] = display_df[col].apply(lambda x: f\"{x*100:.1f}%\")\n",
        "\n",
        "print(display_df.to_string(index=False))\n",
        "\n",
        "# Detailed analysis\n",
        "print(\"\\n\\nðŸ“ˆ Detailed Faithfulness Analysis:\")\n",
        "print(\"=\"*120)\n",
        "\n",
        "for model_name, results in evaluation_results.items():\n",
        "    detailed = results[\"detailed\"]\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    print(f\"  Combined Faithfulness: {detailed['faithfulness_mean']*100:.1f}% (Ïƒ={detailed['faithfulness_std']*100:.1f}%)\")\n",
        "    print(f\"  Range: [{detailed['faithfulness_min']*100:.1f}%, {detailed['faithfulness_max']*100:.1f}%]\")\n",
        "    print(f\"  Quality Distribution: High={detailed['high_faithfulness']}, Medium={detailed['medium_faithfulness']}, Low={detailed['low_faithfulness']}\")\n",
        "    print(f\"  Potential Hallucinations: {detailed['potential_hallucinations']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Advanced Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive visualizations with Plotly\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=2,\n",
        "    subplot_titles=(\n",
        "        \"Faithfulness Scores by Model\",\n",
        "        \"Metric Comparison (Semantic vs NLI vs Overlap)\",\n",
        "        \"Per-Question Faithfulness Heatmap\",\n",
        "        \"Quality Distribution\"\n",
        "    ),\n",
        "    specs=[\n",
        "        [{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
        "        [{\"type\": \"heatmap\"}, {\"type\": \"pie\"}]\n",
        "    ]\n",
        ")\n",
        "\n",
        "colors = {\n",
        "    \"High-Faithfulness-RAG\": \"#2ecc71\",\n",
        "    \"Medium-Faithfulness-RAG\": \"#f39c12\",\n",
        "    \"Low-Faithfulness-Baseline\": \"#e74c3c\"\n",
        "}\n",
        "\n",
        "# Plot 1: Overall Faithfulness Scores\n",
        "model_names = list(evaluation_results.keys())\n",
        "faithfulness_scores = [evaluation_results[m][\"detailed\"][\"faithfulness_mean\"] * 100 for m in model_names]\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Bar(\n",
        "        name=\"Combined Faithfulness\",\n",
        "        x=[m.replace(\"-\", \" \") for m in model_names],\n",
        "        y=faithfulness_scores,\n",
        "        marker_color=[colors[m] for m in model_names],\n",
        "        text=[f\"{s:.1f}%\" for s in faithfulness_scores],\n",
        "        textposition='outside'\n",
        "    ),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# Plot 2: Metric Comparison (grouped bar)\n",
        "metrics_to_plot = [\"semantic_mean\", \"nli_mean\", \"overlap_mean\"]\n",
        "metric_labels = [\"Semantic\", \"NLI\", \"Token Overlap\"]\n",
        "bar_colors = [\"#3498db\", \"#9b59b6\", \"#1abc9c\"]\n",
        "\n",
        "for i, (metric, label, color) in enumerate(zip(metrics_to_plot, metric_labels, bar_colors)):\n",
        "    values = [evaluation_results[m][\"detailed\"][metric] * 100 for m in model_names]\n",
        "    fig.add_trace(\n",
        "        go.Bar(\n",
        "            name=label,\n",
        "            x=[m.replace(\"-\", \" \") for m in model_names],\n",
        "            y=values,\n",
        "            marker_color=color,\n",
        "            text=[f\"{v:.1f}%\" for v in values],\n",
        "            textposition='outside'\n",
        "        ),\n",
        "        row=1, col=2\n",
        "    )\n",
        "\n",
        "# Plot 3: Heatmap of per-question faithfulness\n",
        "heatmap_data = []\n",
        "questions_short = [q[:30] + \"...\" for q in df_rag[\"question\"].tolist()]\n",
        "\n",
        "for answer_type, model_name in [\n",
        "    (\"faithful_answer\", \"High\"),\n",
        "    (\"partial_faithful\", \"Medium\"),\n",
        "    (\"unfaithful_answer\", \"Low\")\n",
        "]:\n",
        "    scores = []\n",
        "    for _, row in df_rag.iterrows():\n",
        "        combined = (\n",
        "            0.5 * row[f'{answer_type}_nli'] +\n",
        "            0.35 * row[f'{answer_type}_semantic'] +\n",
        "            0.15 * row[f'{answer_type}_overlap']\n",
        "        )\n",
        "        scores.append(combined)\n",
        "    heatmap_data.append(scores)\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Heatmap(\n",
        "        z=heatmap_data,\n",
        "        x=[f\"Q{i+1}\" for i in range(len(df_rag))],\n",
        "        y=[\"High\", \"Medium\", \"Low\"],\n",
        "        colorscale=[[0, \"#e74c3c\"], [0.5, \"#f39c12\"], [1, \"#2ecc71\"]],\n",
        "        text=np.round(np.array(heatmap_data), 2),\n",
        "        texttemplate=\"%{text}\",\n",
        "        textfont={\"size\": 10},\n",
        "        zmin=0,\n",
        "        zmax=1\n",
        "    ),\n",
        "    row=2, col=1\n",
        ")\n",
        "\n",
        "# Plot 4: Pie chart showing quality distribution for all answers combined\n",
        "all_combined = []\n",
        "for answer_type in ['faithful_answer', 'partial_faithful', 'unfaithful_answer']:\n",
        "    for _, row in df_rag.iterrows():\n",
        "        combined = (\n",
        "            0.5 * row[f'{answer_type}_nli'] +\n",
        "            0.35 * row[f'{answer_type}_semantic'] +\n",
        "            0.15 * row[f'{answer_type}_overlap']\n",
        "        )\n",
        "        all_combined.append(combined)\n",
        "\n",
        "high_count = sum(1 for s in all_combined if s >= 0.8)\n",
        "medium_count = sum(1 for s in all_combined if 0.5 <= s < 0.8)\n",
        "low_count = sum(1 for s in all_combined if s < 0.5)\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Pie(\n",
        "        labels=[\"High Faithfulness\", \"Medium Faithfulness\", \"Low Faithfulness\"],\n",
        "        values=[high_count, medium_count, low_count],\n",
        "        marker_colors=[\"#2ecc71\", \"#f39c12\", \"#e74c3c\"],\n",
        "        title=\"Overall Distribution\"\n",
        "    ),\n",
        "    row=2, col=2\n",
        ")\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(\n",
        "    title_text=\"Comprehensive Faithfulness Analysis\",\n",
        "    showlegend=True,\n",
        "    height=800,\n",
        "    width=1200,\n",
        "    barmode='group'\n",
        ")\n",
        "\n",
        "fig.update_yaxes(title_text=\"Faithfulness (%)\", row=1, col=1)\n",
        "fig.update_yaxes(title_text=\"Score (%)\", row=1, col=2)\n",
        "fig.update_xaxes(title_text=\"Question\", row=2, col=1)\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Additional visualization: Radar chart comparing models\n",
        "fig2 = go.Figure()\n",
        "\n",
        "categories = ['Semantic', 'NLI', 'Token Overlap', 'Combined', 'High Quality %']\n",
        "\n",
        "for model_name in model_names:\n",
        "    detailed = evaluation_results[model_name][\"detailed\"]\n",
        "    values = [\n",
        "        detailed[\"semantic_mean\"] * 100,\n",
        "        detailed[\"nli_mean\"] * 100,\n",
        "        detailed[\"overlap_mean\"] * 100,\n",
        "        detailed[\"faithfulness_mean\"] * 100,\n",
        "        detailed[\"high_pct\"]\n",
        "    ]\n",
        "    \n",
        "    fig2.add_trace(go.Scatterpolar(\n",
        "        r=values + [values[0]],  # Close the loop\n",
        "        theta=categories + [categories[0]],\n",
        "        name=model_name.replace(\"-\", \" \"),\n",
        "        line=dict(color=colors[model_name])\n",
        "    ))\n",
        "\n",
        "fig2.update_layout(\n",
        "    polar=dict(\n",
        "        radialaxis=dict(\n",
        "            visible=True,\n",
        "            range=[0, 100]\n",
        "        )\n",
        "    ),\n",
        "    title=\"Model Comparison Radar Chart\",\n",
        "    showlegend=True,\n",
        "    height=500,\n",
        "    width=700\n",
        ")\n",
        "\n",
        "fig2.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_hallucinations(predictions: List[str],\n",
        "                           contexts: List[str],\n",
        "                           questions: List[str] = None,\n",
        "                           threshold: float = 0.5) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Analyze potential hallucinations in model outputs.\n",
        "    \n",
        "    Args:\n",
        "        predictions: List of model predictions\n",
        "        contexts: List of source contexts\n",
        "        questions: Optional list of questions\n",
        "        threshold: Faithfulness threshold below which to flag as potential hallucination\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with hallucination analysis\n",
        "    \"\"\"\n",
        "    analysis_data = []\n",
        "    \n",
        "    for i, (pred, ctx) in enumerate(zip(predictions, contexts)):\n",
        "        # Calculate all faithfulness scores\n",
        "        semantic = faithfulness_metrics.compute_semantic_faithfulness(pred, ctx)\n",
        "        nli_result = faithfulness_metrics.compute_nli_faithfulness(pred, ctx)\n",
        "        overlap = faithfulness_metrics.compute_token_overlap_faithfulness(pred, ctx)\n",
        "        \n",
        "        # Combined score\n",
        "        combined = 0.5 * nli_result[\"faithfulness\"] + 0.35 * semantic + 0.15 * overlap\n",
        "        \n",
        "        # Determine hallucination category\n",
        "        if combined >= 0.8:\n",
        "            category = \"Highly Faithful\"\n",
        "            risk = \"Low\"\n",
        "        elif combined >= 0.6:\n",
        "            category = \"Mostly Faithful\"\n",
        "            risk = \"Low\"\n",
        "        elif combined >= 0.4:\n",
        "            category = \"Partially Faithful\"\n",
        "            risk = \"Medium\"\n",
        "        elif combined >= 0.2:\n",
        "            category = \"Likely Hallucination\"\n",
        "            risk = \"High\"\n",
        "        else:\n",
        "            category = \"Severe Hallucination\"\n",
        "            risk = \"Critical\"\n",
        "        \n",
        "        # Analyze specific hallucination indicators\n",
        "        indicators = []\n",
        "        if nli_result[\"faithfulness\"] < 0.3:\n",
        "            indicators.append(\"NLI contradiction\")\n",
        "        if semantic < 0.5:\n",
        "            indicators.append(\"Low semantic similarity\")\n",
        "        if overlap < 0.3:\n",
        "            indicators.append(\"Low token overlap\")\n",
        "        \n",
        "        row = {\n",
        "            \"idx\": i,\n",
        "            \"question\": questions[i] if questions else f\"Q{i+1}\",\n",
        "            \"prediction\": pred[:100] + \"...\" if len(pred) > 100 else pred,\n",
        "            \"context_preview\": ctx[:80] + \"...\" if len(ctx) > 80 else ctx,\n",
        "            \"semantic_score\": semantic,\n",
        "            \"nli_score\": nli_result[\"faithfulness\"],\n",
        "            \"overlap_score\": overlap,\n",
        "            \"combined_score\": combined,\n",
        "            \"category\": category,\n",
        "            \"risk_level\": risk,\n",
        "            \"indicators\": \", \".join(indicators) if indicators else \"None\"\n",
        "        }\n",
        "        analysis_data.append(row)\n",
        "    \n",
        "    return pd.DataFrame(analysis_data)\n",
        "\n",
        "# Analyze all three answer types\n",
        "print(\"ðŸ” Hallucination Analysis:\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "for answer_type, label in [\n",
        "    (\"unfaithful_answer\", \"Low-Faithfulness-Baseline\"),\n",
        "    (\"partial_faithful\", \"Medium-Faithfulness-RAG\"),\n",
        "    (\"faithful_answer\", \"High-Faithfulness-RAG\")\n",
        "]:\n",
        "    analysis = analyze_hallucinations(\n",
        "        df_rag[answer_type].tolist(),\n",
        "        df_rag[\"context\"].tolist(),\n",
        "        df_rag[\"question\"].tolist()\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nðŸ“Œ {label}:\")\n",
        "    \n",
        "    # Category distribution\n",
        "    category_counts = analysis[\"category\"].value_counts()\n",
        "    for category, count in category_counts.items():\n",
        "        pct = count / len(analysis) * 100\n",
        "        print(f\"   {category}: {count} ({pct:.1f}%)\")\n",
        "    \n",
        "    # Show examples with risk\n",
        "    high_risk = analysis[analysis[\"risk_level\"].isin([\"High\", \"Critical\"])]\n",
        "    if len(high_risk) > 0:\n",
        "        print(f\"\\n   âš ï¸  High-Risk Examples ({len(high_risk)}):\")\n",
        "        for _, row in high_risk.head(2).iterrows():\n",
        "            print(f\"      Q: {row['question'][:50]}...\")\n",
        "            print(f\"      Answer: {row['prediction'][:60]}...\")\n",
        "            print(f\"      Score: {row['combined_score']:.2f} | Indicators: {row['indicators']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Production-Ready Evaluation Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FaithfulnessPipeline:\n",
        "    \"\"\"\n",
        "    Production-ready faithfulness evaluation pipeline for MLflow.\n",
        "    Includes comprehensive metrics, hallucination detection, and reporting.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self,\n",
        "                 experiment_name: str,\n",
        "                 tracking_uri: str = None,\n",
        "                 embedding_model: str = \"all-MiniLM-L6-v2\",\n",
        "                 nli_model: str = \"cross-encoder/nli-deberta-v3-small\"):\n",
        "        \"\"\"\n",
        "        Initialize the faithfulness pipeline.\n",
        "        \n",
        "        Args:\n",
        "            experiment_name: MLflow experiment name\n",
        "            tracking_uri: Optional MLflow tracking URI\n",
        "            embedding_model: Sentence transformer model\n",
        "            nli_model: NLI cross-encoder model\n",
        "        \"\"\"\n",
        "        self.experiment_name = experiment_name\n",
        "        \n",
        "        if tracking_uri:\n",
        "            mlflow.set_tracking_uri(tracking_uri)\n",
        "        \n",
        "        mlflow.set_experiment(experiment_name)\n",
        "        \n",
        "        # Initialize metrics\n",
        "        self.metrics_calculator = FaithfulnessMetrics(embedding_model, nli_model)\n",
        "        \n",
        "        # Create metric suite\n",
        "        self.metrics = [\n",
        "            self.metrics_calculator.create_semantic_faithfulness_metric(),\n",
        "            self.metrics_calculator.create_nli_faithfulness_metric(),\n",
        "            self.metrics_calculator.create_token_overlap_metric(),\n",
        "            self.metrics_calculator.create_comprehensive_faithfulness_metric()\n",
        "        ]\n",
        "    \n",
        "    def evaluate_model(self,\n",
        "                       model_name: str,\n",
        "                       predictions: List[str],\n",
        "                       contexts: List[str],\n",
        "                       questions: List[str] = None,\n",
        "                       metadata: Dict = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Evaluate a model and log results to MLflow.\n",
        "        \n",
        "        Args:\n",
        "            model_name: Name of the model being evaluated\n",
        "            predictions: Model predictions/answers\n",
        "            contexts: Source contexts\n",
        "            questions: Optional questions for context\n",
        "            metadata: Optional metadata to log\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary with evaluation results\n",
        "        \"\"\"\n",
        "        with mlflow.start_run(run_name=model_name):\n",
        "            # Log parameters\n",
        "            mlflow.log_param(\"model_name\", model_name)\n",
        "            mlflow.log_param(\"num_samples\", len(predictions))\n",
        "            mlflow.log_param(\"pipeline_version\", \"1.0.0\")\n",
        "            mlflow.log_param(\"embedding_model\", self.metrics_calculator.embedding_model_name)\n",
        "            mlflow.log_param(\"nli_model\", self.metrics_calculator.nli_model_name)\n",
        "            \n",
        "            if metadata:\n",
        "                for key, value in metadata.items():\n",
        "                    mlflow.log_param(key, value)\n",
        "            \n",
        "            # Prepare evaluation dataframe\n",
        "            eval_df = pd.DataFrame({\n",
        "                \"predictions\": predictions,\n",
        "                \"targets\": contexts\n",
        "            })\n",
        "            \n",
        "            # Run MLflow evaluation\n",
        "            results = mlflow.evaluate(\n",
        "                data=eval_df,\n",
        "                targets=\"targets\",\n",
        "                predictions=\"predictions\",\n",
        "                extra_metrics=self.metrics,\n",
        "                evaluators=\"default\"\n",
        "            )\n",
        "            \n",
        "            # Calculate detailed metrics\n",
        "            detailed_metrics = self._calculate_all_metrics(predictions, contexts)\n",
        "            \n",
        "            # Log detailed metrics\n",
        "            for key, value in detailed_metrics.items():\n",
        "                if isinstance(value, (int, float)):\n",
        "                    mlflow.log_metric(key, value)\n",
        "            \n",
        "            # Run hallucination analysis\n",
        "            hallucination_analysis = analyze_hallucinations(predictions, contexts, questions)\n",
        "            \n",
        "            # Log hallucination metrics\n",
        "            risk_counts = hallucination_analysis[\"risk_level\"].value_counts()\n",
        "            for risk, count in risk_counts.items():\n",
        "                safe_name = risk.lower().replace(\" \", \"_\")\n",
        "                mlflow.log_metric(f\"risk_{safe_name}_count\", count)\n",
        "            \n",
        "            # Generate and log report\n",
        "            report = self._generate_evaluation_report(\n",
        "                model_name, detailed_metrics, hallucination_analysis\n",
        "            )\n",
        "            mlflow.log_text(report, \"faithfulness_report.txt\")\n",
        "            \n",
        "            # Generate recommendations\n",
        "            recommendations = self._generate_recommendations(detailed_metrics)\n",
        "            mlflow.log_text(recommendations, \"recommendations.txt\")\n",
        "            \n",
        "            return {\n",
        "                \"metrics\": results.metrics,\n",
        "                \"detailed\": detailed_metrics,\n",
        "                \"hallucination_analysis\": hallucination_analysis,\n",
        "                \"report\": report,\n",
        "                \"recommendations\": recommendations\n",
        "            }\n",
        "    \n",
        "    def _calculate_all_metrics(self,\n",
        "                               predictions: List[str],\n",
        "                               contexts: List[str]) -> Dict:\n",
        "        \"\"\"Calculate comprehensive metrics.\"\"\"\n",
        "        semantic_scores = []\n",
        "        nli_scores = []\n",
        "        overlap_scores = []\n",
        "        \n",
        "        for pred, ctx in zip(predictions, contexts):\n",
        "            semantic_scores.append(self.metrics_calculator.compute_semantic_faithfulness(pred, ctx))\n",
        "            nli_scores.append(self.metrics_calculator.compute_nli_faithfulness(pred, ctx)['faithfulness'])\n",
        "            overlap_scores.append(self.metrics_calculator.compute_token_overlap_faithfulness(pred, ctx))\n",
        "        \n",
        "        combined_scores = [0.5*n + 0.35*s + 0.15*o for n, s, o in zip(nli_scores, semantic_scores, overlap_scores)]\n",
        "        \n",
        "        return {\n",
        "            # Combined metrics\n",
        "            \"faithfulness_combined\": np.mean(combined_scores),\n",
        "            \"faithfulness_std\": np.std(combined_scores),\n",
        "            \"faithfulness_min\": np.min(combined_scores),\n",
        "            \"faithfulness_max\": np.max(combined_scores),\n",
        "            \"faithfulness_median\": np.median(combined_scores),\n",
        "            \n",
        "            # Individual metrics\n",
        "            \"semantic_mean\": np.mean(semantic_scores),\n",
        "            \"nli_mean\": np.mean(nli_scores),\n",
        "            \"overlap_mean\": np.mean(overlap_scores),\n",
        "            \n",
        "            # Quality distribution\n",
        "            \"high_faithfulness\": sum(1 for s in combined_scores if s >= 0.8),\n",
        "            \"medium_faithfulness\": sum(1 for s in combined_scores if 0.5 <= s < 0.8),\n",
        "            \"low_faithfulness\": sum(1 for s in combined_scores if s < 0.5),\n",
        "            \n",
        "            # Percentages\n",
        "            \"high_pct\": sum(1 for s in combined_scores if s >= 0.8) / len(combined_scores) * 100,\n",
        "            \"low_pct\": sum(1 for s in combined_scores if s < 0.5) / len(combined_scores) * 100,\n",
        "            \n",
        "            # Hallucination indicators\n",
        "            \"potential_hallucinations\": sum(1 for s in combined_scores if s < 0.4),\n",
        "            \"severe_hallucinations\": sum(1 for s in combined_scores if s < 0.2),\n",
        "            \n",
        "            # Quality indicators\n",
        "            \"acceptable_quality\": 1.0 if np.mean(combined_scores) >= 0.6 else 0.0,\n",
        "            \"high_quality\": 1.0 if np.mean(combined_scores) >= 0.8 else 0.0\n",
        "        }\n",
        "    \n",
        "    def _generate_evaluation_report(self,\n",
        "                                    model_name: str,\n",
        "                                    metrics: Dict,\n",
        "                                    hallucination_analysis: pd.DataFrame) -> str:\n",
        "        \"\"\"Generate a text report.\"\"\"\n",
        "        report_lines = [\n",
        "            \"=\"*80,\n",
        "            f\"FAITHFULNESS EVALUATION REPORT: {model_name}\",\n",
        "            \"=\"*80,\n",
        "            \"\",\n",
        "            \"PERFORMANCE METRICS:\",\n",
        "            \"-\"*40,\n",
        "            f\"  Combined Faithfulness:    {metrics['faithfulness_combined']*100:.1f}%\",\n",
        "            f\"  Semantic Faithfulness:    {metrics['semantic_mean']*100:.1f}%\",\n",
        "            f\"  NLI Faithfulness:         {metrics['nli_mean']*100:.1f}%\",\n",
        "            f\"  Token Overlap:            {metrics['overlap_mean']*100:.1f}%\",\n",
        "            \"\",\n",
        "            f\"  Score Range: [{metrics['faithfulness_min']*100:.1f}%, {metrics['faithfulness_max']*100:.1f}%]\",\n",
        "            f\"  Standard Deviation: {metrics['faithfulness_std']*100:.1f}%\",\n",
        "            \"\",\n",
        "            \"QUALITY DISTRIBUTION:\",\n",
        "            \"-\"*40,\n",
        "            f\"  High Faithfulness (>=80%):    {int(metrics['high_faithfulness'])} ({metrics['high_pct']:.1f}%)\",\n",
        "            f\"  Medium Faithfulness (50-80%): {int(metrics['medium_faithfulness'])} ({100-metrics['high_pct']-metrics['low_pct']:.1f}%)\",\n",
        "            f\"  Low Faithfulness (<50%):      {int(metrics['low_faithfulness'])} ({metrics['low_pct']:.1f}%)\",\n",
        "            \"\",\n",
        "            \"HALLUCINATION ANALYSIS:\",\n",
        "            \"-\"*40,\n",
        "            f\"  Potential Hallucinations:  {int(metrics['potential_hallucinations'])}\",\n",
        "            f\"  Severe Hallucinations:     {int(metrics['severe_hallucinations'])}\",\n",
        "            \"\"\n",
        "        ]\n",
        "        \n",
        "        # Add risk distribution\n",
        "        risk_counts = hallucination_analysis[\"risk_level\"].value_counts()\n",
        "        report_lines.append(\"RISK DISTRIBUTION:\")\n",
        "        report_lines.append(\"-\"*40)\n",
        "        for risk in [\"Low\", \"Medium\", \"High\", \"Critical\"]:\n",
        "            count = risk_counts.get(risk, 0)\n",
        "            pct = count / len(hallucination_analysis) * 100\n",
        "            report_lines.append(f\"  {risk}: {count} ({pct:.1f}%)\")\n",
        "        \n",
        "        report_lines.extend([\n",
        "            \"\",\n",
        "            \"QUALITY ASSESSMENT:\",\n",
        "            \"-\"*40,\n",
        "            f\"  Acceptable Quality (>=60%): {'Yes' if metrics['acceptable_quality'] else 'No'}\",\n",
        "            f\"  High Quality (>=80%):       {'Yes' if metrics['high_quality'] else 'No'}\",\n",
        "            \"\",\n",
        "            \"=\"*80\n",
        "        ])\n",
        "        \n",
        "        return \"\\n\".join(report_lines)\n",
        "    \n",
        "    def _generate_recommendations(self, metrics: Dict) -> str:\n",
        "        \"\"\"Generate actionable recommendations.\"\"\"\n",
        "        recommendations = [\"RECOMMENDATIONS:\", \"=\"*50, \"\"]\n",
        "        \n",
        "        faithfulness = metrics['faithfulness_combined']\n",
        "        \n",
        "        if faithfulness < 0.4:\n",
        "            recommendations.extend([\n",
        "                \"âš ï¸ CRITICAL: Very low faithfulness (<40%)\",\n",
        "                \"\",\n",
        "                \"Immediate actions required:\",\n",
        "                \"1. Review and improve retrieval pipeline\",\n",
        "                \"2. Add explicit grounding instructions to prompts\",\n",
        "                \"3. Implement citation requirements\",\n",
        "                \"4. Consider using more restrictive generation parameters\",\n",
        "                \"5. Add post-generation fact-checking\",\n",
        "                \"\"\n",
        "            ])\n",
        "        elif faithfulness < 0.6:\n",
        "            recommendations.extend([\n",
        "                \"âš ï¸ WARNING: Below acceptable threshold (<60%)\",\n",
        "                \"\",\n",
        "                \"Actions to consider:\",\n",
        "                \"1. Improve retriever relevance scoring\",\n",
        "                \"2. Add chain-of-thought prompting for grounding\",\n",
        "                \"3. Implement answer verification step\",\n",
        "                \"4. Review failure cases for patterns\",\n",
        "                \"\"\n",
        "            ])\n",
        "        elif faithfulness < 0.8:\n",
        "            recommendations.extend([\n",
        "                \"âœ“ ACCEPTABLE: Good performance (60-80%)\",\n",
        "                \"\",\n",
        "                \"Potential improvements:\",\n",
        "                \"1. Focus on edge cases with low scores\",\n",
        "                \"2. Fine-tune for specific failure patterns\",\n",
        "                \"3. Add confidence scoring for uncertain answers\",\n",
        "                \"\"\n",
        "            ])\n",
        "        else:\n",
        "            recommendations.extend([\n",
        "                \"âœ… EXCELLENT: High faithfulness (>=80%)\",\n",
        "                \"\",\n",
        "                \"Maintenance considerations:\",\n",
        "                \"1. Monitor for drift in production\",\n",
        "                \"2. Expand test set for edge cases\",\n",
        "                \"3. Consider stricter thresholds\",\n",
        "                \"\"\n",
        "            ])\n",
        "        \n",
        "        # Add hallucination-specific recommendations\n",
        "        if metrics['potential_hallucinations'] > 0:\n",
        "            recommendations.extend([\n",
        "                f\"ðŸ” Hallucination Alert: {int(metrics['potential_hallucinations'])} potential hallucinations detected\",\n",
        "                \"\",\n",
        "                \"Hallucination mitigation:\",\n",
        "                \"1. Implement answer-context alignment checks\",\n",
        "                \"2. Add explicit 'I don't know' handling\",\n",
        "                \"3. Use constrained decoding when possible\",\n",
        "                \"4. Add human-in-the-loop for critical answers\",\n",
        "                \"\"\n",
        "            ])\n",
        "        \n",
        "        return \"\\n\".join(recommendations)\n",
        "    \n",
        "    def compare_models(self,\n",
        "                       model_results: Dict[str, Dict],\n",
        "                       baseline_name: str = None) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Compare multiple model results.\n",
        "        \n",
        "        Args:\n",
        "            model_results: Dictionary mapping model names to their results\n",
        "            baseline_name: Optional baseline model name for comparison\n",
        "            \n",
        "        Returns:\n",
        "            DataFrame with comparison results\n",
        "        \"\"\"\n",
        "        comparison_data = []\n",
        "        baseline_score = None\n",
        "        \n",
        "        if baseline_name and baseline_name in model_results:\n",
        "            baseline_score = model_results[baseline_name][\"detailed\"][\"faithfulness_combined\"]\n",
        "        \n",
        "        for model_name, results in model_results.items():\n",
        "            detailed = results[\"detailed\"]\n",
        "            \n",
        "            row = {\n",
        "                \"Model\": model_name,\n",
        "                \"Faithfulness\": detailed[\"faithfulness_combined\"],\n",
        "                \"Semantic\": detailed[\"semantic_mean\"],\n",
        "                \"NLI\": detailed[\"nli_mean\"],\n",
        "                \"Overlap\": detailed[\"overlap_mean\"],\n",
        "                \"High Quality %\": detailed[\"high_pct\"],\n",
        "                \"Hallucinations\": int(detailed[\"potential_hallucinations\"])\n",
        "            }\n",
        "            \n",
        "            if baseline_score is not None:\n",
        "                row[\"vs Baseline\"] = detailed[\"faithfulness_combined\"] - baseline_score\n",
        "            \n",
        "            comparison_data.append(row)\n",
        "        \n",
        "        return pd.DataFrame(comparison_data).sort_values(\"Faithfulness\", ascending=False)\n",
        "\n",
        "# Demo the production pipeline\n",
        "print(\"ðŸš€ Production Pipeline Demo\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "pipeline = FaithfulnessPipeline(\n",
        "    experiment_name=f\"/Users/{user_email}/faithfulness-production-demo\"\n",
        ")\n",
        "\n",
        "# Evaluate all models through the pipeline\n",
        "pipeline_results = {}\n",
        "\n",
        "for model_name, config in models_config.items():\n",
        "    print(f\"\\nðŸ“ Evaluating {model_name}...\")\n",
        "    result = pipeline.evaluate_model(\n",
        "        model_name,\n",
        "        config[\"predictions\"],\n",
        "        df_rag[\"context\"].tolist(),\n",
        "        df_rag[\"question\"].tolist(),\n",
        "        config[\"config\"]\n",
        "    )\n",
        "    pipeline_results[model_name] = result\n",
        "\n",
        "print(\"\\nâœ… Pipeline evaluation completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show model comparison\n",
        "comparison = pipeline.compare_models(pipeline_results, baseline_name=\"Low-Faithfulness-Baseline\")\n",
        "\n",
        "print(\"\\nðŸ“Š Model Comparison (vs Low-Faithfulness-Baseline):\")\n",
        "print(\"=\"*120)\n",
        "\n",
        "# Format for display\n",
        "display_comparison = comparison.copy()\n",
        "for col in [\"Faithfulness\", \"Semantic\", \"NLI\", \"Overlap\"]:\n",
        "    display_comparison[col] = display_comparison[col].apply(lambda x: f\"{x*100:.1f}%\")\n",
        "if \"vs Baseline\" in display_comparison.columns:\n",
        "    display_comparison[\"vs Baseline\"] = display_comparison[\"vs Baseline\"].apply(lambda x: f\"{x*100:+.1f}%\")\n",
        "display_comparison[\"High Quality %\"] = display_comparison[\"High Quality %\"].apply(lambda x: f\"{x:.1f}%\")\n",
        "\n",
        "print(display_comparison.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display reports and recommendations\n",
        "for model_name, result in pipeline_results.items():\n",
        "    print(f\"\\n{'='*100}\")\n",
        "    print(result[\"report\"])\n",
        "    print(\"\\n\" + result[\"recommendations\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Advanced Use Cases\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced Use Case 1: Claim-level faithfulness with verification\n",
        "def create_claim_level_metric(metrics_calculator):\n",
        "    \"\"\"\n",
        "    Create metric that evaluates each claim in the answer separately.\n",
        "    Useful for long-form answers with multiple statements.\n",
        "    \"\"\"\n",
        "    def claim_level_metric(predictions, targets, metrics):\n",
        "        predictions = FaithfulnessMetrics._to_string_list(predictions)\n",
        "        contexts = FaithfulnessMetrics._to_string_list(targets)\n",
        "        \n",
        "        all_scores = []\n",
        "        claim_counts = []\n",
        "        supported_counts = []\n",
        "        \n",
        "        for pred, ctx in zip(predictions, contexts):\n",
        "            result = metrics_calculator.compute_claim_faithfulness(pred, ctx)\n",
        "            all_scores.append(result[\"faithfulness\"])\n",
        "            claim_counts.append(result[\"num_claims\"])\n",
        "            supported_counts.append(result[\"supported_claims\"])\n",
        "        \n",
        "        return MetricValue(\n",
        "            aggregate_results={\n",
        "                \"claim_faithfulness_mean\": np.mean(all_scores),\n",
        "                \"total_claims\": sum(claim_counts),\n",
        "                \"supported_claims\": sum(supported_counts),\n",
        "                \"claim_support_ratio\": sum(supported_counts) / max(sum(claim_counts), 1)\n",
        "            },\n",
        "            scores=all_scores\n",
        "        )\n",
        "    \n",
        "    return make_metric(\n",
        "        eval_fn=claim_level_metric,\n",
        "        greater_is_better=True,\n",
        "        name=\"claim_level_faithfulness\"\n",
        "    )\n",
        "\n",
        "# Demo claim-level analysis\n",
        "print(\"ðŸ“Š Claim-Level Faithfulness Analysis:\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "for idx, row in df_rag.head(3).iterrows():\n",
        "    print(f\"\\nQuestion: {row['question']}\")\n",
        "    print(f\"Context: {row['context'][:80]}...\")\n",
        "    \n",
        "    for answer_type, label in [(\"faithful_answer\", \"Faithful\"), (\"unfaithful_answer\", \"Unfaithful\")]:\n",
        "        result = faithfulness_metrics.compute_claim_faithfulness(row[answer_type], row[\"context\"])\n",
        "        print(f\"\\n  {label} Answer: {row[answer_type][:60]}...\")\n",
        "        print(f\"  Claims found: {result['num_claims']}, Supported: {result['supported_claims']}\")\n",
        "        print(f\"  Overall faithfulness: {result['faithfulness']:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced Use Case 2: Multi-context faithfulness (multiple retrieved documents)\n",
        "def create_multi_context_metric(metrics_calculator, aggregation: str = \"max\"):\n",
        "    \"\"\"\n",
        "    Create metric for answers grounded in multiple retrieved documents.\n",
        "    Uses max/mean/any aggregation across contexts.\n",
        "    \n",
        "    Args:\n",
        "        metrics_calculator: FaithfulnessMetrics instance\n",
        "        aggregation: How to aggregate scores across contexts (\"max\", \"mean\", \"any\")\n",
        "    \"\"\"\n",
        "    def multi_context_metric(predictions, targets, metrics):\n",
        "        predictions = FaithfulnessMetrics._to_string_list(predictions)\n",
        "        # Targets expected to be pipe-separated multiple contexts\n",
        "        multi_contexts = FaithfulnessMetrics._to_string_list(targets)\n",
        "        \n",
        "        final_scores = []\n",
        "        \n",
        "        for pred, ctx_string in zip(predictions, multi_contexts):\n",
        "            # Split multiple contexts\n",
        "            contexts = [c.strip() for c in ctx_string.split('|||')]\n",
        "            \n",
        "            # Score against each context\n",
        "            context_scores = []\n",
        "            for ctx in contexts:\n",
        "                if ctx:\n",
        "                    nli = metrics_calculator.compute_nli_faithfulness(pred, ctx)['faithfulness']\n",
        "                    context_scores.append(nli)\n",
        "            \n",
        "            # Aggregate scores\n",
        "            if not context_scores:\n",
        "                final_scores.append(0.0)\n",
        "            elif aggregation == \"max\":\n",
        "                final_scores.append(max(context_scores))\n",
        "            elif aggregation == \"mean\":\n",
        "                final_scores.append(np.mean(context_scores))\n",
        "            elif aggregation == \"any\":\n",
        "                final_scores.append(1.0 if any(s >= 0.5 for s in context_scores) else 0.0)\n",
        "            else:\n",
        "                final_scores.append(max(context_scores))\n",
        "        \n",
        "        return MetricValue(\n",
        "            aggregate_results={\n",
        "                f\"multi_context_faithfulness_{aggregation}\": np.mean(final_scores),\n",
        "                \"num_with_support\": sum(1 for s in final_scores if s >= 0.5)\n",
        "            },\n",
        "            scores=final_scores\n",
        "        )\n",
        "    \n",
        "    return make_metric(\n",
        "        eval_fn=multi_context_metric,\n",
        "        greater_is_better=True,\n",
        "        name=f\"multi_context_faithfulness_{aggregation}\"\n",
        "    )\n",
        "\n",
        "# Demo with multi-context data\n",
        "multi_context_data = pd.DataFrame({\n",
        "    \"question\": [\n",
        "        \"What are the health benefits of green tea?\",\n",
        "        \"How does solar energy work?\"\n",
        "    ],\n",
        "    \"contexts\": [\n",
        "        \"Green tea contains antioxidants called catechins. ||| Green tea may help reduce the risk of heart disease. ||| Studies show green tea can boost metabolism.\",\n",
        "        \"Solar panels convert sunlight into electricity. ||| Photovoltaic cells use semiconductors to generate power. ||| Solar energy is renewable and sustainable.\"\n",
        "    ],\n",
        "    \"answer\": [\n",
        "        \"Green tea is rich in antioxidants and may help reduce heart disease risk while boosting metabolism.\",\n",
        "        \"Solar panels use photovoltaic cells with semiconductors to convert sunlight into renewable electricity.\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\nðŸ“Š Multi-Context Faithfulness Demo:\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "for idx, row in multi_context_data.iterrows():\n",
        "    print(f\"\\nQuestion: {row['question']}\")\n",
        "    print(f\"Answer: {row['answer']}\")\n",
        "    print(f\"Contexts: {len(row['contexts'].split('|||'))} retrieved documents\")\n",
        "    \n",
        "    # Score against each context\n",
        "    contexts = row['contexts'].split('|||')\n",
        "    for i, ctx in enumerate(contexts):\n",
        "        score = faithfulness_metrics.compute_nli_faithfulness(row['answer'], ctx.strip())['faithfulness']\n",
        "        print(f\"  Context {i+1}: {score:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced Use Case 3: Citation verification\n",
        "def create_citation_faithfulness_metric(metrics_calculator):\n",
        "    \"\"\"\n",
        "    Create metric that verifies cited claims match their sources.\n",
        "    Expects answers with inline citations like [1], [2], etc.\n",
        "    \"\"\"\n",
        "    def citation_metric(predictions, targets, metrics):\n",
        "        predictions = FaithfulnessMetrics._to_string_list(predictions)\n",
        "        # Targets contain numbered sources separated by |||\n",
        "        source_lists = FaithfulnessMetrics._to_string_list(targets)\n",
        "        \n",
        "        scores = []\n",
        "        citation_accuracy_list = []\n",
        "        \n",
        "        for pred, sources_str in zip(predictions, source_lists):\n",
        "            # Parse sources\n",
        "            sources = {i+1: s.strip() for i, s in enumerate(sources_str.split('|||'))}\n",
        "            \n",
        "            # Find citations in answer\n",
        "            citation_pattern = r'\\[(\\d+)\\]'\n",
        "            citations = re.findall(citation_pattern, pred)\n",
        "            \n",
        "            if not citations:\n",
        "                # No citations - evaluate overall\n",
        "                all_sources = ' '.join(sources.values())\n",
        "                nli = metrics_calculator.compute_nli_faithfulness(pred, all_sources)['faithfulness']\n",
        "                scores.append(nli)\n",
        "                citation_accuracy_list.append(1.0)  # No citations to verify\n",
        "            else:\n",
        "                # Verify each citation\n",
        "                citation_scores = []\n",
        "                for cite_num in citations:\n",
        "                    cite_num = int(cite_num)\n",
        "                    if cite_num in sources:\n",
        "                        # Find the claim before/around this citation\n",
        "                        nli = metrics_calculator.compute_nli_faithfulness(pred, sources[cite_num])['faithfulness']\n",
        "                        citation_scores.append(nli)\n",
        "                \n",
        "                if citation_scores:\n",
        "                    scores.append(np.mean(citation_scores))\n",
        "                    citation_accuracy_list.append(sum(1 for s in citation_scores if s >= 0.5) / len(citation_scores))\n",
        "                else:\n",
        "                    scores.append(0.0)\n",
        "                    citation_accuracy_list.append(0.0)\n",
        "        \n",
        "        return MetricValue(\n",
        "            aggregate_results={\n",
        "                \"citation_faithfulness\": np.mean(scores),\n",
        "                \"citation_accuracy\": np.mean(citation_accuracy_list)\n",
        "            },\n",
        "            scores=scores\n",
        "        )\n",
        "    \n",
        "    return make_metric(\n",
        "        eval_fn=citation_metric,\n",
        "        greater_is_better=True,\n",
        "        name=\"citation_faithfulness\"\n",
        "    )\n",
        "\n",
        "# Demo citation verification\n",
        "citation_data = pd.DataFrame({\n",
        "    \"question\": [\"What is the capital of France?\"],\n",
        "    \"sources\": [\"France is a country in Western Europe. Its capital is Paris. ||| Paris has a population of about 2.2 million. ||| The Eiffel Tower is located in Paris.\"],\n",
        "    \"answer\": [\"The capital of France is Paris [1], with a population of about 2.2 million people [2]. The city is famous for the Eiffel Tower [3].\"]\n",
        "})\n",
        "\n",
        "print(\"\\nðŸ“Š Citation Verification Demo:\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "for idx, row in citation_data.iterrows():\n",
        "    print(f\"\\nQuestion: {row['question']}\")\n",
        "    print(f\"Answer: {row['answer']}\")\n",
        "    sources = row['sources'].split('|||')\n",
        "    print(\"Sources:\")\n",
        "    for i, src in enumerate(sources):\n",
        "        print(f\"  [{i+1}] {src.strip()[:60]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Best Practices and Guidelines\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Best practices reference\n",
        "best_practices = \"\"\"\n",
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "â•‘                      FAITHFULNESS METRIC BEST PRACTICES                          â•‘\n",
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ 1. CHOOSING THE RIGHT FAITHFULNESS APPROACH                                      â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚  Use Case                      â”‚ Recommended Approach                            â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚\n",
        "â”‚  RAG QA Systems                â”‚ NLI + Semantic (combined)                       â”‚\n",
        "â”‚  Document Summarization        â”‚ Claim-level verification                        â”‚\n",
        "â”‚  Fact Verification             â”‚ NLI-based entailment                            â”‚\n",
        "â”‚  Multi-document QA             â”‚ Multi-context faithfulness                      â”‚\n",
        "â”‚  Cited Answers                 â”‚ Citation verification                           â”‚\n",
        "â”‚  Quick Baseline                â”‚ Token overlap                                   â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ 2. HALLUCINATION DETECTION THRESHOLDS                                            â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚  Faithfulness Score  â”‚ Interpretation          â”‚ Action                         â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚\n",
        "â”‚  >= 0.80             â”‚ Highly Faithful         â”‚ Safe for production            â”‚\n",
        "â”‚  0.60 - 0.80         â”‚ Mostly Faithful         â”‚ Review edge cases              â”‚\n",
        "â”‚  0.40 - 0.60         â”‚ Partially Faithful      â”‚ Manual verification needed     â”‚\n",
        "â”‚  0.20 - 0.40         â”‚ Likely Hallucination    â”‚ Flag for review                â”‚\n",
        "â”‚  < 0.20              â”‚ Severe Hallucination    â”‚ Block from production          â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ 3. MODEL SELECTION GUIDELINES                                                    â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚  Embedding Models (Semantic Similarity):                                         â”‚\n",
        "â”‚  â€¢ all-MiniLM-L6-v2: Fast, good for general use                                 â”‚\n",
        "â”‚  â€¢ all-mpnet-base-v2: Higher quality, slower                                    â”‚\n",
        "â”‚  â€¢ paraphrase-multilingual: Multi-language support                              â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚  NLI Models (Entailment):                                                        â”‚\n",
        "â”‚  â€¢ cross-encoder/nli-deberta-v3-small: Good balance                             â”‚\n",
        "â”‚  â€¢ cross-encoder/nli-deberta-v3-base: Higher quality                            â”‚\n",
        "â”‚  â€¢ cross-encoder/nli-roberta-base: Alternative option                           â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ 4. COMMON FAITHFULNESS PITFALLS                                                  â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚  âŒ Using only token overlap (misses semantic understanding)                     â”‚\n",
        "â”‚  âŒ Not handling pandas Series in MLflow metric functions                        â”‚\n",
        "â”‚  âŒ Ignoring claim-level analysis for long-form answers                          â”‚\n",
        "â”‚  âŒ Using single threshold for all content types                                 â”‚\n",
        "â”‚  âŒ Not verifying citations separately                                           â”‚\n",
        "â”‚  âŒ Treating neutral NLI as unfaithful (it's uncertain, not wrong)              â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ 5. IMPROVING RAG FAITHFULNESS                                                    â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚  Retrieval Improvements:                                                         â”‚\n",
        "â”‚  â€¢ Use dense retrieval with semantic search                                      â”‚\n",
        "â”‚  â€¢ Implement re-ranking for better context selection                             â”‚\n",
        "â”‚  â€¢ Filter irrelevant chunks before generation                                    â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚  Generation Improvements:                                                        â”‚\n",
        "â”‚  â€¢ Add \"Only use provided context\" instructions                                 â”‚\n",
        "â”‚  â€¢ Require inline citations                                                      â”‚\n",
        "â”‚  â€¢ Use chain-of-thought for grounding                                           â”‚\n",
        "â”‚  â€¢ Lower temperature for more deterministic outputs                              â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚  Post-processing:                                                                â”‚\n",
        "â”‚  â€¢ Implement answer-context verification                                         â”‚\n",
        "â”‚  â€¢ Add confidence scores                                                         â”‚\n",
        "â”‚  â€¢ Flag uncertain answers for human review                                       â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ 6. FAITHFULNESS vs RELEVANCE vs CORRECTNESS                                      â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚  Metric        â”‚ Question                          â”‚ Example                     â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚\n",
        "â”‚  Faithfulness  â”‚ Is answer supported by context?   â”‚ \"Paris\" from doc â†’ âœ“       â”‚\n",
        "â”‚  Relevance     â”‚ Does answer address the question? â”‚ Answers \"capital?\" â†’ âœ“     â”‚\n",
        "â”‚  Correctness   â”‚ Is the answer factually true?     â”‚ Paris IS capital â†’ âœ“       â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚  An answer can be:                                                               â”‚\n",
        "â”‚  â€¢ Faithful but irrelevant (answers wrong question from context)                â”‚\n",
        "â”‚  â€¢ Relevant but unfaithful (answers question but not from context)              â”‚\n",
        "â”‚  â€¢ Faithful but incorrect (context itself has errors)                           â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚  â†’ Use all three metrics for comprehensive RAG evaluation                       â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\"\"\"\n",
        "\n",
        "print(best_practices)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Summary and Next Steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\"\"\n",
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "â•‘                        SUMMARY: FAITHFULNESS WITH MLFLOW                         â•‘\n",
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "âœ… WHAT WE COVERED:\n",
        "\n",
        "1. FAITHFULNESS FUNDAMENTALS\n",
        "   â€¢ Semantic similarity-based faithfulness (embeddings)\n",
        "   â€¢ NLI-based faithfulness (entailment detection)\n",
        "   â€¢ Token overlap faithfulness (baseline)\n",
        "   â€¢ Combined/comprehensive faithfulness scoring\n",
        "\n",
        "2. HALLUCINATION DETECTION\n",
        "   â€¢ Risk categorization (Low/Medium/High/Critical)\n",
        "   â€¢ Claim-level verification\n",
        "   â€¢ Indicator analysis (NLI contradiction, low similarity, etc.)\n",
        "\n",
        "3. MLFLOW INTEGRATION\n",
        "   â€¢ Custom metric creation with make_metric()\n",
        "   â€¢ Proper handling of pandas Series inputs\n",
        "   â€¢ Logging metrics, parameters, and artifacts\n",
        "   â€¢ Generating evaluation reports and recommendations\n",
        "\n",
        "4. ADVANCED USE CASES\n",
        "   â€¢ Claim-level faithfulness (verify each statement)\n",
        "   â€¢ Multi-context faithfulness (multiple retrieved documents)\n",
        "   â€¢ Citation verification (inline citations matching sources)\n",
        "\n",
        "5. PRODUCTION PIPELINE\n",
        "   â€¢ Comprehensive evaluation with FaithfulnessPipeline class\n",
        "   â€¢ Automated reporting and recommendations\n",
        "   â€¢ Model comparison functionality\n",
        "\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "ðŸ“Š KEY METRICS CREATED:\n",
        "   â€¢ semantic_faithfulness    - Embedding-based similarity\n",
        "   â€¢ nli_faithfulness         - NLI entailment scoring\n",
        "   â€¢ token_overlap_faithfulness - Token overlap baseline\n",
        "   â€¢ comprehensive_faithfulness - Weighted combination (50% NLI + 35% semantic + 15% overlap)\n",
        "\n",
        "ðŸ” HALLUCINATION THRESHOLDS:\n",
        "   â€¢ >= 0.80: Highly Faithful (production ready)\n",
        "   â€¢ 0.60-0.80: Mostly Faithful (monitor)\n",
        "   â€¢ 0.40-0.60: Partially Faithful (review needed)\n",
        "   â€¢ < 0.40: Potential Hallucination (flag/block)\n",
        "\n",
        "ðŸ”— NEXT STEPS:\n",
        "   1. View results in MLflow UI: mlflow ui\n",
        "   2. Compare experiments across runs\n",
        "   3. Combine with relevance and correctness metrics\n",
        "   4. Set up automated evaluation pipelines for RAG systems\n",
        "   5. Implement human-in-the-loop for low-confidence answers\n",
        "\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "ðŸ“š RELATED METRICS TO EXPLORE:\n",
        "   â€¢ Answer Similarity - For evaluating answer quality vs reference\n",
        "   â€¢ ROUGE/BLEU - For summarization and translation\n",
        "   â€¢ Exact Match - For factoid QA\n",
        "   â€¢ Relevance - For checking if answer addresses the question\n",
        "\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up MLflow experiment\n",
        "mlflow.set_experiment(f\"/Users/{user_email}/faithfulness-demo\")\n",
        "\n",
        "def evaluate_rag_model(model_name: str,\n",
        "                       predictions: List[str],\n",
        "                       contexts: List[str],\n",
        "                       model_config: Dict = None) -> Tuple[Dict, Dict]:\n",
        "    \"\"\"\n",
        "    Evaluate a RAG model using faithfulness metrics and log to MLflow.\n",
        "    \"\"\"\n",
        "    with mlflow.start_run(run_name=model_name):\n",
        "        # Log model configuration\n",
        "        mlflow.log_param(\"model_name\", model_name)\n",
        "        mlflow.log_param(\"num_samples\", len(predictions))\n",
        "        mlflow.log_param(\"embedding_model\", faithfulness_metrics.embedding_model_name)\n",
        "        mlflow.log_param(\"nli_model\", faithfulness_metrics.nli_model_name)\n",
        "\n",
        "        if model_config:\n",
        "            for key, value in model_config.items():\n",
        "                mlflow.log_param(key, value)\n",
        "\n",
        "        # Create evaluation dataframe\n",
        "        # Note: For faithfulness, we use context as targets\n",
        "        eval_df = pd.DataFrame({\n",
        "            \"predictions\": predictions,\n",
        "            \"targets\": contexts  # Context is used as target for faithfulness\n",
        "        })\n",
        "\n",
        "        # Evaluate with faithfulness metrics\n",
        "        metrics = [\n",
        "            semantic_faithfulness,\n",
        "            nli_faithfulness,\n",
        "            token_overlap,\n",
        "            comprehensive_faithfulness\n",
        "        ]\n",
        "\n",
        "        results = mlflow.evaluate(\n",
        "            data=eval_df,\n",
        "            targets=\"targets\",\n",
        "            predictions=\"predictions\",\n",
        "            extra_metrics=metrics,\n",
        "            evaluators=\"default\"\n",
        "        )\n",
        "\n",
        "        # Calculate additional detailed metrics\n",
        "        detailed_scores = calculate_detailed_faithfulness(predictions, contexts)\n",
        "\n",
        "        # Log detailed metrics\n",
        "        for key, value in detailed_scores.items():\n",
        "            if isinstance(value, (int, float)):\n",
        "                mlflow.log_metric(key, value)\n",
        "\n",
        "        return results.metrics, detailed_scores\n",
        "\n",
        "def calculate_detailed_faithfulness(predictions: List[str], contexts: List[str]) -> Dict:\n",
        "    \"\"\"\n",
        "    Calculate detailed faithfulness statistics.\n",
        "    \"\"\"\n",
        "    semantic_scores = []\n",
        "    nli_scores = []\n",
        "    overlap_scores = []\n",
        "    \n",
        "    for pred, ctx in zip(predictions, contexts):\n",
        "        semantic_scores.append(faithfulness_metrics.compute_semantic_faithfulness(pred, ctx))\n",
        "        nli_scores.append(faithfulness_metrics.compute_nli_faithfulness(pred, ctx)['faithfulness'])\n",
        "        overlap_scores.append(faithfulness_metrics.compute_token_overlap_faithfulness(pred, ctx))\n",
        "    \n",
        "    # Combined scores\n",
        "    combined_scores = [0.5*n + 0.35*s + 0.15*o for n, s, o in zip(nli_scores, semantic_scores, overlap_scores)]\n",
        "    \n",
        "    detailed = {\n",
        "        # Basic statistics\n",
        "        \"faithfulness_mean\": np.mean(combined_scores),\n",
        "        \"faithfulness_std\": np.std(combined_scores),\n",
        "        \"faithfulness_min\": np.min(combined_scores),\n",
        "        \"faithfulness_max\": np.max(combined_scores),\n",
        "        \"faithfulness_median\": np.median(combined_scores),\n",
        "        \n",
        "        # Individual metric means\n",
        "        \"semantic_mean\": np.mean(semantic_scores),\n",
        "        \"nli_mean\": np.mean(nli_scores),\n",
        "        \"overlap_mean\": np.mean(overlap_scores),\n",
        "        \n",
        "        # Quality distribution\n",
        "        \"high_faithfulness\": sum(1 for s in combined_scores if s >= 0.8),\n",
        "        \"medium_faithfulness\": sum(1 for s in combined_scores if 0.5 <= s < 0.8),\n",
        "        \"low_faithfulness\": sum(1 for s in combined_scores if s < 0.5),\n",
        "        \n",
        "        # Percentages\n",
        "        \"high_pct\": sum(1 for s in combined_scores if s >= 0.8) / len(combined_scores) * 100,\n",
        "        \"medium_pct\": sum(1 for s in combined_scores if 0.5 <= s < 0.8) / len(combined_scores) * 100,\n",
        "        \"low_pct\": sum(1 for s in combined_scores if s < 0.5) / len(combined_scores) * 100,\n",
        "        \n",
        "        # Hallucination detection\n",
        "        \"potential_hallucinations\": sum(1 for s in combined_scores if s < 0.4)\n",
        "    }\n",
        "    \n",
        "    return detailed\n",
        "\n",
        "# Evaluate all three model variants\n",
        "models_config = {\n",
        "    \"High-Faithfulness-RAG\": {\n",
        "        \"predictions\": df_rag[\"faithful_answer\"].tolist(),\n",
        "        \"config\": {\"architecture\": \"RAG-GPT4\", \"retriever\": \"dense\", \"grounding\": \"strict\"}\n",
        "    },\n",
        "    \"Medium-Faithfulness-RAG\": {\n",
        "        \"predictions\": df_rag[\"partial_faithful\"].tolist(),\n",
        "        \"config\": {\"architecture\": \"RAG-GPT3.5\", \"retriever\": \"sparse\", \"grounding\": \"moderate\"}\n",
        "    },\n",
        "    \"Low-Faithfulness-Baseline\": {\n",
        "        \"predictions\": df_rag[\"unfaithful_answer\"].tolist(),\n",
        "        \"config\": {\"architecture\": \"GPT-baseline\", \"retriever\": \"none\", \"grounding\": \"none\"}\n",
        "    }\n",
        "}\n",
        "\n",
        "evaluation_results = {}\n",
        "for model_name, config in models_config.items():\n",
        "    print(f\"\\nEvaluating {model_name}...\")\n",
        "    metrics, detailed = evaluate_rag_model(\n",
        "        model_name,\n",
        "        config[\"predictions\"],\n",
        "        df_rag[\"context\"].tolist(),\n",
        "        config[\"config\"]\n",
        "    )\n",
        "    evaluation_results[model_name] = {\"metrics\": metrics, \"detailed\": detailed}\n",
        "\n",
        "print(\"\\nâœ… Evaluation completed for all models!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
