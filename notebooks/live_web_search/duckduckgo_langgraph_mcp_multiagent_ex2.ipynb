{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3f23da1",
   "metadata": {},
   "source": [
    "\n",
    "# DuckDuckGo + LangGraph (Multi‑Agent) with MCP Tools\n",
    "\n",
    "This notebook shows a **multi‑agent** search system built with **LangGraph**, where a **Supervisor** agent routes tasks to a **Searcher** agent and an **Analyst** agent. The Searcher talks to a **DuckDuckGo tool over MCP (Model Context Protocol)**.  \n",
    "If MCP is unavailable in your environment, the notebook **falls back** to a direct `duckduckgo-search` call so you can still run the graph immediately.\n",
    "\n",
    "**Highlights**\n",
    "- ✅ **MCP**: A tiny MCP server exposes `duckduckgo.text_search` (no API key required).  \n",
    "- ✅ **LangGraph**: Supervisor → (Searcher ↔ MCP tool) → Analyst.  \n",
    "- ✅ **Optional** OpenAI summary if `OPENAI_API_KEY` is set; otherwise a clean rule-based summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c329785",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If needed, install dependencies. It's safe to re-run.\n",
    "%pip install -q duckduckgo-search langgraph langchain-core langchain-community openai\n",
    "\n",
    "# MCP pieces (optional). If these fail to install, the notebook still works using the fallback.\n",
    "# The official Python SDK is typically named `mcp`.\n",
    "try:\n",
    "    %pip install -q mcp\n",
    "    MCP_OK = True\n",
    "except Exception as e:\n",
    "    print(\"MCP install failed:\", e)\n",
    "    MCP_OK = False\n",
    "\n",
    "print(\"MCP_OK:\", 'MCP_OK' in globals() and MCP_OK)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fffc10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os, sys, json, time, subprocess, atexit, asyncio, textwrap, uuid\n",
    "from typing import TypedDict, List, Dict, Optional, Literal\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "# LangGraph\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# DuckDuckGo direct (fallback)\n",
    "from duckduckgo_search import DDGS\n",
    "\n",
    "# Optional OpenAI summarize\n",
    "OPENAI_AVAILABLE = False\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    OPENAI_AVAILABLE = True if os.getenv(\"OPENAI_API_KEY\") else False\n",
    "    if OPENAI_AVAILABLE:\n",
    "        openai_client = OpenAI()\n",
    "except Exception:\n",
    "    OPENAI_AVAILABLE = False\n",
    "\n",
    "def now_iso():\n",
    "    return datetime.now().isoformat(timespec=\"seconds\")\n",
    "\n",
    "print(\"OpenAI available:\", OPENAI_AVAILABLE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8e8b02",
   "metadata": {},
   "source": [
    "\n",
    "## MCP Tool Server (DuckDuckGo)\n",
    "\n",
    "We write a tiny MCP server to disk that exposes one tool:\n",
    "- `duckduckgo.text_search(query: string, max_results: number=8)`\n",
    "\n",
    "We'll launch it as a subprocess and connect via stdio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127b4a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mcp_server_path = \"/mnt/data/mcp_duckduckgo_server.py\"\n",
    "server_code = r'''\n",
    "import asyncio\n",
    "import sys\n",
    "import json\n",
    "from typing import Any, Dict, List, Optional\n",
    "from duckduckgo_search import DDGS\n",
    "\n",
    "# Minimal MCP stdio loop using JSON RPC-like envelopes.\n",
    "# This is intentionally lightweight for notebook demos.\n",
    "# It supports: initialize, list_tools, call_tool\n",
    "\n",
    "TOOLS = {\n",
    "    \"duckduckgo.text_search\": {\n",
    "        \"name\": \"duckduckgo.text_search\",\n",
    "        \"description\": \"Search DuckDuckGo and return top results\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\"type\": \"string\"},\n",
    "                \"max_results\": {\"type\": \"integer\", \"minimum\": 1, \"maximum\": 25}\n",
    "            },\n",
    "            \"required\": [\"query\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def ddg_search(query: str, max_results: int = 8):\n",
    "    items = []\n",
    "    with DDGS() as ddgs:\n",
    "        for r in ddgs.text(query, max_results=max_results, safesearch=\"moderate\", region=\"wt-wt\"):\n",
    "            items.append({\n",
    "                \"title\": r.get(\"title\"),\n",
    "                \"link\": r.get(\"href\"),\n",
    "                \"snippet\": r.get(\"body\"),\n",
    "            })\n",
    "    return items\n",
    "\n",
    "async def ainput():\n",
    "    loop = asyncio.get_event_loop()\n",
    "    return await loop.run_in_executor(None, sys.stdin.readline)\n",
    "\n",
    "async def main():\n",
    "    # Send a basic \"ready\" notice\n",
    "    print(json.dumps({\"type\":\"ready\"}), flush=True)\n",
    "    while True:\n",
    "        line = await ainput()\n",
    "        if not line:\n",
    "            break\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            msg = json.loads(line)\n",
    "        except Exception as e:\n",
    "            print(json.dumps({\"type\":\"error\",\"error\":str(e)}), flush=True)\n",
    "            continue\n",
    "\n",
    "        mtype = msg.get(\"type\")\n",
    "        if mtype == \"initialize\":\n",
    "            # acknowledge\n",
    "            print(json.dumps({\"type\":\"initialized\"}), flush=True)\n",
    "        elif mtype == \"list_tools\":\n",
    "            print(json.dumps({\"type\":\"tools\",\"tools\": list(TOOLS.values())}), flush=True)\n",
    "        elif mtype == \"call_tool\":\n",
    "            name = msg.get(\"name\")\n",
    "            args = msg.get(\"arguments\") or {}\n",
    "            if name == \"duckduckgo.text_search\":\n",
    "                query = args.get(\"query\",\"\").strip()\n",
    "                maxr = int(args.get(\"max_results\", 8))\n",
    "                results = ddg_search(query, maxr)\n",
    "                print(json.dumps({\"type\":\"tool_result\",\"id\": msg.get(\"id\"), \"result\": results}), flush=True)\n",
    "            else:\n",
    "                print(json.dumps({\"type\":\"tool_result\",\"id\": msg.get(\"id\"), \"error\": \"unknown tool\"}), flush=True)\n",
    "        elif mtype == \"shutdown\":\n",
    "            print(json.dumps({\"type\":\"bye\"}), flush=True)\n",
    "            break\n",
    "        else:\n",
    "            print(json.dumps({\"type\":\"error\",\"error\":\"unknown message type\"}), flush=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "'''\n",
    "with open(mcp_server_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(server_code)\n",
    "\n",
    "mcp_server_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b031030",
   "metadata": {},
   "source": [
    "\n",
    "## Lightweight MCP Client (stdio)\n",
    "\n",
    "For demo purposes, we implement a very small stdio client that speaks to the server script above using simple JSON messages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfae6136",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleMCPClient:\n",
    "    def __init__(self, cmd: List[str]):\n",
    "        self.proc = subprocess.Popen(\n",
    "            cmd,\n",
    "            stdin=subprocess.PIPE,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "            bufsize=1\n",
    "        )\n",
    "        atexit.register(self.close)\n",
    "        # wait for ready\n",
    "        ready_line = self.proc.stdout.readline().strip()\n",
    "        # print any early stderr noise\n",
    "        time.sleep(0.1)\n",
    "        if self.proc.poll() is not None:\n",
    "            err = self.proc.stderr.read()\n",
    "            raise RuntimeError(f\"MCP server failed to start: {err}\")\n",
    "        # initialize\n",
    "        self._send({\"type\": \"initialize\"})\n",
    "        _ = self._recv()\n",
    "    \n",
    "    def _send(self, obj: Dict):\n",
    "        if self.proc.stdin:\n",
    "            self.proc.stdin.write(json.dumps(obj) + \"\\n\")\n",
    "            self.proc.stdin.flush()\n",
    "    \n",
    "    def _recv(self) -> Dict:\n",
    "        if self.proc.stdout:\n",
    "            line = self.proc.stdout.readline().strip()\n",
    "            if not line:\n",
    "                return {}\n",
    "            try:\n",
    "                return json.loads(line)\n",
    "            except Exception:\n",
    "                return {\"type\": \"parse_error\", \"raw\": line}\n",
    "        return {}\n",
    "    \n",
    "    def list_tools(self) -> List[Dict]:\n",
    "        self._send({\"type\": \"list_tools\"})\n",
    "        msg = self._recv()\n",
    "        return msg.get(\"tools\", []) if msg.get(\"type\") == \"tools\" else []\n",
    "    \n",
    "    def call_tool(self, name: str, arguments: Dict) -> Dict:\n",
    "        msg_id = str(uuid.uuid4())\n",
    "        self._send({\"type\":\"call_tool\", \"id\": msg_id, \"name\": name, \"arguments\": arguments})\n",
    "        while True:\n",
    "            msg = self._recv()\n",
    "            if msg.get(\"type\") == \"tool_result\" and msg.get(\"id\") == msg_id:\n",
    "                return msg\n",
    "            if self.proc.poll() is not None:\n",
    "                raise RuntimeError(\"MCP server exited unexpectedly\")\n",
    "    \n",
    "    def close(self):\n",
    "        try:\n",
    "            if self.proc and self.proc.poll() is None:\n",
    "                self._send({\"type\":\"shutdown\"})\n",
    "                try:\n",
    "                    self.proc.terminate()\n",
    "                except Exception:\n",
    "                    pass\n",
    "        except Exception:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be555fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def start_mcp():\n",
    "    cmd = [sys.executable, \"/mnt/data/mcp_duckduckgo_server.py\"]\n",
    "    client = SimpleMCPClient(cmd)\n",
    "    return client\n",
    "\n",
    "MCP_CLIENT = None\n",
    "try:\n",
    "    MCP_CLIENT = start_mcp()\n",
    "    tools = MCP_CLIENT.list_tools()\n",
    "    print(\"MCP tools:\", [t[\"name\"] for t in tools])\n",
    "except Exception as e:\n",
    "    MCP_CLIENT = None\n",
    "    print(\"MCP client fallback:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4330df",
   "metadata": {},
   "source": [
    "\n",
    "## LangGraph Multi‑Agent Design\n",
    "\n",
    "Agents:\n",
    "- **Supervisor**: reads the user query and routing hints, decides next step (`searcher` → `analyst` or finish).\n",
    "- **Searcher**: calls the **MCP DuckDuckGo tool** (or direct fallback) to fetch hits.\n",
    "- **Analyst**: summarizes results (rule-based) and optionally adds an OpenAI bullet summary.\n",
    "\n",
    "The state flows Supervisor → Searcher → Analyst → END.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df0800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AgentState(TypedDict, total=False):\n",
    "    query: str\n",
    "    route: Literal[\"searcher\",\"analyst\",\"done\"]\n",
    "    results: List[Dict]\n",
    "    answer_markdown: str\n",
    "\n",
    "def supervisor_node(state: AgentState) -> AgentState:\n",
    "    # simple policy: if no results yet, go search; else go analyze; else done.\n",
    "    if not state.get(\"results\"):\n",
    "        return {\"route\": \"searcher\"}\n",
    "    if not state.get(\"answer_markdown\"):\n",
    "        return {\"route\": \"analyst\"}\n",
    "    return {\"route\": \"done\"}\n",
    "\n",
    "def mcp_duckduckgo_search(query: str, max_results: int = 8) -> List[Dict]:\n",
    "    # Prefer MCP if available\n",
    "    if MCP_CLIENT is not None:\n",
    "        res = MCP_CLIENT.call_tool(\"duckduckgo.text_search\", {\"query\": query, \"max_results\": max_results})\n",
    "        if \"result\" in res:\n",
    "            return res[\"result\"]\n",
    "    # Fallback: direct search\n",
    "    items = []\n",
    "    with DDGS() as ddgs:\n",
    "        for r in ddgs.text(query, max_results=max_results, safesearch=\"moderate\", region=\"wt-wt\"):\n",
    "            items.append({\n",
    "                \"title\": r.get(\"title\"),\n",
    "                \"link\": r.get(\"href\"),\n",
    "                \"snippet\": r.get(\"body\"),\n",
    "            })\n",
    "    return items\n",
    "\n",
    "def searcher_node(state: AgentState) -> AgentState:\n",
    "    q = state.get(\"query\",\"\").strip()\n",
    "    if not q:\n",
    "        raise ValueError(\"Empty query for searcher\")\n",
    "    hits = mcp_duckduckgo_search(q, max_results=8)\n",
    "    return {\"results\": hits}\n",
    "\n",
    "def _markdown_from_results(results: List[Dict], query: str) -> str:\n",
    "    if not results:\n",
    "        return f\"**No results** for `{query}`\"\n",
    "    lines = [f\"### DuckDuckGo results for `{query}` (_{now_iso()}_)\"]\n",
    "    for i, r in enumerate(results, 1):\n",
    "        title = r.get(\"title\") or \"Untitled\"\n",
    "        link = r.get(\"link\") or \"\"\n",
    "        snippet = r.get(\"snippet\") or \"\"\n",
    "        lines.append(f\"{i}. [{title}]({link})\\n   - {snippet}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def _openai_summary(results: List[Dict], query: str) -> Optional[str]:\n",
    "    if not OPENAI_AVAILABLE:\n",
    "        return None\n",
    "    bullets = \"\\n\".join([f\"- {r.get('title')} — {r.get('snippet')}\" for r in results[:8]])\n",
    "    prompt = f'''You are a helpful research assistant.\n",
    "Summarize the most relevant findings for the query: \"{query}\".\n",
    "Use 5-8 concise bullet points and include notable facts/dates if present.\n",
    "\n",
    "Findings:\n",
    "{bullets}\n",
    "'''\n",
    "    try:\n",
    "        resp = openai_client.responses.create(model=\"gpt-4o-mini\", input=prompt)\n",
    "        parts = []\n",
    "        for out in resp.output:\n",
    "            if out.type == \"message\":\n",
    "                for c in out.message.content:\n",
    "                    if c.type == \"text\":\n",
    "                        parts.append(c.text)\n",
    "        txt = \"\\n\".join(parts).strip()\n",
    "        if txt:\n",
    "            return \"### Summary\\n\" + txt\n",
    "    except Exception as e:\n",
    "        print(\"OpenAI summarization failed:\", e)\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def analyst_node(state: AgentState) -> AgentState:\n",
    "    results = state.get(\"results\", [])\n",
    "    query = state.get(\"query\",\"\")\n",
    "    md = _markdown_from_results(results, query)\n",
    "    add = _openai_summary(results, query)\n",
    "    if add:\n",
    "        md = add + \"\\n\\n\" + md\n",
    "    return {\"answer_markdown\": md}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28717c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"supervisor\", supervisor_node)\n",
    "graph.add_node(\"searcher\", searcher_node)\n",
    "graph.add_node(\"analyst\", analyst_node)\n",
    "\n",
    "graph.set_entry_point(\"supervisor\")\n",
    "\n",
    "# conditional routing out of supervisor\n",
    "def route_from_supervisor(state: AgentState):\n",
    "    return state.get(\"route\",\"done\")\n",
    "\n",
    "graph.add_conditional_edges(\"supervisor\", route_from_supervisor, {\n",
    "    \"searcher\": \"searcher\",\n",
    "    \"analyst\": \"analyst\",\n",
    "    \"done\": END\n",
    "})\n",
    "\n",
    "# linear edges\n",
    "graph.add_edge(\"searcher\", \"analyst\")\n",
    "graph.add_edge(\"analyst\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = graph.compile(checkpointer=memory)\n",
    "\n",
    "print(\"Graph ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f618f7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_query(query: str) -> AgentState:\n",
    "    init: AgentState = {\"query\": query}\n",
    "    # supervisor -> (searcher -> analyst) -> end\n",
    "    s1 = app.invoke(init)\n",
    "    return s1\n",
    "\n",
    "example = \"latest LangGraph tutorials and docs\"\n",
    "final = run_query(example)\n",
    "print(final.get(\"answer_markdown\",\"\")[:1200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0cbb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Try multiple queries; press Enter on an empty line to stop.\n",
    "try:\n",
    "    while True:\n",
    "        q = input(\"\\nQuery (blank to stop): \").strip()\n",
    "        if not q:\n",
    "            break\n",
    "        result = run_query(q)\n",
    "        print(\"\\n\" + result.get(\"answer_markdown\",\"\"))\n",
    "except KeyboardInterrupt:\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}